{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ishikaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ishikaj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests  # library to make HTTP requests\n",
    "from bs4 import BeautifulSoup  # library for web scraping\n",
    "from googlesearch import search  # library for performing Google searches\n",
    "import concurrent.futures  # library for parallelism\n",
    "import threading  # library for thread synchronization\n",
    "import pickle  # library for saving data to a file\n",
    "import re # library for regular expressions\n",
    "import nltk # library for natural language processing\n",
    "import time # library for time\n",
    "import json # library for JSON\n",
    "import math # library for mathematical operations\n",
    "from nltk.corpus import stopwords # library for stop words\n",
    "from nltk.stem import PorterStemmer # library for stemming\n",
    "from nltk.tokenize import word_tokenize # library for tokenization\n",
    "\n",
    "nltk.download('stopwords') # download stop words\n",
    "nltk.download('punkt') # download tokenizer\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # set of stop words\n",
    "ps = PorterStemmer() # stemmer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Forests of India\"\n",
    "query2 = \"Tiger Density in India\"\n",
    "query3 = \"Night safari in Forests\"\n",
    "query4 = \"Tiger AND Safari AND Leopard\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Crawl the top 20 web pages from Google search engine\n",
    "This function takes a query as input and returns a list of URLs obtained from Google search results. The function uses the `search()` function from the `googlesearch` library to perform the search and retrieve the URLs. The search results are limited to 20 and the language is set to \"en\" (English). The URLs are stored in the `url_list` variable and returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  crawl the top 20 web pages from google search engine\n",
    "def crawl(query):\n",
    "    \"\"\"\n",
    "    Returns a list of urls for the given query\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The query to be searched on google\n",
    "\n",
    "    Returns:\n",
    "    url_list (list): A list of urls for the given query\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #  get the list of urls\n",
    "    url_list = []\n",
    "\n",
    "    #  loop through the urls and get the text\n",
    "    for url in search(query, num_results=50, lang=\"en\"):\n",
    "        url_list.append(url)\n",
    "\n",
    "    print(\"length of url list: \", len(url_list))\n",
    "    #  return the list of urls\n",
    "    return url_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Get the first 3 paragraphs from the URL\n",
    "\n",
    "This function takes a URL as input and returns the first three non-empty paragraphs from the page as a list of strings. The function uses the `requests` library to retrieve the HTML code of the page and the `BeautifulSoup` library to parse the HTML and the first three non-empty `<p>` tags are retrieved and returned as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first 3 paragraph from the url\n",
    "def get_text_paragraphs(url):\n",
    "    \"\"\"\n",
    "    Returns the first 3 paragraphs from the url\n",
    "\n",
    "    Parameters:\n",
    "    url (str): url of the web page\n",
    "\n",
    "    Returns:\n",
    "    list: list of first 3 paragraphs\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #  get the first 3 paragraphs from the url\n",
    "    paragraphs = []\n",
    "    try:\n",
    "        #  get the html code from the url, if url taking more than 10 seconds to respond, then skip the url\n",
    "        page = requests.get(url, timeout=10)\n",
    "\n",
    "        if page.status_code != 200:\n",
    "            print(\"Error in getting text\")\n",
    "            return []\n",
    "       \n",
    "        #  parse the html code using beautiful soup library\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        #  get the body tag\n",
    "        body = soup.find('body')\n",
    "\n",
    "        #  if body tag is not present, then skip the url\n",
    "        if body is None:\n",
    "            print(\"Body tag not found\")\n",
    "            return []\n",
    "        \n",
    "\n",
    "        #  get first 3 non empty <p> tags \n",
    "        p_tags = body.find_all('p')\n",
    "\n",
    "        # if there are nested <p> tags, then get the text from the inner most <p> tag\n",
    "        for i in range(len(p_tags)):\n",
    "            while p_tags[i].find('p') is not None:\n",
    "                p_tags[i] = p_tags[i].find('p')\n",
    "\n",
    "        #  if there are less than 3 <p> tags, then get all the <span> tags\n",
    "        if len(p_tags) < 3:\n",
    "            p_tags = body.find_all('span')\n",
    "\n",
    "            # if there are nested <span> tags, then get the text from the inner most <span> tag\n",
    "            for i in range(len(p_tags)):\n",
    "                while p_tags[i].find('span') is not None:\n",
    "                    p_tags[i] = p_tags[i].find('span')\n",
    "        \n",
    "        #  if there are less than 3 <span> tags, then get all the <div> tags\n",
    "        if len(p_tags) < 3:\n",
    "            p_tags = body.find_all('div')\n",
    "\n",
    "            # if there are nested <div> tags, then get the text from the inner most <div> tag\n",
    "            for i in range(len(p_tags)):\n",
    "                while p_tags[i].find('div') is not None:\n",
    "                    p_tags[i] = p_tags[i].find('div')\n",
    "            \n",
    "        \n",
    "        # remove empty tags\n",
    "        p_tags = [p for p in p_tags if p.text.strip() != '']\n",
    "\n",
    "        # remove paragraphs with non english and non number characters\n",
    "        p_tags = [p for p in p_tags if re.search('[a-zA-Z0-9]', p.text.strip()) is not None]\n",
    "\n",
    "        #  get first 3 tags\n",
    "        p_tags = p_tags[:3]\n",
    "\n",
    "        #  get the text from the tags\n",
    "        paragraphs = [p.text for p in p_tags]\n",
    "\n",
    "    except:\n",
    "        print(\"Here : Error in getting text\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract the first three paragraphs from each of the webpages.\n",
    "\n",
    "This function takes a search query as input and returns a list of the first three paragraphs from each of the top 20 websites returned by a Google search for the query. The function first calls the `crawl()` function to get a list of URLs for the query. The function then loops through the URLs and calls the `get_text()` function to retrieve the first three paragraphs from each website. The paragraphs from all the websites are combined into a single list and returned by the function.d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraphs(query):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a dictionary of urls and their first 3 paragraphs\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The query to be searched on google\n",
    "\n",
    "    Returns:\n",
    "    pages (dict): A dictionary of urls and their first 3 paragraphs\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #  get the list of urls\n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    url_list = crawl(query)\n",
    "\n",
    "    # remove duplicate urls\n",
    "    url_list = list(dict.fromkeys(url_list))\n",
    "\n",
    "    #  get the first 3 paragraphs from each url in parallel\n",
    "    pages = {}\n",
    "    print(f\"Retrieving pages...\")\n",
    "\n",
    "    # define a lock to prevent race conditions when updating pages dict\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # submit a separate thread to retrieve the first 3 pages for each URL\n",
    "        future_to_url = {executor.submit(get_text_paragraphs, url): url for url in url_list}\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            #  if pages dic has 20 entries, then stop\n",
    "            if len(pages) == 20:\n",
    "                break\n",
    "\n",
    "            url = future_to_url[future]\n",
    "\n",
    "            try:\n",
    "                para = future.result()\n",
    "                if len(para) == 3:\n",
    "                    print(f\"Retrieved {url} is ok\")\n",
    "                    # acquire lock before updating pages dict\n",
    "                    lock.acquire()\n",
    "\n",
    "                    pages[url] = para\n",
    "\n",
    "                    # release lock after updating pages dict\n",
    "                    lock.release()\n",
    "                else:\n",
    "                    print(f\"Error in getting text from {url}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"Exception occurred while getting text from {url}: {exc}\")\n",
    "\n",
    "    print(f\"Retrieved {len(pages)} pages\")\n",
    "    return pages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to clean the text\n",
    "\n",
    "This function takes a list of strings as input and returns a list of strings after removing the punctuations, numbers, and special characters from the strings. The function uses the `re` library to remove the punctuations, numbers, and special characters from the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the text\n",
    "def clean_text(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a list of words after removing stop words and special characters\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be cleaned\n",
    "\n",
    "    Returns:\n",
    "    list: A list of words after removing stop words and special characters\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # remove all the special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "    # remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # covert to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove all the stopwords\n",
    "    text = [word for word in word_tokenize(text) if word not in stop_words]\n",
    "\n",
    "    #  join the words\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to create inverted index\n",
    "\n",
    "This function takes a list of documents and name of document as input and returns an inverted index for the documents. The function first creates a dictionary `inverted_index` to store the inverted index. The function loops through the words in the documents and adds the document name to the list of documents for the word in the inverted index. The function returns the inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a program in Python to build an inverted index of these document\n",
    "def create_inverted_index(documents, doc_names):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns an inverted index of the documents\n",
    "\n",
    "    Parameters:\n",
    "    documents (list): A list of documents\n",
    "    doc_names (list): A list of document names\n",
    "\n",
    "    Returns:\n",
    "    dict: An inverted index of the documents\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #  get the list of words in the documents\n",
    "    words = []\n",
    "    for doc in documents:\n",
    "        words.extend(doc.split())\n",
    "\n",
    "    #  remove duplicate words\n",
    "    words = list(dict.fromkeys(words))\n",
    "\n",
    "    #  create an inverted index\n",
    "    inverted_index = {}\n",
    "    for word in words:\n",
    "        inverted_index[word] = []\n",
    "        for i in range(len(documents)):\n",
    "            if word in documents[i].split():\n",
    "                inverted_index[word].append(doc_names[i])\n",
    "\n",
    "    return inverted_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the paragraphs from the top 20 websites for the query1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Forests of India\n",
      "length of url list:  52\n",
      "Retrieving pages...\n",
      "Retrieved http://edugreen.teri.res.in/explore/forestry/history.htm is ok\n",
      "Retrieved https://www.vedantu.com/question-answer/what-are-the-different-types-of-forests-in-india-5ecf59425c02fb0719ca96f1 is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://www.toppr.com/ask/question/describe-the-forests-of-india/\n",
      "Retrieved https://fsi.nic.in/scheme-of-classification is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://india.mongabay.com/2022/01/the-state-of-indias-forests-losing-forests-gaining-plantations/\n",
      "Retrieved https://www.india.gov.in/topics/environment-forest is ok\n",
      "Retrieved https://www.drishtiias.com/to-the-points/paper1/types-of-forests-in-india is ok\n",
      "Retrieved https://pib.gov.in/PressReleasePage.aspx?PRID=1789635 is ok\n",
      "Retrieved https://www.clubmahindra.com/blog/experience/top-6-forests-in-india is ok\n",
      "Here : Error in getting text\n",
      "Error in getting text from https://ifs.nic.in/\n",
      "Error in getting text\n",
      "Error in getting text from https://www.discoverwalks.com/blog/india/10-most-beautiful-forest-in-india/\n",
      "Retrieved https://www.forbes.com/sites/priyashukla/2022/03/30/forests-in-india-are-disappearing-faster-than-previously-thought/ is ok\n",
      "Retrieved https://www.downtoearth.org.in/blog/forests/international-day-of-forests-can-india-shift-from-a-plantation-mindset-to-an-eco-restoration-one-82005 is ok\n",
      "Retrieved https://byjus.com/free-ias-prep/natural-vegetation-in-india/ is ok\n",
      "Retrieved https://www.weforum.org/agenda/2022/03/these-13-innovative-organizations-are-conserving-and-restoring-india-s-forest-landscapes/ is ok\n",
      "Retrieved https://www.cseindia.org/forest-in-india-7691 is ok\n",
      "Retrieved https://www.globalforestwatch.org/blog/data-and-research/whats-happening-in-india-forests/ is ok\n",
      "Retrieved https://traveltriangle.com/blog/rain-forests-in-india/ is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://academic.oup.com/book/35227/chapter/299755542\n",
      "Retrieved https://www.shaalaa.com/question-bank-solutions/describe-the-forests-of-india-natural-vegetation_200407 is ok\n",
      "Retrieved https://timesofindia.indiatimes.com/india/counting-trees-properly/articleshow/89022405.cms is ok\n",
      "Here : Error in getting text\n",
      "Error in getting text from https://www.adotrip.com/blog/mangrove-forests-in-india\n",
      "Retrieved https://commons.wikimedia.org/wiki/Category:Forests_in_India is ok\n",
      "Retrieved https://www.studyiq.com/articles/tropical-evergreen-forests/ is ok\n",
      "Retrieved https://pickyourtrail.com/blog/rain-forests-in-india/ is ok\n",
      "Retrieved https://www.nature.com/articles/120251a0 is ok\n",
      "Error in getting text\n",
      "Error in getting text\n",
      "Here : Error in getting text\n",
      "Here : Error in getting text\n",
      "Retrieved 20 pages\n"
     ]
    }
   ],
   "source": [
    "pages1 = get_paragraphs(query1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FInd all the paragraphs from the top 20 websites for the query2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tiger Density in India\n",
      "length of url list:  52\n",
      "Retrieving pages...\n",
      "Retrieved https://byjus.com/ias-questions/which-national-park-has-highest-number-of-tigers/ is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://www.insightsonindia.com/2022/03/11/tiger-density-in-india/\n",
      "Retrieved https://www.thehindu.com/news/national/other-states/Kaziranga-has-the-worlds-highest-tiger-density-report/article16373525.ece is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://www.newindianexpress.com/nation/2020/jul/28/corbett-reserve-has-highest-tiger-density-in-india-report-2175962.html\n",
      "Retrieved https://en.wikipedia.org/wiki/Tiger_reserves_of_India is ok\n",
      "Here : Error in getting text\n",
      "Error in getting text from https://optimizeias.com/tiger-density-in-india/\n",
      "Retrieved https://www.acubeias.com/current-affairs-for-upsc/tiger-density-in-india is ok\n",
      "Retrieved https://www.drishtiias.com/daily-updates/daily-news-analysis/corbett-tiger-reserve-uttarakhand-1 is ok\n",
      "Retrieved https://www.corbettnationalpark.in/blog/celebrating-great-news-for-indias-tigers-corbett-records-highest-density-of-tigers-in-world/ is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://www.hindustantimes.com/india-news/corbett-has-india-s-largest-tiger-population-among-50-reserves-report/story-iwcEKO7UbDeGqbj88ypRWN.html\n",
      "Retrieved http://www.walkthroughindia.com/wildlife/top-10-tiger-reserves-in-india-with-maximum-tiger-population/ is ok\n",
      "Retrieved https://www.firstpost.com/india/name-change-row-aside-jim-corbett-park-is-torchbearer-in-indias-tiger-conservation-success-story-10042191.html is ok\n",
      "Retrieved https://www.civilsdaily.com/news/tiger-density-in-india/ is ok\n",
      "Retrieved https://www.tigersafariindia.co.uk/tiger-reserves-with-highest-tiger-population-in-india/ is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://upsccolorfullnotes.com/51-tiger-reserves-in-india-with-map/\n",
      "Retrieved https://www.clearias.com/tiger-reserves-in-india/ is ok\n",
      "Retrieved https://www.ndtv.com/india-news/exclusive-inside-indias-wild-reserve-with-highest-tiger-density-1694943 is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://ntca.gov.in/\n",
      "Retrieved https://lotusarise.com/tiger-reserves-in-india-upsc/ is ok\n",
      "Error in getting text from https://www.instagram.com/p/CDOH9FWhwHS/\n",
      "Retrieved https://vajiramias.com/current-affairs/all-india-tiger-estimation-report-2018/5f2248481d5def7de0991598/ is ok\n",
      "Retrieved https://bigcatsindia.com/tiger-census-2018/ is ok\n",
      "Retrieved https://www.moneycontrol.com/news/trends/features/jungle-bells-corbett-maneater-threatens-to-spoil-the-party-for-merry-holidaymakers-9796161.html is ok\n",
      "Retrieved https://www.thrillophilia.com/best-tiger-reserves-in-india is ok\n",
      "Retrieved https://abhipedia.abhimanu.com/Article/STATE/MzM0MDgy/Highest-Tiger-density-Assam is ok\n",
      "Retrieved https://www.authenticindiatours.com/2022/07/12/best-places-to-find-tigers-in-india/ is ok\n",
      "Retrieved https://pib.gov.in/PressReleasePage.aspx?PRID=1580622 is ok\n",
      "Error in getting text\n",
      "Error in getting text\n",
      "Here : Error in getting text\n",
      "Here : Error in getting text\n",
      "Here : Error in getting text\n",
      "Here : Error in getting text\n",
      "Here : Error in getting text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body tag not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body tag not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body tag not found\n",
      "Retrieved 20 pages\n"
     ]
    }
   ],
   "source": [
    "pages2 = get_paragraphs(query2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the paragraphs from the top 20 websites for the query3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Night safari in Forests\n",
      "length of url list:  51\n",
      "Retrieving pages...\n",
      "Retrieved https://www.deccanherald.com/national/south/environmentalists-protest-jungle-night-safari-in-wayanad-forest-1154114.html is ok\n",
      "Retrieved https://www.tadobanationalparkonline.in/night-safari-in-tadoba.php is ok\n",
      "Retrieved https://www.mptourism.com/night-safari-at-these-national-parks-in-madhya-pradesh.html is ok\n",
      "Retrieved https://forest.mponline.gov.in/ is ok\n",
      "Retrieved http://www.walkthroughindia.com/wildlife/top-5-places-proposed-night-safari-india/ is ok\n",
      "Retrieved https://www.dailypioneer.com/2022/india/india---s-first-night-safari-to-come-up-in-lucknow.html is ok\n",
      "Retrieved https://thelivenagpur.com/2022/10/13/night-safaris-banned-in-maha-and-mp/ is ok\n",
      "Retrieved https://www.jimcorbettnationalpark.co.in/online-corbett-night-stay-booking.html is ok\n",
      "Retrieved http://www.clarissaresorts.com/night-safari.php is ok\n",
      "Retrieved https://timesofindia.indiatimes.com/travel/travel-news/mp-you-will-now-be-able-to-enjoy-night-safaris-in-3-national-parks/articleshow/81392465.cms is ok\n",
      "Retrieved https://rajaji-nationalpark.co.in/safari-rajaji-national-park.html is ok\n",
      "Retrieved https://en.wikipedia.org/wiki/Night_safari is ok\n",
      "Retrieved https://www.thehindu.com/news/national/other-states/controversy-over-assam-cmsadhgurus-night-safari-in-kaziranga-national-park/article65934371.ece is ok\n",
      "Retrieved https://www.maharashtratourism.gov.in/night-safari is ok\n",
      "Retrieved https://www.holidify.com/places/durshet/night-safari-sightseeing-4635.html is ok\n",
      "Retrieved https://www.findyoursafari.com/blog/night-safari-at-indian-national-parks-is-a-real-thing is ok\n",
      "Retrieved https://www.kenriverlodge.com/night-safari-in-panna is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://redearth.in/blog/2022/04/29/night-safari-in-tadoba/\n",
      "Retrieved https://www.corbetttourism.com/onlinecorbettbooking.html is ok\n",
      "Here : Error in getting text\n",
      "Error in getting text from https://www.pannatigerreserve.in/How%20to%20visit%20park.htm\n",
      "Retrieved https://www.thegreatnext.com/dandeli-forest-safari/ is ok\n",
      "Retrieved https://timesofindia.indiatimes.com/city/lucknow/animals-new-abode-kukrail-zoo-and-night-safari-to-be-ready-by-2023-end/articleshow/96187330.cms is ok\n",
      "Here : Error in getting text\n",
      "Here : Error in getting text\n",
      "Retrieved 20 pages\n"
     ]
    }
   ],
   "source": [
    "pages3 = get_paragraphs(query3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the paragraphs from the top 20 websites for the query4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tiger AND Safari AND Leopard\n",
      "length of url list:  50\n",
      "Retrieving pages...\n",
      "Retrieved https://curlytales.com/wildlife-safaris-in-rajasthan-that-ensures-tiger-leopard-spotting/ is ok\n",
      "Retrieved https://www.tigersafariindia.co.uk/leopard-safari-and-tiger-safari-in-india/ is ok\n",
      "Retrieved https://m.timesofindia.com/home/sunday-times/aside-tigers-let-the-leopard-safari-begin/articleshow/97615427.cms is ok\n",
      "Retrieved https://www.responsibletravel.com/holidays/big-cat-safaris/travel-guide/siberian-tiger-and-amur-leopard-safaris is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://www.naturesafariindia.com/tiger-safari-tours/tiger-leopards-and-lions-photographic-safari/\n",
      "Error in getting text\n",
      "Error in getting text from https://www.tigersafariindia.com/tour-packages/tiger-taj-leopard-safari/\n",
      "Error in getting text\n",
      "Error in getting text from https://www.kanha.net/tiger-safari-tours/taj-tiger-and-leopard-safari-tour-in-india/\n",
      "Retrieved https://www.ranthamborenationalpark.com/tiger-leopard-safari-tour-ranthambore-and-jawai.html is ok\n",
      "Retrieved https://www.naturalworldsafaris.com/holidays/asia/sri-lanka/tigers-and-leopards-of-asia is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://www.jhalanaleopardreserve.com/\n",
      "Error in getting text from https://m.facebook.com/123596309393294/\n",
      "Retrieved https://www.jaipurstuff.com/jaipur-first-city-having-tiger-safari-with-leopard-elephant-and-lion-together/ is ok\n",
      "Retrieved https://bigcatsindia.com/ is ok\n",
      "Retrieved https://www.tigersafari.net/tiger-lion-and-leopard-tour/ is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://www.freepik.com/free-vector/tiger-prints-patterns-safari-leopard-jaguar-skin_8565981.htm\n",
      "Retrieved https://www.wildlifetrails.co.uk/parks-to-visit-to-see-tigers-and-leopards/ is ok\n",
      "Retrieved https://www.pugdundeesafaris.com/blog/finding-leopards-in-the-tiger-country/ is ok\n",
      "Retrieved https://www.apex-expeditions.com/expeditions/tigers-snow-leopards-india-safari/ is ok\n",
      "Retrieved https://www.armstrongsafaris.com/post/luxury-rajasthan-tiger-leopard-cultural-safari-with-armstrong-safaris-and-bespoke-india-travel is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://jhalanaleopardsafari.com/\n",
      "Error in getting text from https://roundglasssustain.com/wildvaults/bandipur-leopard\n",
      "Retrieved https://www.beyondwild.in/tiger_safari is ok\n",
      "Retrieved https://www.explorationscompany.com/asia/inspire-me/tiger-safaris is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://www.tigerwalah.com/customized-wildlife/leopard-safari-jaipur\n",
      "Retrieved https://timesofindia.indiatimes.com/city/nagpur/leopard-bison-compete-with-tigers-popularity-this-safari-season/articleshow/49322923.cms is ok\n",
      "Retrieved https://www.forbes.com/sites/sandramacgregor/2022/08/11/discover-5-under-the-radar-destinations-for-tigers-pumas-jaguars-and-leopards/ is ok\n",
      "Retrieved https://voygr.com/journeys/tiger-safari-in-india/tiger-safari-extension-to-snow-leopard-expedition/ is ok\n",
      "Error in getting text\n",
      "Error in getting text from https://www.pench.net/tiger-safari-tours/black-leopard-safari-tour-in-india/\n",
      "Retrieved https://www.10adventures.com/tour/snow-leopard-tiger-trek/ is ok\n",
      "Retrieved https://www.amazon.in/Balloonistics-Leopard-Cheetah-Printed-Balloon/dp/B07QMSDZ3F is ok\n",
      "Here : Error in getting text\n",
      "Here : Error in getting text\n",
      "Error in getting text\n",
      "Here : Error in getting text\n",
      "Retrieved 20 pages\n"
     ]
    }
   ],
   "source": [
    "pages4 = get_paragraphs(query4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the pages dictionary in a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  save paragraphs in pkl file\n",
    "# with open('pages1.pkl', 'wb') as f:\n",
    "#     pickle.dump(pages1, f)\n",
    "\n",
    "# with open('pages2.pkl', 'wb') as f:\n",
    "#     pickle.dump(pages2, f)\n",
    "\n",
    "# with open('pages3.pkl', 'wb') as f:\n",
    "#     pickle.dump(pages3, f)\n",
    "\n",
    "# with open('pages4.pkl', 'wb') as f:\n",
    "#     pickle.dump(pages4, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the pages dictionary from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  open the pkl file\n",
    "with open('pages1.pkl', 'rb') as f:\n",
    "    pages1 = pickle.load(f)\n",
    "\n",
    "with open('pages2.pkl', 'rb') as f:\n",
    "    pages2 = pickle.load(f)\n",
    "\n",
    "with open('pages3.pkl', 'rb') as f:\n",
    "    pages3 = pickle.load(f)\n",
    "\n",
    "with open('pages4.pkl', 'rb') as f:\n",
    "    pages4 = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean the text in the pages dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  clean the text\n",
    "cleaned_pages1 = {}\n",
    "for url, paragraphs in pages1.items():\n",
    "    cleaned_pages1[url] = [clean_text(paragraph) for paragraph in paragraphs]\n",
    "\n",
    "cleaned_pages2 = {}\n",
    "for url, paragraphs in pages2.items():\n",
    "    cleaned_pages2[url] = [clean_text(paragraph) for paragraph in paragraphs]\n",
    "\n",
    "cleaned_pages3 = {}\n",
    "for url, paragraphs in pages3.items():\n",
    "    cleaned_pages3[url] = [clean_text(paragraph) for paragraph in paragraphs]\n",
    "\n",
    "cleaned_pages4 = {}\n",
    "for url, paragraphs in pages4.items():\n",
    "    cleaned_pages4[url] = [clean_text(paragraph) for paragraph in paragraphs]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the document to save the paragraphs information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create documents for query 1 using the cleaned pages\n",
    "for i in range(1, 21):\n",
    "    globals()[f\"d{i}_q1\"] = globals()[f\"cleaned_pages1\"][list(globals()[f\"cleaned_pages1\"])[i-1]]\n",
    "    globals()[f\"d{i}_q1\"] = ' '.join(globals()[f\"d{i}_q1\"])\n",
    "\n",
    "#  create documents for query 2 using the cleaned pages\n",
    "for i in range(1, 21):\n",
    "    globals()[f\"d{i}_q2\"] = globals()[f\"cleaned_pages2\"][list(globals()[f\"cleaned_pages2\"])[i-1]]\n",
    "    globals()[f\"d{i}_q2\"] = ' '.join(globals()[f\"d{i}_q2\"])\n",
    "\n",
    "#  create documents for query 3 using the cleaned pages\n",
    "for i in range(1, 21):\n",
    "    globals()[f\"d{i}_q3\"] = globals()[f\"cleaned_pages3\"][list(globals()[f\"cleaned_pages3\"])[i-1]]\n",
    "    globals()[f\"d{i}_q3\"] = ' '.join(globals()[f\"d{i}_q3\"])\n",
    "\n",
    "#  create documents for query 4 using the cleaned pages\n",
    "for i in range(1, 21):\n",
    "    globals()[f\"d{i}_q4\"] = globals()[f\"cleaned_pages4\"][list(globals()[f\"cleaned_pages4\"])[i-1]]\n",
    "    globals()[f\"d{i}_q4\"] = ' '.join(globals()[f\"d{i}_q4\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the documents in text files in documents folder\n",
    "for i in range(1, 21):\n",
    "    with open(f\"documents/d{i}_q1.txt\", \"w\") as f:\n",
    "        f.write(globals()[f\"d{i}_q1\"])\n",
    "    \n",
    "    with open(f\"documents/d{i}_q2.txt\", \"w\") as f:\n",
    "        f.write(globals()[f\"d{i}_q2\"])\n",
    "\n",
    "    with open(f\"documents/d{i}_q3.txt\", \"w\") as f:\n",
    "        f.write(globals()[f\"d{i}_q3\"])\n",
    "\n",
    "    with open(f\"documents/d{i}_q4.txt\", \"w\") as f:\n",
    "        f.write(globals()[f\"d{i}_q4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all the documents\n",
    "documents = []\n",
    "for i in range(1, 21):\n",
    "    documents.append(globals()[f\"d{i}_q1\"])\n",
    "    documents.append(globals()[f\"d{i}_q2\"])\n",
    "    documents.append(globals()[f\"d{i}_q3\"])\n",
    "    documents.append(globals()[f\"d{i}_q4\"])\n",
    "\n",
    "#  create a list of all the document names\n",
    "doc_names = []\n",
    "for i in range(1, 21):\n",
    "    doc_names.append(f\"d{i}_q1\")\n",
    "    doc_names.append(f\"d{i}_q2\")\n",
    "    doc_names.append(f\"d{i}_q3\")\n",
    "    doc_names.append(f\"d{i}_q4\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the inverted index for the paragraphs in the pages dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an inverted index\n",
    "inverted_index = create_inverted_index(documents, doc_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the inverted index in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #  save the inverted index in a text file\n",
    "# with open('inverted_index.json', 'w') as file:\n",
    "#     file.write(json.dumps(inverted_index))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open the inverted index from the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  read the inverted index from the text file\n",
    "with open('inverted_index.json', 'r') as file:\n",
    "    inverted_index = json.loads(file.read())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the document names to integers for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  change the document names to integers\n",
    "for word, doc_names in inverted_index.items():\n",
    "    for i in range(len(doc_names)):\n",
    "        doc_names[i] = int(doc_names[i][1:].split(\"_\")[0]) + (int(doc_names[i].split(\"_\")[1][1:]) - 1) * 20\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save modified inverted index in a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('inverted_index_modified.json', 'w') as file:\n",
    "    file.write(json.dumps(inverted_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the inverted index by the number of documents in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the inverted index by the number of documents each word appears in\n",
    "sorted_inverted_index = {k: v for k, v in sorted(inverted_index.items(), key=lambda item: len(item[1]), reverse=True)}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print top 3 words with the highest number of documents and bottom 3 words with the lowest number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 words in the inverted index:\n",
      "india\n",
      "tiger\n",
      "safari\n",
      "______________________________________________________\n",
      "\n",
      "Bottom 3 words in the inverted index:\n",
      "share\n",
      "pack\n",
      "balloon\n"
     ]
    }
   ],
   "source": [
    "#  print top 3 and bottom 3 words in the inverted index\n",
    "print(\"Top 3 words in the inverted index:\")\n",
    "for i in range(3):\n",
    "    print(list(sorted_inverted_index.keys())[i])\n",
    "\n",
    "print(\"______________________________________________________\")\n",
    "print()\n",
    "print(\"Bottom 3 words in the inverted index:\")\n",
    "for i in range(3):\n",
    "    print(list(sorted_inverted_index.keys())[-i-1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge posting-lists\n",
    "\n",
    "Implement the merge algorithm for intersecting the postings of two terms, as well as\n",
    "code to use it to process Boolean queries. When there are multiple query terms, make\n",
    "sure that your algorithm uses the optimization of performing the most restrictive\n",
    "intersection first.\n",
    "Using your algorithm and the index you built on the web documents, process the\n",
    "following queries:\n",
    "1. Tiger AND Safari\n",
    "2.  Wildlife AND Poaching\n",
    "3.  Leopard AND Night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries = ['Tiger AND Safari', 'Wildlife AND Poaching', 'Leopard AND Night']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to merge the posting lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_algo(queries):\n",
    "  \n",
    "  \"\"\" \n",
    "  Returns the postings list of the query using merge algorithm\n",
    "\n",
    "    Parameters:\n",
    "    queries (list): list of words in the query\n",
    "\n",
    "    Returns:\n",
    "    list: postings list of the query\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  answer_postings = []\n",
    "\n",
    "  # loop through the queries and append the postings list of each query to answer_postings \n",
    "  for i in queries:\n",
    "    # check if the word is in the inverted index\n",
    "    if sorted_inverted_index.get(i) == None:\n",
    "      print(i, \" Not Present\")\n",
    "      # return empty list if the word is not in the inverted index\n",
    "      return []\n",
    "    \n",
    "    else:\n",
    "      \n",
    "      answer_postings.append(sorted_inverted_index[i])\n",
    "      \n",
    "  # help restrictive intersection first\n",
    "  answer_postings.sort(key=len)\n",
    "  \n",
    "  top = answer_postings[0]\n",
    "\n",
    "  for i in range(1, len(answer_postings)):\n",
    "    #intersection with first list\n",
    "    print(\"happen\")\n",
    "\n",
    "    m = len(top)\n",
    "    \n",
    "    n = len(answer_postings[i])\n",
    "\n",
    "    print(top)\n",
    "\n",
    "    print(answer_postings[i])\n",
    "\n",
    "    k = 0\n",
    "    l = 0\n",
    "\n",
    "    merged = []\n",
    "\n",
    "    #  merge the two lists\n",
    "    while(k < m and l < n):\n",
    "\n",
    "      # if the two elements are equal, append it to the merged list\n",
    "      if(top[k] == answer_postings[i][l]):\n",
    "\n",
    "        merged.append(top[k])\n",
    "\n",
    "        k = k + 1\n",
    "        l = l + 1\n",
    "      else:\n",
    "        # if the element in the first list is smaller, increment the index of the first list\n",
    "        if(top[k] < answer_postings[i][l]):\n",
    "          k = k + 1\n",
    "        # if the element in the second list is smaller, increment the index of the second list\n",
    "        else:\n",
    "          l = l + 1\n",
    "\n",
    "    top = merged\n",
    "\n",
    "  return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_queries(query):\n",
    "\n",
    "    #  split the query into words\n",
    "    query = query.split(\"AND\")\n",
    "\n",
    "    #  remove the spaces from the words\n",
    "    query =  [i.strip() for i in query]\n",
    "\n",
    "    #  clean the words\n",
    "    query = [clean_text(i) for i in query]\n",
    "\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to process the boolean queries and return the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_1b(user_query):\n",
    "    \"\"\" \n",
    "    Function to process the query and return the relevant documents\n",
    "\n",
    "        Parameters:\n",
    "        user_query (str): query entered by the user\n",
    "\n",
    "        Returns:\n",
    "        list: list of relevant documents\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    #  extract the queries from the user query\n",
    "    user_query = extract_queries(user_query)\n",
    "\n",
    "    # remove stop words from the query\n",
    "    print(\"query\" , user_query)\n",
    "\n",
    "    retrieved_docs = merge_algo(user_query)\n",
    "    # print(\"retrieved docs\", retrieved_docs)\n",
    "    \n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Results:\n",
      "[[62, 44, 25, 65, 66, 47, 67, 48, 68, 69, 30, 74, 78, 39, 80], [], [71]]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# process the queries\n",
    "for query in sample_queries:\n",
    "    results.append(process_query_1b(query))\n",
    "print(\"Results:\")\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the same query 100 times and calculating the average time taken to process the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Tiger AND Safari\n",
      "query ['tiger', 'safari']\n",
      "happen\n",
      "[41, 62, 43, 44, 25, 45, 65, 46, 66, 47, 67, 48, 68, 49, 69, 30, 70, 51, 52, 74, 56, 57, 77, 78, 39, 59, 80]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Wildlife AND Poaching\n",
      "query ['wildlife', 'poaching']\n",
      "poaching  Not Present\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved docs for  Leopard AND Night\n",
      "query ['leopard', 'night']\n",
      "happen\n",
      "[62, 65, 66, 67, 69, 70, 71, 77, 78, 39, 80]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Time taken for merge algorithm:  [0.07725262641906738, 0.05397748947143555, 0.07256388664245605]\n"
     ]
    }
   ],
   "source": [
    "#Testing provided queries\n",
    "\n",
    "# 100 random queries\n",
    "\n",
    "time_for_merge = []\n",
    "for i in sample_queries:\n",
    "    start_time_1b = time.time()\n",
    "    for j in range(100):\n",
    "\n",
    "        print(\"Retrieved docs for \", i)\n",
    "        process_query_1b(i)\n",
    "\n",
    "    end_time_1b = time.time()\n",
    "    time_for_merge.append(end_time_1b - start_time_1b)\n",
    "\n",
    "print(\"Time taken for merge algorithm: \", time_for_merge)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Skip-pointers\n",
    "\n",
    "Re-index the same set of documents crawled in part 1 with skip-pointers. For a posting\n",
    "list of length P, use P evenly-spaced skip pointers. Execute the same three queries\n",
    "mentioned above 100 times each and compare the absolute total time taken to run them\n",
    "for the index with skip-pointers and index without skip-pointers (created during part 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skip_lists(posting_list):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a skip list of the posting list\n",
    "\n",
    "    Parameters:\n",
    "    posting_list (list): A posting list\n",
    "\n",
    "    Returns:\n",
    "    skip_list (list): A skip list of the posting list\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #  create a skip list\n",
    "    skip_list = []\n",
    "    \n",
    "    # get the size of the posting list\n",
    "    size = len(posting_list)\n",
    "\n",
    "    # For a posting list of length P, use P evenly-spaced skip pointers.\n",
    "    # The first skip pointer is at the Pth document, the second at the 2Pth document, and so on.\n",
    "    # The last skip pointer is at the last document in the posting list.\n",
    "    skip_count = int(math.sqrt(size))\n",
    "\n",
    "    period = size // skip_count\n",
    "\n",
    "    curr_skip_index = period - 1\n",
    "\n",
    "    for i in range(size):\n",
    "        # if the current index is equal to the current skip index, add the document to the skip list with a skip pointer of the next skip index\n",
    "        if i == curr_skip_index:\n",
    "\n",
    "            skip_list.append([posting_list[i], curr_skip_index + period])\n",
    "\n",
    "            #  update the current skip index\n",
    "            curr_skip_index += period\n",
    "\n",
    "        # if the current index is not equal to the current skip index, add the document to the skip list with a skip pointer of 0\n",
    "        else:\n",
    "            \n",
    "            skip_list.append([posting_list[i], 0])\n",
    "\n",
    "            \n",
    "\n",
    "    return skip_list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create the inverted index with skip pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skip_pointers(inverted_index):\n",
    "    \"\"\"\n",
    "    Create skip pointers for the inverted index.\n",
    "\n",
    "    Parameters:\n",
    "    inverted_index (dict): An inverted index of the documents\n",
    "\n",
    "    Returns:\n",
    "    dict: An inverted index of the documents with skip pointers\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #  create skip pointers\n",
    "    skip_inv_index = {}\n",
    "    for word, doc_list in inverted_index.items():\n",
    "\n",
    "        #  create a skip list for each posting list\n",
    "        skip_list = create_skip_lists(doc_list)\n",
    "\n",
    "        #  update the posting list with the skip list\n",
    "        skip_inv_index[word] = skip_list\n",
    "\n",
    "    return skip_inv_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_posting_list_with_skip_pointers(query1, query2, inverted_index):\n",
    "\n",
    "    \"\"\" \n",
    "\n",
    "    Function to merge the posting lists of two words using skip pointers\n",
    "\n",
    "\n",
    "        Parameters:\n",
    "        query1 (str): first word in the query\n",
    "        query2 (str): second word in the query\n",
    "        inverted_index (dict): inverted index of the documents\n",
    "\n",
    "        Returns:\n",
    "        list: merged posting list\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    #  find the posting list for the first word in the query\n",
    "    posting_list1 = inverted_index[query1]\n",
    "\n",
    "    #  find the posting list for the second word in the query\n",
    "    posting_list2 = inverted_index[query2]\n",
    "\n",
    "    #  create a list to store the merged posting list\n",
    "    merged_posting_list = []\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    #  merge the posting lists\n",
    "    while i < len(posting_list1) and j < len(posting_list2):\n",
    " \n",
    "\n",
    "        # if the document id of the first posting list is equal to the document id of the second posting list, add the document id to the merged posting list\n",
    "        if posting_list1[i][0] == posting_list2[j][0]:\n",
    "            merged_posting_list.append(posting_list1[i][0])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # if the document id of the first posting list is less than the document id of the second posting list, increment the first posting list index\n",
    "        elif posting_list1[i][0] < posting_list2[j][0]:\n",
    "            \n",
    "            # if the skip pointer of the first posting list is not 0 and the document id of the skip pointer is less than the document id of the second posting list, update the first posting list index\n",
    "            if posting_list1[i][1] != 0 and posting_list1[i][1] < len(posting_list1) and posting_list1[posting_list1[i][1]][0] <= posting_list2[j][0]:\n",
    "                \n",
    "                #  update the first posting list index\n",
    "                while posting_list1[i][1] != 0 and posting_list1[i][1] < len(posting_list1) and posting_list1[posting_list1[i][1]][0] <= posting_list2[j][0]:\n",
    "                    i = posting_list1[i][1]\n",
    "\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        # if the document id of the first posting list is greater than the document id of the second posting list, increment the second posting list index\n",
    "        else:\n",
    "\n",
    "            # if the skip pointer of the second posting list is not 0 and the document id of the skip pointer is less than the document id of the first posting list, update the second posting list index\n",
    "            if posting_list2[j][1] != 0 and posting_list2[j][1] < len(posting_list2) and posting_list2[posting_list2[j][1]][0] <= posting_list1[i][0]:\n",
    "                \n",
    "                # update the second posting list index\n",
    "                while posting_list2[j][1] != 0 and posting_list2[j][1] < len(posting_list2) and posting_list2[posting_list2[j][1]][0] <= posting_list1[i][0]:\n",
    "                    j = posting_list2[j][1]\n",
    "\n",
    "            else:\n",
    "                j += 1\n",
    "    \n",
    "    return merged_posting_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_skip_pointers(queries, inverted_index):\n",
    "\n",
    "    \"\"\"\n",
    "    Search for documents using the inverted index with skip pointers.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    queries (list): A list of queries\n",
    "    inverted_index (dict): An inverted index of the documents\n",
    "\n",
    "    Returns:\n",
    "    list: A list of documents that match the queries\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #  check if the queries are in the inverted index, if not return an empty list\n",
    "    for query in queries:\n",
    "        if query not in inverted_index:\n",
    "            return []\n",
    "    \n",
    "    # if only one query is given, return the posting list of the query\n",
    "    if len(queries) == 1:\n",
    "        return inverted_index[queries[0]]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    #  merge the posting lists of the first two queries\n",
    "    result = merge_posting_list_with_skip_pointers(queries[0], queries[1], inverted_index)\n",
    "\n",
    "    #  merge the posting lists of the remaining queries\n",
    "    for i in range(2, len(queries)):\n",
    "        result = merge_posting_list_with_skip_pointers(result, queries[i], inverted_index)\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_taken_with_skip_pointers(queries, inverted_index):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the time taken to search for documents using the inverted index with skip pointers.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    queries (list): A list of queries\n",
    "    inverted_index (dict): An inverted index of the documents\n",
    "\n",
    "    Returns:\n",
    "    float: The time taken to search for documents using the inverted index with skip pointers\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #  start the timer\n",
    "    start = time.time()\n",
    "    res = []\n",
    "    for i in range(100):\n",
    "        res = search_with_skip_pointers(queries, inverted_index)\n",
    "    \n",
    "    print(res)\n",
    "    \n",
    "    #  stop the timer\n",
    "    end = time.time()\n",
    "\n",
    "    #  calculate the time taken\n",
    "    time_taken = end - start\n",
    "\n",
    "    # avg time taken\n",
    "    avg_time_taken = time_taken / 100\n",
    "\n",
    "    return avg_time_taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create skip pointers for the inverted index\n",
    "inverted_index_with_skip_pointers = create_skip_pointers(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  save the inverted index with skip pointers in a text file\n",
    "with open('inverted_index_with_skip_pointers.json', 'w') as file:\n",
    "    file.write(json.dumps(inverted_index_with_skip_pointers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries_to_search = [\"Tiger AND Safari\", \"Wildlife AND Poaching\", \"Leopard AND Night\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "\n",
    "#  extract the words from the queries\n",
    "for query in sample_queries:\n",
    "    queries.append(extract_queries(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 44, 25, 65, 66, 47, 67, 48, 68, 69, 30, 80]\n",
      "[]\n",
      "[71]\n",
      "Time taken to search for documents using the inverted index with skip pointers:  [8.013248443603516e-05, 0.0, 4.952192306518555e-05]\n"
     ]
    }
   ],
   "source": [
    "#  calculate the time taken to search for documents using the inverted index with skip pointers\n",
    "time_taken = []\n",
    "for query in queries:\n",
    "    time_taken.append(time_taken_with_skip_pointers(query, inverted_index_with_skip_pointers))\n",
    "\n",
    "print(\"Time taken to search for documents using the inverted index with skip pointers: \", time_taken)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Correction\n",
    "Create a 3-gram index for the same documents crawled in part 1. Consider the modified\n",
    "queries with spelling mistakes :\n",
    "\n",
    "1. Tiger AND Saphari\n",
    "2. Wyldlife AND Poching\n",
    "3. Leprd AND Night\n",
    "\n",
    "Implement the spelling correction technique on query terms using the 3-gram index.\n",
    "\n",
    "After that run the same algorithm developed in part 2 to extract the relevant documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create the 3-gram index for the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building up of trigrams from present words\n",
    "def make_trigrams():\n",
    "\n",
    "  \"\"\" \n",
    "  This function will make trigrams from the words present in the inverted index\n",
    "\n",
    "  Parameters:\n",
    "  None\n",
    "\n",
    "  Returns:\n",
    "  trigrams (dict): A dictionary of trigrams and the words that contain them\n",
    "\n",
    "  \"\"\"\n",
    "  trigrams = {}\n",
    "\n",
    "  # traverse through the inverted index\n",
    "  for i in sorted_inverted_index.keys():\n",
    "\n",
    "    # if the length of the word is less than 3, skip it\n",
    "    if(len(i)<3):\n",
    "      continue\n",
    "\n",
    "    # if the length of the word is greater than 3, make trigrams\n",
    "    else:\n",
    "      # traverse through the word\n",
    "      j=0\n",
    "\n",
    "      # loop until the last trigram is made\n",
    "      while((j+2)<len(i)):\n",
    "\n",
    "        # make the trigram\n",
    "        tri = i[j] + i[j+1] + i[j+2]\n",
    "\n",
    "        # if the trigram is already present in the dictionary, append the word to the list of words that contain the trigram\n",
    "        if tri in trigrams.keys():\n",
    "          trigrams[tri].append(i)\n",
    "        # if the trigram is not present in the dictionary, add the trigram to the dictionary\n",
    "        else:\n",
    "          trigrams[tri] = [i]\n",
    "\n",
    "        # increment the index\n",
    "        j = j+1\n",
    "  \n",
    "\n",
    "  print('final', trigrams)\n",
    "  \n",
    "  return trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final {'ind': ['india', 'indian', 'find', 'findings', 'bigcatsindia', 'indeed', 'vindhya', 'hind', 'behind', 'mind'], 'ndi': ['india', 'indian', 'findings', 'surrounding', 'spending', 'understanding', 'bigcatsindia', 'undisputed', 'expanding', 'conditions', 'ending'], 'dia': ['india', 'indian', 'bigcatsindia'], 'tig': ['tiger', 'tigers'], 'ige': ['tiger', 'tigers'], 'ger': ['tiger', 'tigers', 'endangered', 'passenger', 'loggers', 'badger'], 'saf': ['safari', 'safaris', 'safe', 'bigcatssafari', 'safely', 'safest'], 'afa': ['safari', 'safaris', 'bigcatssafari'], 'far': ['safari', 'safaris', 'welfare', 'bigcatssafari', 'far'], 'ari': ['safari', 'safaris', 'various', 'variety', 'sharing', 'sanctuaries', 'bigcatssafari', 'variables', 'clearias', 'tadobaandhari', 'andhari', 'itineraries'], 'nat': ['national', 'nature', 'natural', 'naturalists', 'international', 'designated', 'karnataka', 'internationally', 'adityanath', 'naturalist', 'nations', 'naturally', 'jonathan', 'examination', 'supernatural', 'illuminated', 'fascinates', 'destinations', 'fortunate', 'rejuvenating', 'destination'], 'ati': ['national', 'population', 'conservation', 'international', 'guwahati', 'initiative', 'initiatives', 'classification', 'organizations', 'asiatic', 'preparation', 'corporation', 'elevation', 'relationship', 'internationally', 'validation', 'limitations', 'populations', 'navigation', 'administration', 'concentration', 'dramatically', 'declaration', 'comparatively', 'accommodation', 'creating', 'nations', 'representative', 'vegetation', 'climatic', 'precipitation', 'innovations', 'publications', 'plantations', 'spatial', 'examination', 'bhadravati', 'education', 'exploration', 'organization', 'destinations', 'location', 'accommodations', 'rejuvenating', 'conservationists', 'destination', 'compatibility', 'estimation', 'dedication'], 'tio': ['national', 'population', 'conservation', 'international', 'edition', 'pollution', 'additional', 'classification', 'organizations', 'protection', 'preparation', 'expeditions', 'corporation', 'elevation', 'composition', 'relationship', 'internationally', 'validation', 'limitations', 'functions', 'populations', 'prevention', 'section', 'navigation', 'administration', 'constitution', 'concentration', 'mentioned', 'sanction', 'declaration', 'accommodation', 'distribution', 'suggestions', 'nations', 'vegetation', 'precipitation', 'innovations', 'collection', 'publications', 'exception', 'conditions', 'addition', 'plantations', 'examination', 'education', 'definition', 'exploration', 'extinction', 'options', 'organization', 'destinations', 'location', 'mentioning', 'question', 'expedition', 'accommodations', 'conservationists', 'destination', 'estimation', 'dedication'], 'ion': ['national', 'population', 'conservation', 'region', 'international', 'edition', 'million', 'lion', 'union', 'pollution', 'additional', 'lions', 'classification', 'organizations', 'protection', 'occasion', 'preparation', 'expeditions', 'corporation', 'elevation', 'composition', 'relationship', 'millions', 'internationally', 'validation', 'limitations', 'functions', 'populations', 'prevention', 'section', 'navigation', 'administration', 'constitution', 'decision', 'concentration', 'mentioned', 'sanction', 'declaration', 'accommodation', 'distribution', 'suggestions', 'nations', 'vegetation', 'precipitation', 'billion', 'innovations', 'collection', 'publications', 'exception', 'occasions', 'conditions', 'addition', 'plantations', 'examination', 'education', 'occasional', 'definition', 'exploration', 'extinction', 'options', 'organization', 'destinations', 'location', 'mentioning', 'question', 'expedition', 'accommodations', 'conservationists', 'destination', 'version', 'estimation', 'dedication'], 'ona': ['national', 'international', 'additional', 'internationally', 'sonanadi', 'personalized', 'jonathan', 'occasional'], 'nal': ['national', 'nocturnal', 'international', 'additional', 'medicinal', 'internationally', 'personalized', 'analyses', 'signals', 'eternal', 'finally', 'occasional', 'pantanal', 'cardinals', 'internal'], 'for': ['forest', 'forests', 'informed', 'rainforest', 'efforts', 'forestry', 'form', 'forum', 'unforgettable', 'forested', 'forsyth', 'forget', 'informal', 'fortunate', 'rainforests'], 'ore': ['forest', 'forests', 'explore', 'ranthambore', 'singapore', 'rainforest', 'forestry', 'moreover', 'core', 'hardcore', 'gorewada', 'forested', 'arboreal', 'ranthambhore', 'rainforests', 'explorer'], 'res': ['forest', 'forests', 'reserves', 'reserve', 'creatures', 'hectares', 'rainforest', 'resources', 'crested', 'researchers', 'forestry', 'response', 'pastures', 'resident', 'pressure', 'impressive', 'reside', 'resorts', 'forested', 'rest', 'procedures', 'respective', 'vultures', 'representative', 'presence', 'progress', 'interesting', 'enclosures', 'threshold', 'present', 'result', 'reserved', 'residents', 'resend', 'rainforests', 'results'], 'est': ['forest', 'forests', 'best', 'highest', 'biggest', 'western', 'oldest', 'largest', 'latest', 'rainforest', 'estimated', 'crested', 'forestry', 'finest', 'test', 'lifestyle', 'hottest', 'forested', 'rest', 'suggestions', 'interesting', 'domestic', 'densest', 'establish', 'richest', 'guests', 'safest', 'invest', 'majestic', 'destinations', 'question', 'rainforests', 'destination', 'estimation'], 'wil': ['wildlife', 'wild', 'wilpattu', 'wilderness'], 'ild': ['wildlife', 'wild', 'wilderness', 'build'], 'ldl': ['wildlife'], 'dli': ['wildlife'], 'lif': ['wildlife', 'life', 'nightlife', 'lifetime', 'lifestyle', 'cliffs'], 'ife': ['wildlife', 'life', 'coniferous', 'nightlife', 'lifetime', 'lifestyle'], 'ers': ['tigers', 'drivers', 'numbers', 'biodiversity', 'diverse', 'panthers', 'researchers', 'offers', 'others', 'covers', 'waters', 'rivers', 'adverse', 'university', 'understanding', 'petersburg', 'perspective', 'watchers', 'traverse', 'photographers', 'kilometers', 'personalized', 'filters', 'poachers', 'loggers', 'members', 'swimmers', 'megadiverse', 'refers', 'meanders', 'towers', 'travelers', 'trackers', 'travellers', 'lovers', 'version', 'stakeholders'], 'par': ['park', 'parks', 'leopard', 'leopards', 'department', 'part', 'prepared', 'preparation', 'apart', 'separately', 'compared', 'comparatively', 'unparalleled', 'parkinson', 'particularly', 'departure', 'paradise'], 'ark': ['park', 'parks', 'barking', 'marked', 'embark', 'marks', 'remarkable', 'bark', 'parkinson', 'dark'], 'als': ['also', 'animals', 'mammals', 'officials', 'locals', 'signals', 'jackals', 'cardinals'], 'lso': ['also'], 'sts': ['forests', 'naturalists', 'tourists', 'scientists', 'beasts', 'environmentalists', 'enthusiasts', 'specialists', 'exists', 'guests', 'rainforests', 'lists', 'conservationists'], 'nig': ['night', 'nightjars', 'nightlife', 'nights'], 'igh': ['night', 'highest', 'sightings', 'highlight', 'highlight', 'sighting', 'high', 'sight', 'might', 'light', 'daylight', 'nightjars', 'weighs', 'nightlife', 'highlighted', 'highlighted', 'lighting', 'eight', 'nights', 'eighty', 'right', 'flights'], 'ght': ['night', 'sightings', 'highlight', 'sighting', 'sight', 'might', 'light', 'daylight', 'nightjars', 'nightlife', 'highlighted', 'lighting', 'sought', 'eight', 'nights', 'eighty', 'right', 'flights', 'ought'], 'one': ['one', 'zone', 'zones', 'ones', 'tone', 'mentioned', 'done', 'keystone', 'honey', 'alone'], 'ese': ['reserves', 'reserve', 'researchers', 'representative', 'presence', 'present', 'reserved', 'resend'], 'ser': ['reserves', 'reserve', 'conservation', 'serpent', 'observe', 'conserving', 'reserved', 'lesser', 'services', 'seriously', 'conservationists', 'browser', 'asserted'], 'erv': ['reserves', 'reserve', 'conservation', 'observe', 'conserving', 'reserved', 'services', 'conservationists'], 'rve': ['reserves', 'reserve', 'survey', 'observe', 'reserved'], 'ves': ['reserves', 'initiatives', 'drives', 'mangroves', 'gives', 'directives', 'caves', 'leaves', 'invest'], 'hom': ['home', 'homes'], 'ome': ['home', 'come', 'comes', 'kilometre', 'moment', 'become', 'kilometers', 'domestic', 'something', 'homes'], 'wor': ['world', 'network', 'work', 'working', 'worshipped', 'worldwide', 'worked'], 'orl': ['world', 'worldwide'], 'rld': ['world', 'worldwide'], 'ani': ['animals', 'animal', 'ranipur', 'organizations', 'meaning', 'surewani', 'bijrani', 'sitabani', 'organization'], 'nim': ['animals', 'animal'], 'ima': ['animals', 'animal', 'climate', 'estimated', 'image', 'primary', 'himalayan', 'ultimate', 'climatic', 'approximately', 'himalaya', 'abhimanu', 'estimation'], 'mal': ['animals', 'animal', 'mammals', 'smaller', 'himalayan', 'malabar', 'semal', 'small', 'normally', 'himalaya', 'informal'], 'rks': ['parks', 'marks', 'storks'], 'leo': ['leopard', 'leopards'], 'eop': ['leopard', 'leopards', 'people'], 'opa': ['leopard', 'leopards'], 'ard': ['leopard', 'leopards', 'board', 'hardcore', 'orchards', 'inwards', 'dashboard', 'hard', 'cardinals', 'rewarding', 'towards'], 'pop': ['population', 'popular', 'populations'], 'opu': ['population', 'popular', 'populations'], 'pul': ['population', 'popular', 'populations'], 'ula': ['population', 'popular', 'populations', 'particularly', 'peninsular'], 'lat': ['population', 'latest', 'relationship', 'populations', 'later', 'latex'], 'ian': ['indian', 'asian', 'giant'], 'bes': ['best', 'probes', 'tribes'], 'cor': ['corbett', 'according', 'corporation', 'core', 'hardcore', 'lanneacoromandelica', 'recorded', 'corporates'], 'orb': ['corbett'], 'rbe': ['corbett'], 'bet': ['corbett', 'better'], 'ett': ['corbett', 'better', 'getting', 'unforgettable', 'pretty'], 'cou': ['country', 'countries', 'encounter', 'could', 'account', 'scour', 'encouraging', 'encountering', 'counting', 'courtesy', 'couples', 'cousins', 'coupled', 'courteous', 'count'], 'oun': ['country', 'around', 'found', 'countries', 'encounter', 'ground', 'round', 'account', 'surrounding', 'encountering', 'counting', 'sound', 'astound', 'abound', 'announced', 'mountain', 'bounce', 'count'], 'unt': ['country', 'countries', 'encounter', 'account', 'encountering', 'counting', 'hunting', 'mountain', 'untamed', 'count'], 'ntr': ['country', 'countries', 'control', 'concentration', 'entry', 'central', 'contribute'], 'try': ['country', 'forestry', 'entry', 'try'], 'spe': ['species', 'special', 'spending', 'aspects', 'respective', 'specializes', 'specialists', 'perspective', 'specially', 'buteamonosperma', 'specialize', 'speaking', 'speed'], 'pec': ['species', 'expect', 'special', 'aspects', 'respective', 'specializes', 'specialists', 'perspective', 'specially', 'specialize'], 'eci': ['species', 'recieved', 'deciduous', 'decided', 'special', 'decision', 'specializes', 'specialists', 'precipitation', 'appreciate', 'specially', 'specialize', 'appreciated'], 'cie': ['species', 'recieved', 'scientists', 'policies', 'ancient'], 'ies': ['species', 'countries', 'activities', 'sanctuaries', 'policies', 'opportunities', 'cities', 'bodies', 'canopies', 'colonies', 'possibilities', 'applies', 'facilities', 'lies', 'beauties', 'communities', 'itineraries'], 'rds': ['leopards', 'birds', 'orchards', 'inwards', 'towards'], 'exp': ['experience', 'explore', 'exploring', 'expect', 'expeditions', 'experiences', 'expert', 'experienced', 'expanse', 'expanding', 'exploration', 'expedition', 'explorer'], 'xpe': ['experience', 'expect', 'expeditions', 'experiences', 'expert', 'experienced', 'expedition'], 'per': ['experience', 'per', 'percent', 'permit', 'perfect', 'experiences', 'expert', 'periphery', 'perspective', 'experienced', 'personalized', 'operates', 'temperate', 'buteamonosperma', 'viper', 'supernatural', 'permits'], 'eri': ['experience', 'characteristics', 'mysterious', 'characteristic', 'experiences', 'encountering', 'sheltering', 'periphery', 'computerized', 'entering', 'experienced', 'bheria', 'eerie', 'heritage', 'seriously'], 'rie': ['experience', 'variety', 'countries', 'sanctuaries', 'experiences', 'friendly', 'friends', 'experienced', 'eerie', 'itineraries'], 'ien': ['experience', 'scientists', 'experiences', 'friendly', 'friends', 'experienced', 'biennial', 'ancient'], 'enc': ['experience', 'pench', 'encounter', 'silence', 'experiences', 'encouraging', 'encountering', 'influence', 'experienced', 'presence', 'subsistence', 'enclosures', 'fenced', 'confidence', 'enchanting'], 'nce': ['experience', 'since', 'advance', 'concern', 'chance', 'silence', 'experiences', 'concentration', 'issuance', 'influence', 'importance', 'experienced', 'presence', 'chances', 'incessant', 'subsistence', 'fenced', 'abundance', 'confidence', 'announced', 'bounce', 'balance'], 'day': ['day', 'today', 'holiday', 'days', 'saturday', 'daylight', 'tuesday', 'daytime'], 'acc': ['according', 'account', 'acclaimed', 'accommodation', 'access', 'accommodations'], 'cco': ['according', 'account', 'accommodation', 'accommodations'], 'ord': ['according', 'order', 'lord', 'recorded', 'extraordinary'], 'rdi': ['according', 'extraordinary', 'cardinals', 'rewarding'], 'din': ['according', 'providing', 'including', 'reading', 'findings', 'surrounding', 'spending', 'understanding', 'leading', 'expanding', 'ending', 'extraordinary', 'cardinals', 'rewarding'], 'ing': ['according', 'following', 'providing', 'visiting', 'sightings', 'using', 'booking', 'singapore', 'sighting', 'exciting', 'spotting', 'including', 'enthralling', 'reading', 'starting', 'sharing', 'exploring', 'growing', 'flowing', 'thrilling', 'findings', 'travelling', 'going', 'comprising', 'things', 'protecting', 'seeing', 'helping', 'barking', 'inspiring', 'getting', 'charging', 'receiving', 'surrounding', 'watching', 'loving', 'freaking', 'assisting', 'maintaining', 'owing', 'driving', 'meaning', 'nothing', 'kingdom', 'racing', 'increasing', 'encouraging', 'meeting', 'combining', 'spending', 'encountering', 'catching', 'counting', 'sheltering', 'closing', 'lighting', 'patrolling', 'monitoring', 'conducting', 'walking', 'understanding', 'shrinking', 'leading', 'ensuring', 'nurturing', 'doubling', 'creating', 'traveling', 'entering', 'angling', 'inkling', 'exhibiting', 'thriving', 'checking', 'working', 'conserving', 'living', 'ongoing', 'camping', 'springing', 'springing', 'crouching', 'interesting', 'searching', 'suckling', 'single', 'moving', 'chousinga', 'lapwing', 'evening', 'keeping', 'expanding', 'viewing', 'studying', 'existing', 'kings', 'hunting', 'chausingha', 'looking', 'breathtaking', 'arranging', 'managing', 'ending', 'everything', 'something', 'timings', 'howling', 'chirping', 'reflecting', 'developing', 'mentioning', 'loging', 'pairing', 'varying', 'rejuvenating', 'rewarding', 'teeming', 'enchanting', 'displaying', 'speaking'], 'sta': ['state', 'states', 'stay', 'starts', 'starting', 'star', 'understanding', 'distant', 'establish', 'sustains', 'stature', 'status', 'stage', 'stars', 'started', 'stand', 'coastal', 'stakeholders'], 'tat': ['state', 'states', 'habitats', 'habitat', 'limitations', 'representative', 'vegetation', 'precipitation', 'plantations', 'stature', 'status'], 'ate': ['state', 'updated', 'states', 'climate', 'water', 'latest', 'date', 'administrated', 'designated', 'estimated', 'greater', 'dedicated', 'rate', 'create', 'waters', 'moderately', 'segregate', 'separately', 'mandated', 'ultimate', 'reiterated', 'reiterate', 'automated', 'generated', 'celebrates', 'cultivated', 'dates', 'waterholes', 'waterhole', 'operates', 'appreciate', 'situated', 'temperate', 'approximately', 'maharashtratehsil', 'later', 'created', 'illuminated', 'corporates', 'aggravated', 'category', 'fascinates', 'fortunate', 'located', 'private', 'appreciated', 'latex'], 'are': ['area', 'areas', 'rare', 'declared', 'hectares', 'narendra', 'prepared', 'square', 'welfare', 'heactare', 'hectare', 'compared', 'awareness', 'disappeared', 'share'], 'rea': ['area', 'areas', 'great', 'read', 'creatures', 'increase', 'spread', 'reading', 'greater', 'decrease', 'create', 'threat', 'break', 'freaking', 'increased', 'increasing', 'reason', 'creating', 'retreat', 'real', 'streams', 'arboreal', 'treasure', 'ready', 'created', 'treat', 'breathtaking', 'reality', 'reaffirmed'], 'zon': ['zone', 'zones'], 'ris': ['safaris', 'tourists', 'tourism', 'characteristics', 'comprising', 'characteristic', 'tourist', 'flourishes', 'frisked', 'nilgiris', 'ecotourism', 'pristine', 'rise', 'risen'], 'kno': ['known', 'know'], 'now': ['known', 'snow', 'know', 'renowned', 'nowhere'], 'own': ['known', 'towns', 'download', 'renowned', 'shown'], 'lik': ['like', 'likes', 'unlike'], 'ike': ['like', 'hikes', 'likes', 'unlike'], 'cat': ['cats', 'cat', 'classification', 'catch', 'dedicated', 'catching', 'bigcatsindia', 'bigcatssafari', 'publications', 'catterall', 'education', 'category', 'location', 'located', 'dedication'], 'ats': ['cats', 'habitats', 'ghats', 'beats', 'bigcatsindia', 'bigcatssafari'], 'con': ['conservation', 'connect', 'conducted', 'concern', 'consider', 'second', 'coniferous', 'contact', 'control', 'conflict', 'constitution', 'concentration', 'conducting', 'subcontinent', 'conserving', 'conducts', 'content', 'contours', 'considerably', 'conditions', 'contains', 'confidence', 'secondary', 'economic', 'contents', 'conservationists', 'continued', 'contribute'], 'ons': ['conservation', 'lions', 'organizations', 'consider', 'expeditions', 'response', 'relationship', 'millions', 'limitations', 'functions', 'populations', 'constitution', 'pythons', 'suggestions', 'nations', 'conserving', 'innovations', 'publications', 'occasions', 'considerably', 'conditions', 'plantations', 'options', 'destinations', 'racoons', 'accommodations', 'conservationists'], 'nse': ['conservation', 'dense', 'insects', 'response', 'sense', 'conserving', 'expanse', 'densest', 'insect', 'immense', 'intensely', 'conservationists'], 'rva': ['conservation', 'conservationists'], 'vat': ['conservation', 'elevation', 'cultivated', 'innovations', 'bhadravati', 'aggravated', 'private', 'conservationists'], 'big': ['big', 'biggest', 'bigcatsindia', 'bigcatssafari'], 'jun': ['jungle', 'jungles', 'june'], 'ung': ['jungle', 'jungles'], 'ngl': ['jungle', 'jungles', 'angling', 'single', 'english'], 'gle': ['jungle', 'jungles', 'eagle', 'toggle', 'single'], 'atu': ['nature', 'natural', 'naturalists', 'creatures', 'saturday', 'naturalist', 'naturally', 'stature', 'status', 'supernatural'], 'tur': ['nature', 'natural', 'naturalists', 'nocturnal', 'creatures', 'turns', 'adventure', 'returns', 'saturday', 'pastures', 'texture', 'disturbace', 'nurturing', 'naturalist', 'future', 'vultures', 'naturally', 'culture', 'agriculture', 'stature', 'supernatural', 'cultural', 'departure', 'turn'], 'ure': ['nature', 'creatures', 'adventure', 'ensure', 'surely', 'pastures', 'texture', 'pressure', 'surewani', 'procedures', 'future', 'vultures', 'sure', 'culture', 'treasure', 'agriculture', 'enclosures', 'stature', 'departure', 'secure'], 'pro': ['project', 'providing', 'protected', 'protection', 'proposed', 'provides', 'protecting', 'products', 'programmes', 'procedures', 'proclaimed', 'prominent', 'protect', 'projects', 'progress', 'probes', 'promise', 'prove', 'process', 'approximately', 'probably'], 'roj': ['project', 'projects'], 'oje': ['project', 'projects'], 'jec': ['project', 'projects'], 'ect': ['project', 'hectares', 'connect', 'protected', 'protection', 'expect', 'perfect', 'protecting', 'insects', 'section', 'hectare', 'directives', 'aspects', 'respective', 'perspective', 'selected', 'protect', 'projects', 'directly', 'collection', 'redirected', 'insect', 'reflecting'], 'lan': ['land', 'lanka', 'plants', 'plant', 'plankton', 'langurs', 'plan', 'grasslands', 'island', 'wetland', 'plantations', 'planted', 'woodlands', 'lanneacoromandelica', 'planet', 'llanos', 'islands', 'landmass', 'thailand', 'landscapes', 'balance'], 'and': ['land', 'uttarakhand', 'bandhavgarh', 'hand', 'andaman', 'mandated', 'bandhavgad', 'understanding', 'panda', 'grasslands', 'vande', 'island', 'wetland', 'expanding', 'ramachandran', 'tadobaandhari', 'chandrapur', 'andhari', 'meanders', 'woodlands', 'lanneacoromandelica', 'thousands', 'sands', 'pandemic', 'islands', 'stand', 'landmass', 'thailand', 'landscapes', 'dandeli'], 'ben': ['bengal', 'benefits', 'beneath'], 'eng': ['bengal', 'passenger', 'challenges', 'english'], 'nga': ['bengal', 'singapore', 'chousinga'], 'gal': ['bengal', 'illegally', 'illegal'], 'xpl': ['explore', 'exploring', 'exploration', 'explorer'], 'plo': ['explore', 'exploring', 'employment', 'exploration', 'explorer'], 'lor': ['explore', 'flora', 'exploring', 'lord', 'exploration', 'explorer'], 'amo': ['among', 'famous', 'infamous', 'amongst', 'buteamonosperma'], 'mon': ['among', 'common', 'monitoring', 'monuments', 'amongst', 'month', 'monkeys', 'montane', 'buteamonosperma', 'monitor'], 'ong': ['among', 'along', 'strong', 'long', 'ongoing', 'amongst'], 'rep': ['report', 'reported', 'prepared', 'reptiles', 'preparation', 'reptile', 'representative', 'crepe', 'repellent'], 'epo': ['report', 'reported'], 'por': ['report', 'singapore', 'opportunity', 'support', 'reported', 'important', 'transport', 'corporation', 'opportunities', 'importance', 'portrait', 'porcupine', 'corporates'], 'ort': ['report', 'north', 'opportunity', 'support', 'reported', 'efforts', 'important', 'transport', 'resorts', 'opportunities', 'importance', 'portrait', 'shortly', 'tortoise', 'northeast', 'fortunate'], 'env': ['environment', 'environmentalists'], 'nvi': ['environment', 'environmentalists'], 'vir': ['environment', 'environmentalists'], 'iro': ['environment', 'environmentalists'], 'ron': ['environment', 'strong', 'environmentalists', 'front'], 'onm': ['environment', 'environmentalists'], 'nme': ['environment', 'government', 'environmentalists'], 'men': ['environment', 'department', 'government', 'assessment', 'commitment', 'environmentalists', 'elements', 'moment', 'implement', 'mentioned', 'movements', 'employment', 'payment', 'endowments', 'monuments', 'instrumental', 'immense', 'armenia', 'tremendous', 'mentioning', 'recommend', 'advertisement', 'achievement'], 'ent': ['environment', 'department', 'different', 'percent', 'government', 'cent', 'enthralling', 'scientists', 'enter', 'assessment', 'adventure', 'serpent', 'commitment', 'magnificent', 'environmentalists', 'elements', 'recently', 'resident', 'enthusiasts', 'moment', 'implement', 'prevention', 'currently', 'concentration', 'current', 'mentioned', 'movements', 'employment', 'recent', 'urgent', 'payment', 'entering', 'entry', 'prominent', 'plenty', 'representative', 'subcontinent', 'endowments', 'monuments', 'content', 'central', 'seventh', 'seventy', 'dependent', 'adjacent', 'present', 'repellent', 'ancient', 'instrumental', 'center', 'entire', 'residents', 'essential', 'contents', 'mentioning', 'advertisement', 'achievement'], 'dep': ['department', 'depends', 'depend', 'dependent', 'departure'], 'epa': ['department', 'prepared', 'preparation', 'separately', 'departure'], 'art': ['department', 'part', 'article', 'starts', 'starting', 'heart', 'apart', 'siddhartha', 'art', 'started', 'particularly', 'departure'], 'rtm': ['department'], 'tme': ['department', 'commitment'], 'num': ['number', 'numbers', 'monuments'], 'umb': ['number', 'numbers'], 'mbe': ['number', 'numbers', 'timber', 'september', 'remember', 'samber', 'members', 'climber'], 'ber': ['number', 'numbers', 'timber', 'bera', 'september', 'october', 'remember', 'samber', 'members', 'bernie', 'fiber', 'climber'], 'goo': ['good'], 'ood': ['good', 'food', 'fuelwood', 'woods', 'livelihoods', 'livelihood', 'wood', 'woodlands', 'blood'], 'cov': ['cover', 'covers', 'covid'], 'ove': ['cover', 'government', 'mangrove', 'governed', 'covers', 'moreover', 'mangroves', 'movements', 'love', 'prove', 'lover', 'lovers'], 'ver': ['cover', 'evergreen', 'several', 'drivers', 'every', 'never', 'government', 'biodiversity', 'diverse', 'governed', 'river', 'however', 'covers', 'moreover', 'rivers', 'adverse', 'university', 'driver', 'traverse', 'megadiverse', 'everything', 'lover', 'verge', 'lovers', 'version', 'advertisement'], 'get': ['get', 'getting', 'getaway', 'altogether', 'unforgettable', 'target', 'vegetation', 'forget'], 'yea': ['years', 'year'], 'ear': ['years', 'year', 'learn', 'bear', 'heart', 'search', 'researchers', 'bears', 'disappeared', 'hear', 'clearias', 'searching', 'earned', 'nearly', 'earn', 'learned'], 'ars': ['years', 'bears', 'nightjars', 'marsh', 'stars', 'jaguars'], 'tre': ['tree', 'trees', 'kilometre', 'retreat', 'streams', 'treasure', 'treat', 'tremendous'], 'ree': ['tree', 'evergreen', 'trees', 'free', 'three', 'breeze', 'agree'], 'eas': ['areas', 'released', 'increase', 'least', 'decrease', 'please', 'season', 'beasts', 'increased', 'increasing', 'reason', 'easily', 'treasure', 'disease', 'east', 'beast', 'easy', 'northeast', 'eastern'], 'pra': ['pradesh'], 'rad': ['pradesh', 'trade', 'paradise'], 'ade': ['pradesh', 'decades', 'cheerleader', 'durgadevi', 'decade', 'made', 'trade', 'headed'], 'des': ['pradesh', 'oldest', 'designated', 'provides', 'includes', 'decades', 'guides', 'destinations', 'destination', 'described'], 'esh': ['pradesh', 'threshold'], 'spo': ['spot', 'spotted', 'spotting', 'transport', 'response', 'spots', 'spoke'], 'pot': ['spot', 'spotted', 'spotting', 'spots'], 'ura': ['natural', 'naturalists', 'encouraging', 'naturalist', 'naturally', 'satpura', 'rural', 'supernatural', 'aura', 'cultural'], 'ral': ['natural', 'naturalists', 'several', 'enthralling', 'kerala', 'naturalist', 'unparalleled', 'naturally', 'central', 'rural', 'generally', 'catterall', 'supernatural', 'cultural'], 'fol': ['following', 'followed', 'follow', 'folded', 'foliage', 'unfolds'], 'oll': ['following', 'pollution', 'followed', 'follow', 'patrolling', 'collection'], 'llo': ['following', 'allowed', 'allow', 'followed', 'follow', 'shallow', 'balloon'], 'low': ['following', 'allowed', 'allow', 'flowing', 'followed', 'follow', 'lower', 'shallow', 'low'], 'owi': ['following', 'growing', 'flowing', 'owing'], 'win': ['following', 'growing', 'flowing', 'owing', 'lapwing', 'viewing'], 'kan': ['kanha'], 'anh': ['kanha'], 'nha': ['kanha'], 'ali': ['naturalists', 'environmentalists', 'validation', 'naturalist', 'validi', 'specializes', 'specialists', 'personalized', 'specialize', 'reality', 'quality'], 'lis': ['naturalists', 'published', 'environmentalists', 'list', 'naturalist', 'specialists', 'bliss', 'establish', 'english', 'lists'], 'ist': ['naturalists', 'tourists', 'minister', 'district', 'ist', 'scientists', 'characteristics', 'administrated', 'environmentalists', 'assisting', 'existed', 'characteristic', 'administration', 'list', 'disturbace', 'naturalist', 'distribution', 'specialists', 'distant', 'tourist', 'coexist', 'subsistence', 'moist', 'existing', 'exists', 'logistics', 'registered', 'pristine', 'lists', 'conservationists', 'historic'], 'gre': ['great', 'evergreen', 'greater', 'segregate', 'progress', 'grey', 'agree'], 'eat': ['great', 'creatures', 'greater', 'create', 'threat', 'beats', 'creating', 'retreat', 'beneath', 'created', 'treat', 'breathtaking'], 'tro': ['tropical', 'subtropical', 'strong', 'patrols', 'control', 'patrolling'], 'rop': ['tropical', 'proposed', 'subtropical', 'drop', 'crops'], 'opi': ['tropical', 'subtropical', 'canopies', 'developing'], 'pic': ['tropical', 'subtropical', 'pic', 'picnic'], 'ica': ['tropical', 'geographical', 'classification', 'subtropical', 'dedicated', 'africa', 'ecological', 'biological', 'dramatically', 'publications', 'antarctica', 'mythological', 'lanneacoromandelica', 'significant', 'dedication'], 'cal': ['tropical', 'geographical', 'call', 'local', 'subtropical', 'locals', 'ecological', 'biological', 'dramatically', 'mythological', 'calls'], 'bac': ['back', 'disturbace', 'outback'], 'ack': ['back', 'package', 'black', 'track', 'outback', 'attacks', 'tracked', 'tracks', 'blackbuck', 'jackals', 'trackers', 'pack'], 'jim': ['jim'], 'see': ['see', 'seeing', 'seen', 'seek', 'seem'], 'wel': ['well', 'welfare'], 'ell': ['well', 'travelling', 'repellent', 'travellers'], 'glo': ['global', 'globally'], 'lob': ['global', 'globally'], 'oba': ['global', 'tadoba', 'globally', 'tadobaandhari', 'probably', 'nicobar'], 'bal': ['global', 'globally', 'balance', 'balloon'], 'dif': ['different', 'differ', 'holidify', 'difficulty'], 'iff': ['different', 'differ', 'cliffs', 'difficulty'], 'ffe': ['different', 'buffer', 'offers', 'offer', 'differ', 'scoffed', 'offered', 'suffered'], 'fer': ['different', 'buffer', 'offers', 'offer', 'coniferous', 'differ', 'offered', 'refers', 'suffered', 'fertility'], 'ere': ['different', 'endangered', 'anywhere', 'interesting', 'nowhere', 'offered', 'mastered', 'suffered', 'registered'], 'ren': ['different', 'narendra', 'currently', 'current', 'awareness', 'renowned'], 'fin': ['find', 'findings', 'finest', 'define', 'defined', 'finally', 'definition'], 'eve': ['evergreen', 'several', 'every', 'never', 'even', 'recieved', 'however', 'prevention', 'achieved', 'believe', 'evening', 'seventh', 'seventy', 'seven', 'everything', 'developing', 'achievement', 'achieve'], 'erg': ['evergreen', 'energy', 'merged', 'verge'], 'rgr': ['evergreen'], 'een': ['evergreen', 'seen'], 'alm': ['almost', 'palm'], 'lmo': ['almost'], 'mos': ['almost', 'topmost'], 'ost': ['almost', 'host', 'topmost', 'post', 'boost'], 'upd': ['updated'], 'pda': ['updated'], 'dat': ['updated', 'date', 'data', 'predator', 'validation', 'mandated', 'accommodation', 'dates', 'predators', 'database', 'accommodations'], 'ted': ['updated', 'spotted', 'reported', 'conducted', 'administrated', 'protected', 'designated', 'estimated', 'crested', 'dedicated', 'visited', 'existed', 'adopted', 'highlighted', 'crafted', 'mandated', 'forested', 'reiterated', 'automated', 'generated', 'united', 'excited', 'selected', 'cultivated', 'undisputed', 'painted', 'acquainted', 'appointed', 'situated', 'adapted', 'adjusted', 'planted', 'redirected', 'created', 'awaited', 'illuminated', 'started', 'aggravated', 'undoubtedly', 'located', 'limited', 'appreciated', 'asserted', 'printed'], 'ope': ['open', 'opened', 'hopefully', 'antelope', 'operates'], 'pen': ['open', 'pench', 'opened', 'depends', 'bhupender', 'serpent', 'spending', 'depend', 'dependent', 'peninsular'], 'jee': ['jeep'], 'eep': ['jeep', 'sleeps', 'keeping', 'keeps'], 'muc': ['much'], 'uch': ['much', 'crouching'], 'tim': ['time', 'timber', 'estimated', 'times', 'lifetime', 'ultimate', 'daytime', 'timings', 'meantime', 'estimation'], 'ime': ['time', 'prime', 'times', 'lifetime', 'acclaimed', 'proclaimed', 'reclaimed', 'claimed', 'daytime', 'meantime'], 'aro': ['around'], 'rou': ['around', 'ground', 'round', 'route', 'surrounding', 'coniferous', 'group', 'groups', 'crouching', 'routes', 'roughly', 'throughout'], 'und': ['around', 'found', 'ground', 'round', 'surrounding', 'fund', 'understanding', 'refundable', 'undisputed', 'sound', 'astound', 'abundance', 'abound', 'undoubtedly'], 'top': ['top', 'topography', 'topmost'], 'acr': ['across'], 'cro': ['across', 'crops', 'crouching', 'crocodile'], 'ros': ['across', 'rose', 'rossouw'], 'oss': ['across', 'possible', 'possibilities', 'rossouw', 'possums'], 'fir': ['first', 'reaffirmed'], 'irs': ['first', 'pairs'], 'rst': ['first', 'understanding'], 'tes': ['states', 'latest', 'sites', 'test', 'hottest', 'courtesy', 'celebrates', 'notes', 'dates', 'termites', 'routes', 'operates', 'corporates', 'fascinates'], 'end': ['endangered', 'narendra', 'depends', 'bhupender', 'legend', 'ends', 'spending', 'friendly', 'tend', 'friends', 'endowments', 'depend', 'dependent', 'tendu', 'splendour', 'end', 'ending', 'tremendous', 'send', 'resend', 'endowed', 'recommend'], 'nda': ['endangered', 'andaman', 'mandated', 'refundable', 'panda', 'abundance', 'secondary'], 'dan': ['endangered', 'abundance', 'dandeli'], 'ang': ['endangered', 'range', 'ranges', 'mangrove', 'change', 'mangroves', 'arrange', 'langurs', 'angling', 'orang', 'changeable', 'arranging', 'changes'], 'nge': ['endangered', 'range', 'ranges', 'change', 'passenger', 'arrange', 'challenges', 'changeable', 'changes'], 'red': ['endangered', 'declared', 'incredible', 'prepared', 'required', 'predator', 'compared', 'retired', 'disappeared', 'red', 'predators', 'offered', 'redirected', 'predominantly', 'predominant', 'mastered', 'suffered', 'registered'], 'rar': ['rare', 'itineraries'], 'ele': ['elephant', 'released', 'elephants', 'elevation', 'elements', 'celebrates', 'selected', 'unparalleled', 'travelers'], 'lep': ['elephant', 'elephants'], 'eph': ['elephant', 'elephants'], 'pha': ['elephant', 'elephants'], 'han': ['elephant', 'uttarakhand', 'rajasthan', 'elephants', 'hand', 'change', 'chance', 'thank', 'thanks', 'sulthan', 'chances', 'jonathan', 'ramachandran', 'chandrapur', 'changeable', 'changes', 'enchanting'], 'ant': ['elephant', 'ranthambore', 'elephants', 'fantastic', 'guaranteed', 'panthers', 'plants', 'important', 'plant', 'canter', 'want', 'grants', 'distant', 'militants', 'occupants', 'antoo', 'giant', 'incessant', 'antelope', 'adjutant', 'plantations', 'planted', 'antarctica', 'predominantly', 'predominant', 'panther', 'significant', 'panthera', 'wants', 'pantanal', 'ranthambhore', 'romantic', 'guarantee', 'enchanting', 'meantime'], 'bir': ['birds', 'bird'], 'ird': ['birds', 'bird', 'hirda'], 'man': ['many', 'mangrove', 'andaman', 'mangroves', 'human', 'mandated', 'chairmanship', 'managed', 'lanneacoromandelica', 'managing', 'abhimanu', 'romantic'], 'any': ['many', 'anywhere', 'company'], 'utt': ['uttarakhand', 'uttar'], 'tta': ['uttarakhand', 'uttar', 'unforgettable', 'attacks'], 'tar': ['uttarakhand', 'hectares', 'starts', 'starting', 'uttar', 'star', 'heactare', 'hectare', 'target', 'antarctica', 'taru', 'stars', 'started'], 'ara': ['uttarakhand', 'maharashtra', 'characteristics', 'guaranteed', 'preparation', 'separately', 'characteristic', 'declaration', 'comparatively', 'unparalleled', 'gujarat', 'bharat', 'maharashtratehsil', 'karaya', 'guarantee', 'paradise'], 'rak': ['uttarakhand'], 'akh': ['uttarakhand', 'ladakh', 'pakhro'], 'kha': ['uttarakhand', 'khaj'], 'hig': ['highest', 'highlight', 'high', 'highlighted'], 'ghe': ['highest'], 'hes': ['highest', 'fishes', 'flourishes', 'patches', 'richest', 'bushes', 'beaches'], 'ins': ['inside', 'inspiring', 'insects', 'institute', 'cousins', 'trains', 'parkinson', 'insect', 'sustains', 'terrapins', 'contains', 'instrumental', 'peninsular'], 'nsi': ['inside', 'density', 'consider', 'considerably'], 'sid': ['inside', 'consider', 'outside', 'resident', 'siddhartha', 'reside', 'considerably', 'aside', 'residents'], 'ide': ['inside', 'wide', 'consider', 'decided', 'outside', 'guide', 'provides', 'guides', 'ideal', 'resident', 'reside', 'guidelines', 'idea', 'considerably', 'aside', 'confidence', 'worldwide', 'residents'], 'tak': ['taken', 'takes', 'karnataka', 'take', 'breathtaking', 'stakeholders'], 'ake': ['taken', 'make', 'takes', 'makes', 'take', 'lakes', 'snakes', 'lake', 'stakeholders'], 'ken': ['taken'], 'sev': ['several', 'seventh', 'seventy', 'seven'], 'era': ['several', 'kerala', 'bera', 'moderately', 'reiterated', 'reiterate', 'generated', 'operates', 'considerably', 'generally', 'catterall', 'temperate', 'terai', 'era', 'panthera', 'itineraries'], 'tou': ['tourists', 'tour', 'tourism', 'tours', 'tourist', 'contours', 'astound', 'ecotourism'], 'our': ['tourists', 'tour', 'tourism', 'tours', 'four', 'hours', 'fourth', 'resources', 'scour', 'encouraging', 'sources', 'courtesy', 'journey', 'tourist', 'flourishes', 'contours', 'splendour', 'courteous', 'ecotourism'], 'uri': ['tourists', 'tourism', 'ensuring', 'nurturing', 'tourist', 'flourishes', 'ecotourism'], 'ead': ['read', 'spread', 'reading', 'cheerleader', 'leading', 'ahead', 'ready', 'headed'], 'cli': ['climate', 'climb', 'climatic', 'cliffs', 'climber'], 'lim': ['climate', 'glimpse', 'climb', 'limitations', 'limit', 'climatic', 'prelims', 'limits', 'climber', 'limited'], 'mat': ['climate', 'estimated', 'match', 'unmatched', 'ultimate', 'dramatically', 'automated', 'climatic', 'approximately', 'estimation'], 'var': ['various', 'variety', 'variables', 'varying'], 'rio': ['various', 'mysterious', 'seriously'], 'iou': ['various', 'mysterious', 'seriously'], 'ous': ['various', 'deciduous', 'mysterious', 'famous', 'coniferous', 'houses', 'bituminous', 'cousins', 'infamous', 'chousinga', 'thousands', 'courteous', 'tremendous', 'seriously'], 'nam': ['namely', 'name'], 'ame': ['namely', 'came', 'name', 'game', 'flame', 'untamed'], 'mel': ['namely'], 'ely': ['namely', 'surely', 'moderately', 'separately', 'comparatively', 'solely', 'absolutely', 'approximately', 'safely', 'completely', 'intensely'], 'geo': ['geographical'], 'eog': ['geographical'], 'ogr': ['geographical', 'topography', 'programmes', 'progress', 'photographers'], 'gra': ['geographical', 'topography', 'programmes', 'grants', 'grasslands', 'photographers', 'grasses', 'aggravated'], 'rap': ['geographical', 'topography', 'rapidly', 'photographers', 'chandrapur', 'terrapins', 'raptors'], 'aph': ['geographical', 'topography', 'photographers'], 'phi': ['geographical'], 'hic': ['geographical', 'thick', 'vehicles', 'thickets'], 'enj': ['enjoy', 'enjoyable'], 'njo': ['enjoy', 'enjoyable'], 'joy': ['enjoy', 'enjoyable'], 'new': ['news', 'new'], 'ews': ['news'], 'peo': ['people'], 'opl': ['people'], 'ple': ['people', 'please', 'implement', 'couples', 'temple', 'plenty', 'plethora', 'splendour', 'incomplete', 'coupled', 'completely'], 'igg': ['biggest'], 'gge': ['biggest', 'logged', 'suggestions', 'loggers'], 'ges': ['biggest', 'pages', 'largest', 'ranges', 'suggestions', 'lodges', 'challenges', 'refuges', 'changes'], 'use': ['used', 'use', 'caused', 'houses'], 'sed': ['used', 'released', 'based', 'proposed', 'increased', 'closed', 'caused', 'blessed', 'disclosed', 'discussed'], 'sin': ['since', 'using', 'singapore', 'comprising', 'increasing', 'closing', 'bigcatsindia', 'cousins', 'single', 'chousinga', 'chausingha'], 'inc': ['since', 'include', 'increase', 'incredible', 'including', 'includes', 'increased', 'increasing', 'incessant', 'incomplete', 'extinction', 'inclusive'], 'mam': ['mammals'], 'amm': ['mammals', 'programmes'], 'mma': ['mammals'], 'sno': ['snow'], 'den': ['dense', 'density', 'golden', 'resident', 'suddenly', 'densest', 'dependent', 'confidence', 'residents', 'hidden'], 'ens': ['dense', 'census', 'density', 'ensure', 'sense', 'ensuring', 'densest', 'immense', 'intensely', 'citizens'], 'ncl': ['include', 'including', 'includes', 'enclosures', 'inclusive'], 'clu': ['include', 'including', 'includes', 'exclusive', 'reclusive', 'secluded', 'inclusive'], 'lud': ['include', 'including', 'includes', 'secluded'], 'ude': ['include', 'includes', 'secluded', 'altitude', 'multitude'], 'scr': ['scrub', 'javascript', 'described'], 'cru': ['scrub'], 'rub': ['scrub'], 'aut': ['authority', 'automated', 'beauties'], 'uth': ['authority', 'south', 'youths', 'southern'], 'tho': ['authority', 'though', 'without', 'methodology', 'pythons', 'plethora', 'mythological', 'python', 'thousands', 'although'], 'hor': ['authority', 'horned', 'plethora', 'horrified', 'shortly', 'ranthambhore'], 'ori': ['authority', 'exploring', 'territorial', 'monitoring', 'origin', 'historic'], 'rit': ['authority', 'territorial', 'heritage'], 'ity': ['authority', 'biodiversity', 'opportunity', 'density', 'city', 'enormity', 'adityanath', 'university', 'vicinity', 'fertility', 'reality', 'compatibility', 'quality'], 'dec': ['declared', 'deciduous', 'decided', 'decrease', 'decades', 'decision', 'declaration', 'decade'], 'ecl': ['declared', 'reclusive', 'declaration', 'reclaimed', 'secluded'], 'cla': ['declared', 'classified', 'classification', 'class', 'claim', 'classes', 'acclaimed', 'declaration', 'proclaimed', 'reclaimed', 'claimed', 'clash'], 'lar': ['declared', 'popular', 'large', 'largest', 'declaration', 'particularly', 'peninsular'], 'noc': ['nocturnal'], 'oct': ['nocturnal', 'october'], 'ctu': ['nocturnal', 'sanctuary', 'sanctuaries', 'actual'], 'urn': ['nocturnal', 'turns', 'returns', 'journey', 'turn'], 'rna': ['nocturnal', 'international', 'karnataka', 'internationally', 'jhirna', 'supernatural', 'eternal', 'internal'], 'mad': ['madhya', 'made', 'mahuamadhuca'], 'adh': ['madhya', 'mahuamadhuca'], 'dhy': ['madhya', 'vindhya'], 'hya': ['madhya', 'vindhya'], 'thr': ['thrill', 'enthralling', 'thrilling', 'threat', 'thriving', 'threshold', 'throughout', 'three'], 'hri': ['thrill', 'shri', 'thrilling', 'shrinking', 'thriving', 'gauhri', 'shrine'], 'ril': ['thrill', 'thrilling', 'april'], 'ill': ['thrill', 'million', 'still', 'hills', 'thrilling', 'killed', 'millions', 'illegally', 'fill', 'villus', 'billion', 'illegal', 'village', 'illuminated', 'skilled', 'villas'], 'les': ['jungles', 'reptiles', 'less', 'couples', 'vehicles', 'variables', 'blessed', 'waterholes', 'lesser', 'styles'], 'com': ['come', 'common', 'com', 'comprising', 'commitment', 'comes', 'composition', 'compared', 'combining', 'comparatively', 'accommodation', 'computerized', 'become', 'company', 'incomplete', 'completely', 'communities', 'accommodations', 'recommend', 'compatibility'], 'sig': ['sign', 'sightings', 'sighting', 'sight', 'designated', 'signals', 'significant'], 'ign': ['sign', 'designated', 'signals', 'significant'], 'flo': ['flora', 'flowing', 'flourishes'], 'ora': ['flora', 'corporation', 'memorable', 'plethora', 'orang', 'corporates', 'exploration'], 'fau': ['fauna'], 'aun': ['fauna', 'launched', 'launch'], 'una': ['fauna', 'fortunate'], 'ees': ['trees'], 'sur': ['survey', 'ensure', 'surely', 'surrounding', 'survival', 'pressure', 'surewani', 'ensuring', 'sure', 'treasure', 'enclosures', 'surya'], 'urv': ['survey', 'survival'], 'vey': ['survey'], 'pan': ['panna', 'panthers', 'panda', 'occupants', 'expanse', 'expanding', 'panther', 'company', 'panthera', 'pandemic', 'pantanal'], 'ann': ['panna', 'banned', 'lanneacoromandelica', 'announced'], 'nna': ['panna', 'jhinna'], 'ran': ['ranthambore', 'range', 'ranges', 'ranipur', 'guaranteed', 'transport', 'arrange', 'bijrani', 'grants', 'orang', 'ramachandran', 'arranging', 'ranthambhore', 'guarantee'], 'nth': ['ranthambore', 'enthralling', 'panthers', 'enthusiasts', 'month', 'seventh', 'panther', 'panthera', 'ranthambhore'], 'tha': ['ranthambore', 'rajasthan', 'thank', 'thanks', 'sulthan', 'siddhartha', 'jonathan', 'ranthambhore', 'thailand'], 'ham': ['ranthambore', 'hampi', 'ranthambhore'], 'amb': ['ranthambore', 'bamboo', 'samber', 'sambar', 'ranthambhore'], 'mbo': ['ranthambore', 'bamboo'], 'bor': ['ranthambore', 'arboreal', 'born'], 'all': ['call', 'enthralling', 'allowed', 'allow', 'globally', 'equally', 'smaller', 'internationally', 'illegally', 'dramatically', 'shallow', 'unparalleled', 'naturally', 'challenges', 'generally', 'specially', 'catterall', 'small', 'normally', 'finally', 'officially', 'calls', 'balloon'], 'dri': ['drivers', 'drives', 'driving', 'driver'], 'riv': ['drivers', 'river', 'drives', 'driving', 'rivers', 'driver', 'thriving', 'strive', 'privacy', 'private'], 'ive': ['drivers', 'biodiversity', 'diverse', 'initiative', 'initiatives', 'give', 'river', 'active', 'exclusive', 'civet', 'live', 'five', 'civets', 'drives', 'rivers', 'impressive', 'elusive', 'gives', 'reclusive', 'directives', 'university', 'comparatively', 'driver', 'respective', 'perspective', 'representative', 'inquisitiveness', 'livelihoods', 'megadiverse', 'livelihood', 'strive', 'inclusive'], 'min': ['minister', 'administrated', 'administration', 'bituminous', 'prominent', 'mine', 'examination', 'predominantly', 'predominant', 'illuminated', 'timings', 'mind', 'teeming'], 'ini': ['minister', 'initiative', 'initiatives', 'administrated', 'maintaining', 'administration', 'combining', 'vicinity', 'definition'], 'nis': ['minister', 'administrated', 'administration', 'conservationists'], 'ste': ['minister', 'western', 'ecosystem', 'mysterious', 'crested', 'existed', 'forested', 'systems', 'waste', 'system', 'subsistence', 'adjusted', 'mastered', 'registered', 'eastern'], 'ter': ['minister', 'western', 'international', 'better', 'water', 'encounter', 'characteristics', 'enter', 'greater', 'mysterious', 'territorial', 'waters', 'canter', 'internationally', 'characteristic', 'twitter', 'encountering', 'sheltering', 'reiterated', 'reiterate', 'petersburg', 'computerized', 'entering', 'kilometers', 'terrain', 'interface', 'waterholes', 'interesting', 'termites', 'termite', 'waterhole', 'filters', 'term', 'catterall', 'later', 'terrapins', 'terai', 'eternal', 'mastered', 'center', 'registered', 'terms', 'internal', 'eastern', 'internet'], 'tot': ['total'], 'ota': ['total', 'pivotal', 'notably'], 'tal': ['total', 'environmentalists', 'pivotal', 'capital', 'chital', 'instrumental', 'coastal'], 'vis': ['visit', 'visiting', 'visitors', 'visited', 'visible'], 'isi': ['visit', 'visiting', 'comprising', 'visitors', 'visited', 'visible', 'decision', 'inquisitiveness'], 'sit': ['visit', 'visiting', 'biodiversity', 'density', 'sites', 'visitors', 'visited', 'composition', 'university', 'sitabani', 'inquisitiveness', 'situated', 'site'], 'ban': ['bandhavgarh', 'banned', 'banks', 'bandhavgad', 'sitabani'], 'ndh': ['bandhavgarh', 'bandhavgad', 'vindhya', 'tadobaandhari', 'andhari'], 'dha': ['bandhavgarh', 'siddhartha', 'bandhavgad', 'tadobaandhari', 'andhari', 'dhauda'], 'hav': ['bandhavgarh', 'bandhavgad'], 'avg': ['bandhavgarh', 'bandhavgad'], 'vga': ['bandhavgarh', 'bandhavgad'], 'gar': ['bandhavgarh', 'nahargarh'], 'arh': ['bandhavgarh', 'nahargarh', 'pachmarhi'], 'cre': ['creatures', 'increase', 'incredible', 'crested', 'decrease', 'create', 'increased', 'increasing', 'creating', 'created', 'crepe'], 'cen': ['census', 'percent', 'cent', 'magnificent', 'recently', 'concentration', 'recent', 'central', 'scenic', 'adjacent', 'center', 'scenes'], 'nsu': ['census', 'ensure', 'ensuring', 'peninsular'], 'sus': ['census', 'sustains'], 'nch': ['pench', 'launched', 'launch', 'enchanting'], 'rov': ['providing', 'mangrove', 'provides', 'mangroves', 'prove'], 'ovi': ['providing', 'provides', 'loving', 'moving', 'covid'], 'vid': ['providing', 'provides', 'avid', 'covid'], 'idi': ['providing', 'validi', 'holidify'], 'wes': ['western'], 'ern': ['western', 'international', 'government', 'governed', 'concern', 'internationally', 'bernie', 'southern', 'supernatural', 'eternal', 'wilderness', 'internal', 'eastern', 'internet'], 'reg': ['region', 'segregate', 'registered'], 'egi': ['region', 'begin', 'registered'], 'gio': ['region'], 'int': ['international', 'maintaining', 'internationally', 'quaint', 'interface', 'interesting', 'painted', 'acquainted', 'appointed', 'internal', 'intensely', 'internet', 'print', 'printed'], 'nte': ['international', 'encounter', 'enter', 'guaranteed', 'canter', 'internationally', 'encountering', 'entering', 'content', 'interface', 'interesting', 'antelope', 'painted', 'acquainted', 'appointed', 'planted', 'center', 'contents', 'internal', 'intensely', 'guarantee', 'internet', 'printed'], 'hab': ['habitats', 'habitat'], 'abi': ['habitats', 'habitat', 'cabinet', 'sabi'], 'bit': ['habitats', 'habitat', 'bituminous', 'exhibiting'], 'ita': ['habitats', 'habitat', 'limitations', 'capital', 'sitabani', 'suitable', 'precipitation', 'militants', 'chital', 'heritage'], 'iti': ['visiting', 'edition', 'exciting', 'initiative', 'initiatives', 'additional', 'activities', 'expeditions', 'composition', 'opportunities', 'cities', 'exhibiting', 'inquisitiveness', 'possibilities', 'facilities', 'conditions', 'addition', 'definition', 'communities', 'expedition', 'itineraries', 'citizens'], 'tin': ['visiting', 'sightings', 'sighting', 'exciting', 'spotting', 'starting', 'protecting', 'getting', 'assisting', 'meeting', 'counting', 'lighting', 'conducting', 'creating', 'subcontinent', 'exhibiting', 'interesting', 'existing', 'hunting', 'reflecting', 'extinction', 'destinations', 'rejuvenating', 'pristine', 'itineraries', 'destination', 'enchanting', 'continued'], 'hti': ['sightings', 'sighting', 'lighting'], 'ngs': ['sightings', 'findings', 'things', 'amongst', 'kings', 'timings'], 'ups': ['upsc', 'groups'], 'psc': ['upsc'], 'ery': ['every', 'bathery', 'periphery', 'everything'], 'fou': ['found', 'four', 'fourth'], 'pla': ['play', 'places', 'plants', 'plant', 'plankton', 'played', 'plan', 'place', 'unsplash', 'plantations', 'planted', 'planet', 'displaying'], 'lay': ['play', 'played', 'himalayan', 'himalaya', 'delayed', 'displaying'], 'lea': ['learn', 'released', 'least', 'please', 'broadleaved', 'cheerleader', 'leading', 'clearias', 'leaf', 'leaves', 'learned', 'herculean'], 'arn': ['learn', 'karnataka', 'arnold', 'earned', 'earn', 'learned'], 'rel': ['released', 'surely', 'relationship', 'reliable', 'prelims', 'squirrel'], 'ase': ['released', 'increase', 'based', 'decrease', 'please', 'increased', 'base', 'database', 'disease'], 'usi': ['using', 'exclusive', 'enthusiasts', 'elusive', 'reclusive', 'cousins', 'chousinga', 'chausingha', 'inclusive'], 'pub': ['published', 'publications'], 'ubl': ['published', 'doubling', 'publications', 'double'], 'bli': ['published', 'doubling', 'publications', 'bliss', 'establish'], 'ish': ['published', 'fishes', 'flourishes', 'swish', 'establish', 'fish', 'english', 'wish'], 'she': ['published', 'sheltering', 'fishes', 'sheer', 'flourishes', 'durshet', 'bushes'], 'hed': ['published', 'launched', 'unmatched', 'schedule', 'beheda'], 'dis': ['district', 'disturbace', 'distribution', 'disappeared', 'distant', 'undisputed', 'disease', 'disclose', 'disclosed', 'discussed', 'paradise', 'displaying'], 'str': ['district', 'administrated', 'forestry', 'strong', 'administration', 'distribution', 'streams', 'striped', 'instrumental', 'strive'], 'tri': ['district', 'countries', 'trip', 'distribution', 'tribes', 'striped', 'strive', 'contribute'], 'ric': ['district', 'price', 'africa', 'priced', 'agriculture', 'richest', 'crickets', 'historic'], 'ict': ['district', 'conflict'], 'ism': ['tourism', 'ecotourism'], 'off': ['officials', 'offers', 'offer', 'offroad', 'officer', 'scoffed', 'offered', 'officially'], 'ffi': ['officials', 'officer', 'officially', 'difficulty', 'reaffirmed'], 'fic': ['officials', 'classification', 'magnificent', 'officer', 'officially', 'significant', 'difficulty'], 'ici': ['officials', 'medicinal', 'policies', 'vicinity', 'medicine', 'officially'], 'cia': ['officials', 'special', 'specializes', 'specialists', 'appreciate', 'specially', 'specialize', 'officially', 'appreciated'], 'ial': ['officials', 'territorial', 'special', 'specializes', 'specialists', 'biennial', 'specially', 'spatial', 'specialize', 'officially', 'essential'], 'feb': ['february'], 'ebr': ['february', 'celebrates', 'zebra'], 'bru': ['february', 'brush'], 'rua': ['february'], 'uar': ['february', 'sanctuary', 'guaranteed', 'january', 'sanctuaries', 'square', 'jaguars', 'guarantee'], 'ary': ['february', 'sanctuary', 'january', 'primary', 'secondary', 'extraordinary', 'varying'], 'typ': ['types', 'type'], 'ype': ['types', 'type'], 'pes': ['types', 'landscapes'], 'nor': ['north', 'norms', 'enormity', 'normally', 'northeast'], 'rth': ['north', 'fourth', 'siddhartha', 'northeast'], 'gro': ['grow', 'mangrove', 'ground', 'growing', 'mangroves', 'group', 'groups'], 'row': ['grow', 'growing', 'browser'], 'ene': ['benefits', 'opened', 'generated', 'awareness', 'inquisitiveness', 'energy', 'generally', 'beneath', 'scenes'], 'nef': ['benefits'], 'efi': ['benefits', 'define', 'defined', 'befits', 'definition'], 'fit': ['benefits', 'befits'], 'its': ['benefits', 'fruits', 'limits', 'befits', 'permits'], 'guw': ['guwahati'], 'uwa': ['guwahati'], 'wah': ['guwahati'], 'aha': ['guwahati', 'maharashtra', 'nahargarh', 'maharashtratehsil'], 'hat': ['guwahati', 'ghats', 'chat'], 'boo': ['booking', 'book', 'bamboo', 'boost', 'boon'], 'ook': ['booking', 'look', 'book', 'brooks', 'looking'], 'oki': ['booking', 'looking'], 'kin': ['booking', 'barking', 'freaking', 'kingdom', 'walking', 'shrinking', 'checking', 'working', 'kings', 'parkinson', 'looking', 'breathtaking', 'speaking'], 'ned': ['opened', 'governed', 'banned', 'horned', 'defined', 'mentioned', 'renowned', 'earned', 'learned'], 'ott': ['spotted', 'spotting', 'hottest'], 'tte': ['spotted', 'better', 'twitter', 'hottest', 'catterall'], 'iet': ['variety'], 'ety': ['variety'], 'owl': ['owls', 'howls', 'howling'], 'wls': ['owls', 'howls'], 'ghl': ['highlight', 'highlighted', 'roughly'], 'hli': ['highlight', 'highlighted'], 'lig': ['highlight', 'light', 'daylight', 'highlighted', 'lighting', 'flights'], 'omm': ['common', 'commitment', 'accommodation', 'communities', 'accommodations', 'recommend'], 'mmo': ['common', 'accommodation', 'accommodations'], 'key': ['key', 'monkeys', 'keystone'], 'fsi': ['fsi', 'dgfsi'], 'uni': ['unique', 'opportunity', 'union', 'opportunities', 'university', 'united', 'communities'], 'niq': ['unique'], 'iqu': ['unique'], 'que': ['unique', 'question'], 'pag': ['pages'], 'age': ['pages', 'package', 'image', 'managed', 'foliage', 'village', 'stage', 'heritage'], 'erc': ['percent', 'herculean'], 'rce': ['percent', 'resources', 'sources'], 'ncr': ['increase', 'incredible', 'increased', 'increasing'], 'rol': ['role', 'patrols', 'control', 'patrolling'], 'ole': ['role', 'whole', 'solely', 'waterholes', 'waterhole', 'dhole'], 'nev': ['never'], 'edi': ['edition', 'incredible', 'editors', 'dedicated', 'expeditions', 'medicinal', 'redirected', 'medicine', 'medium', 'expedition', 'dedication'], 'dit': ['edition', 'additional', 'editors', 'expeditions', 'adityanath', 'conditions', 'addition', 'expedition'], 'gov': ['government', 'governed', 'gov'], 'rnm': ['government'], 'shr': ['shri', 'shrinking', 'shrine'], 'erm': ['permit', 'termites', 'termite', 'term', 'buteamonosperma', 'terms', 'permits'], 'rmi': ['permit', 'enormity', 'termites', 'termite', 'permits'], 'mit': ['permit', 'commitment', 'enormity', 'limitations', 'limit', 'limits', 'termites', 'termite', 'permits', 'limited'], 'tod': ['today', 'toddy'], 'oda': ['today', 'accommodation', 'accommodations'], 'zoo': ['zoo', 'zoos'], 'nag': ['nagpur', 'managed', 'managing'], 'agp': ['nagpur'], 'gpu': ['nagpur'], 'pur': ['nagpur', 'ranipur', 'jaipur', 'satpura', 'chandrapur'], 'buf': ['buffer'], 'uff': ['buffer', 'stuff', 'suffered'], 'nes': ['zones', 'witness', 'finest', 'ones', 'lines', 'unesco', 'guidelines', 'awareness', 'inquisitiveness', 'wilderness', 'scenes'], 'asi': ['asia', 'asian', 'occasion', 'asiatic', 'increasing', 'basis', 'easily', 'occasions', 'aside', 'occasional'], 'sia': ['asia', 'asian', 'asiatic', 'enthusiasts'], 'gap': ['singapore'], 'apo': ['singapore'], 'raj': ['rajasthan'], 'aja': ['rajasthan'], 'jas': ['rajasthan'], 'ast': ['rajasthan', 'fantastic', 'least', 'beasts', 'last', 'pastures', 'enthusiasts', 'fast', 'waste', 'astound', 'east', 'beast', 'mastered', 'past', 'northeast', 'coastline', 'eastern', 'coastal'], 'sth': ['rajasthan'], 'pac': ['package', 'impact', 'space', 'pachmarhi', 'pack'], 'cka': ['package', 'jackals'], 'kag': ['package'], 'exc': ['exciting', 'exclusive', 'excited', 'exception'], 'xci': ['exciting', 'excited'], 'cit': ['exciting', 'city', 'excited', 'cities', 'cite', 'citizens'], 'urs': ['tours', 'hours', 'langurs', 'contours', 'durshet'], 'mil': ['million', 'family', 'millions', 'militants', 'jhilmil', 'tamil'], 'lli': ['million', 'enthralling', 'thrilling', 'travelling', 'millions', 'patrolling', 'billion'], 'lio': ['million', 'lion', 'lions', 'millions', 'billion'], 'hec': ['hectares', 'hectare', 'checking'], 'cta': ['hectares', 'heactare', 'hectare'], 'inf': ['informed', 'rainforest', 'influence', 'infamous', 'informal', 'rainforests'], 'nfo': ['informed', 'rainforest', 'unforgettable', 'informal', 'unfolds', 'rainforests'], 'orm': ['informed', 'form', 'norms', 'enormity', 'normally', 'informal'], 'rme': ['informed', 'warmer', 'armenia', 'reaffirmed'], 'med': ['informed', 'medicinal', 'acclaimed', 'proclaimed', 'reclaimed', 'claimed', 'medicine', 'medium', 'untamed', 'reaffirmed'], 'bio': ['biodiversity', 'biological'], 'iod': ['biodiversity'], 'odi': ['biodiversity', 'modi', 'bodies', 'crocodile', 'wodier'], 'div': ['biodiversity', 'diverse', 'megadiverse'], 'rsi': ['biodiversity', 'university', 'version'], 'dib': ['incredible'], 'ibl': ['incredible', 'visible', 'possible'], 'ble': ['incredible', 'visible', 'possible', 'available', 'unforgettable', 'reliable', 'refundable', 'enjoyable', 'memorable', 'suitable', 'remarkable', 'variables', 'tabled', 'blessed', 'changeable', 'double', 'table'], 'who': ['whole'], 'hol': ['whole', 'holiday', 'waterholes', 'waterhole', 'threshold', 'holds', 'mythological', 'dhole', 'holidify', 'stakeholders'], 'nts': ['elephants', 'plants', 'elements', 'movements', 'grants', 'endowments', 'monuments', 'militants', 'occupants', 'wants', 'residents', 'contents'], 'ven': ['even', 'adventure', 'prevention', 'inquisitiveness', 'evening', 'seventh', 'seventy', 'seven', 'rejuvenating'], 'fan': ['fantastic'], 'nta': ['fantastic', 'environmentalists', 'maintaining', 'contact', 'representative', 'montane', 'plantations', 'antarctica', 'contains', 'instrumental', 'mountain', 'untamed', 'pantanal'], 'tas': ['fantastic'], 'sti': ['fantastic', 'still', 'characteristics', 'estimated', 'assisting', 'characteristic', 'constitution', 'suggestions', 'institute', 'interesting', 'stilt', 'domestic', 'existing', 'logistics', 'majestic', 'destinations', 'question', 'pristine', 'destination', 'estimation'], 'tic': ['fantastic', 'article', 'characteristics', 'asiatic', 'characteristic', 'dramatically', 'climatic', 'motichur', 'domestic', 'antarctica', 'logistics', 'majestic', 'particularly', 'exotic', 'romantic'], 'lac': ['places', 'black', 'place', 'blackbuck'], 'ace': ['places', 'peace', 'space', 'disturbace', 'race', 'place', 'interface', 'adjacent'], 'ces': ['places', 'resources', 'experiences', 'sources', 'success', 'access', 'chances', 'incessant', 'process', 'services'], 'tti': ['spotting', 'getting'], 'arg': ['large', 'largest', 'charging', 'nahargarh', 'target', 'marg'], 'rge': ['large', 'largest', 'unforgettable', 'urgent', 'target', 'forget', 'merged', 'verge'], 'tay': ['stay'], 'old': ['oldest', 'golden', 'told', 'arnold', 'folded', 'threshold', 'holds', 'unfolds', 'stakeholders'], 'lde': ['oldest', 'golden', 'folded', 'wilderness', 'stakeholders'], 'rse': ['diverse', 'adverse', 'traverse', 'megadiverse'], 'udi': ['including'], 'bla': ['black', 'blackbuck'], 'bea': ['bear', 'beasts', 'bears', 'beats', 'bean', 'beast', 'beauties', 'beaches'], 'mah': ['maharashtra', 'maharashtratehsil', 'mahuamadhuca'], 'har': ['maharashtra', 'sharing', 'characteristics', 'charging', 'characteristic', 'hardcore', 'siddhartha', 'nahargarh', 'orchards', 'bharat', 'tadobaandhari', 'maharashtratehsil', 'andhari', 'charm', 'hard', 'share'], 'ras': ['maharashtra', 'grasslands', 'maharashtratehsil', 'grasses'], 'ash': ['maharashtra', 'unsplash', 'clash', 'dashboard', 'maharashtratehsil'], 'sht': ['maharashtra', 'maharashtratehsil'], 'htr': ['maharashtra', 'maharashtratehsil'], 'tra': ['maharashtra', 'administrated', 'travelling', 'track', 'transport', 'travel', 'administration', 'concentration', 'trail', 'traveling', 'traverse', 'trains', 'central', 'tracked', 'tracks', 'portrait', 'trade', 'maharashtratehsil', 'travels', 'extraordinary', 'travelers', 'trackers', 'travellers'], 'del': ['delhi', 'guidelines', 'lanneacoromandelica', 'delayed', 'dandeli'], 'elh': ['delhi'], 'lhi': ['delhi'], 'loc': ['local', 'locals', 'location', 'located'], 'oca': ['local', 'locals', 'location', 'located'], 'gha': ['ghats', 'chausingha'], 'pri': ['prime', 'price', 'comprising', 'april', 'primary', 'priced', 'springing', 'privacy', 'private', 'pristine', 'print', 'printed'], 'rim': ['prime', 'primary'], 'nar': ['narendra', 'extraordinary'], 'ndr': ['narendra', 'ramachandran', 'chandrapur'], 'dra': ['narendra', 'dramatically', 'ramachandran', 'chandrapur', 'bhadravati'], 'mod': ['modi', 'moderately', 'accommodation', 'accommodations', 'mode'], 'sai': ['said', 'usain'], 'aid': ['said'], 'spr': ['spread', 'springing'], 'pre': ['spread', 'prepared', 'preparation', 'predator', 'pressure', 'prevention', 'impressive', 'premium', 'representative', 'presence', 'precipitation', 'prelims', 'pre', 'supreme', 'predators', 'appreciate', 'present', 'predominantly', 'predominant', 'pretty', 'appreciated'], 'tad': ['tadoba', 'tadobaandhari'], 'ado': ['tadoba', 'adopted', 'tadobaandhari'], 'dob': ['tadoba', 'tadobaandhari'], 'rip': ['trip', 'periphery', 'striped', 'javascript'], 'ias': ['ias', 'enthusiasts', 'clearias'], 'ful': ['full', 'unlawful', 'hopefully', 'wonderful'], 'ull': ['full', 'hopefully'], 'roy': ['royal'], 'oya': ['royal', 'enjoyable'], 'yal': ['royal', 'yala'], 'det': ['details', 'detail'], 'eta': ['details', 'getaway', 'detail', 'vegetation', 'cheetah'], 'tai': ['details', 'maintaining', 'detail', 'captain', 'sustains', 'contains', 'mountain', 'obtain'], 'ail': ['details', 'email', 'available', 'detail', 'kukrail', 'trail', 'civilsdaily', 'mail', 'thailand'], 'ils': ['details', 'civilsdaily'], 'wit': ['within', 'witness', 'without', 'twitter'], 'ith': ['within', 'without', 'faith', 'either'], 'thi': ['within', 'thick', 'things', 'nothing', 'thickets', 'everything', 'something', 'think'], 'hin': ['within', 'jhinna', 'things', 'rhino', 'watching', 'nothing', 'catching', 'crouching', 'hind', 'searching', 'everything', 'something', 'behind', 'think'], 'opp': ['opportunity', 'opportunities'], 'ppo': ['opportunity', 'support', 'opportunities', 'appointed'], 'rtu': ['opportunity', 'opportunities', 'nurturing', 'departure', 'fortunate'], 'tun': ['opportunity', 'opportunities', 'fortunate'], 'nit': ['opportunity', 'initiative', 'initiatives', 'monitoring', 'opportunities', 'united', 'vicinity', 'monitor', 'definition', 'communities'], 'hra': ['enthralling'], 'lin': ['enthralling', 'thrilling', 'travelling', 'online', 'lines', 'link', 'patrolling', 'doubling', 'guidelines', 'traveling', 'angling', 'inkling', 'linked', 'uplink', 'suckling', 'howling', 'coastline'], 'mak': ['make', 'makes'], 'rti': ['article', 'starting', 'particularly', 'fertility', 'advertisement'], 'icl': ['article', 'vehicles'], 'cle': ['article', 'vehicles', 'clearias', 'cycle'], 'til': ['still', 'reptiles', 'reptile', 'stilt', 'fertility'], 'wat': ['water', 'watch', 'waters', 'watching', 'watchers', 'waterholes', 'waterhole'], 'hil': ['hills', 'jhilmil'], 'lls': ['hills', 'calls'], 'way': ['way', 'always', 'wayanad', 'getaway', 'away'], 'dry': ['dry'], 'dee': ['deer', 'indeed'], 'eer': ['deer', 'cheerleader', 'sheer', 'eerie'], 'imb': ['timber', 'climb', 'climber'], 'ass': ['assam', 'classified', 'classification', 'class', 'assessment', 'passenger', 'assisting', 'classes', 'pass', 'assess', 'grasslands', 'passes', 'grasses', 'landmass', 'asserted'], 'ssa': ['assam', 'bigcatssafari', 'incessant'], 'sam': ['assam', 'samber', 'sambar'], 'sup': ['support', 'supreme', 'supernatural'], 'upp': ['support'], 'nco': ['encounter', 'encouraging', 'encountering', 'incomplete'], 'rte': ['reported', 'courtesy', 'courteous', 'started', 'asserted'], 'nio': ['union'], 'adi': ['reading', 'adityanath', 'leading', 'sonanadi', 'megadiverse', 'paradise'], 'ema': ['email', 'remarkable', 'semal'], 'mai': ['email', 'maintaining', 'mainly', 'mail'], 'tia': ['initiative', 'initiatives', 'spatial', 'essential'], 'iat': ['initiative', 'initiatives', 'asiatic', 'appreciate', 'appreciated'], 'tiv': ['initiative', 'initiatives', 'activities', 'active', 'directives', 'comparatively', 'respective', 'perspective', 'representative', 'cultivated', 'inquisitiveness'], 'ker': ['kerala', 'trackers'], 'ala': ['kerala', 'yala', 'udawalawe', 'himalayan', 'dhikala', 'malabar', 'halad', 'salai', 'palas', 'himalaya', 'balance'], 'roa': ['road', 'broadleaved', 'broadly', 'offroad', 'roads'], 'oad': ['road', 'broadleaved', 'broadly', 'offroad', 'roads', 'download'], 'rec': ['recieved', 'receiving', 'recently', 'reclusive', 'directives', 'recent', 'precipitation', 'directly', 'reclaimed', 'appreciate', 'redirected', 'recorded', 'recommend', 'appreciated'], 'iev': ['recieved', 'achieved', 'believe', 'achievement', 'achieve'], 'ved': ['recieved', 'broadleaved', 'achieved', 'reserved'], 'lau': ['launched', 'launch'], 'unc': ['launched', 'launch', 'functions', 'announced', 'bounce'], 'che': ['launched', 'researchers', 'cheerleader', 'unmatched', 'schedule', 'watchers', 'checking', 'poachers', 'patches', 'richest', 'beaches', 'cheetah'], 'rts': ['starts', 'efforts', 'resorts'], 'sci': ['scientists', 'fascinates'], 'nti': ['scientists', 'prevention', 'counting', 'mentioned', 'subcontinent', 'hunting', 'entire', 'essential', 'mentioning', 'romantic', 'enchanting', 'meantime', 'continued'], 'tis': ['scientists', 'advertisement'], 'pol': ['pollution', 'policy', 'policies'], 'llu': ['pollution', 'villus', 'illuminated'], 'lut': ['pollution', 'absolutely'], 'uti': ['pollution', 'constitution', 'distribution', 'beauties'], 'rai': ['rainforest', 'kukrail', 'trail', 'rain', 'raises', 'trains', 'terrain', 'portrait', 'terai', 'rainforests'], 'ain': ['rainforest', 'maintaining', 'maintaining', 'chain', 'usain', 'rain', 'trains', 'terrain', 'captain', 'quaint', 'painted', 'acquainted', 'mainly', 'sustains', 'contains', 'mountain', 'rainforests', 'obtain'], 'sou': ['south', 'resources', 'sought', 'sources', 'sound', 'rossouw', 'southern'], 'out': ['south', 'outside', 'without', 'route', 'youths', 'outback', 'routes', 'southern', 'throughout'], 'alp': ['alpine'], 'lpi': ['alpine', 'helping'], 'pin': ['alpine', 'helping', 'pin', 'camping', 'porcupine', 'keeping', 'terrapins', 'chirping', 'developing'], 'ine': ['alpine', 'online', 'finest', 'define', 'defined', 'lines', 'cabinet', 'guidelines', 'prominent', 'subcontinent', 'mine', 'nine', 'porcupine', 'shrine', 'medicine', 'coastline', 'pristine', 'itineraries'], 'wid': ['wide', 'worldwide'], 'soi': ['soil'], 'oil': ['soil', 'oil'], 'opo': ['topography', 'proposed'], 'pog': ['topography'], 'phy': ['topography'], 'las': ['classified', 'classification', 'class', 'last', 'classes', 'unsplash', 'clash', 'palas', 'villas'], 'ssi': ['classified', 'classification', 'assisting', 'possible', 'impressive', 'possibilities'], 'sif': ['classified', 'classification'], 'ifi': ['classified', 'classification', 'magnificent', 'horrified', 'deified', 'significant'], 'fie': ['classified', 'fields', 'horrified', 'deified'], 'ied': ['classified', 'horrified', 'deified'], 'bas': ['based', 'basis', 'base', 'database'], 'epe': ['depends', 'depend', 'dependent', 'crepe', 'repellent'], 'nds': ['depends', 'ends', 'friends', 'grasslands', 'woodlands', 'thousands', 'bonds', 'sands', 'islands', 'landscapes'], 'cid': ['deciduous', 'decided'], 'idu': ['deciduous'], 'duo': ['deciduous'], 'uou': ['deciduous'], 'ngr': ['mangrove', 'mangroves'], 'add': ['additional', 'addition'], 'ddi': ['additional', 'addition'], 'onn': ['connect'], 'nne': ['connect', 'banned', 'lanneacoromandelica'], 'nec': ['connect'], 'adv': ['advance', 'adventure', 'adverse', 'advertisement'], 'dva': ['advance'], 'van': ['advance', 'vande'], 'anc': ['advance', 'sanctuary', 'chance', 'sanctuaries', 'issuance', 'sanction', 'importance', 'chances', 'abundance', 'ancient', 'balance'], 'rns': ['turns', 'returns'], 'hou': ['hours', 'though', 'without', 'houses', 'chousinga', 'throughout', 'thousands', 'although'], 'ond': ['conducted', 'second', 'wonder', 'conducting', 'conducts', 'conditions', 'gond', 'secondary', 'bonds', 'wonderful'], 'ndu': ['conducted', 'conducting', 'conducts', 'tendu'], 'duc': ['conducted', 'products', 'conducting', 'conducts', 'education'], 'uct': ['conducted', 'products', 'conducting', 'conducts'], 'cte': ['conducted', 'characteristics', 'protected', 'characteristic', 'selected', 'redirected'], 'owe': ['allowed', 'followed', 'however', 'lower', 'showed', 'towers', 'endowed'], 'wed': ['allowed', 'followed', 'showed', 'endowed'], 'eye': ['eye', 'eyes'], 'act': ['activities', 'characteristics', 'impact', 'active', 'contact', 'characteristic', 'heactare', 'actual'], 'cti': ['activities', 'protection', 'active', 'protecting', 'functions', 'section', 'directives', 'conducting', 'sanction', 'respective', 'perspective', 'collection', 'antarctica', 'reflecting', 'extinction'], 'ivi': ['activities', 'receiving', 'driving', 'civilsdaily', 'thriving', 'living', 'civil'], 'vit': ['activities'], 'tie': ['activities', 'opportunities', 'cities', 'possibilities', 'facilities', 'beauties', 'communities'], 'foo': ['food'], 'gir': ['gir', 'nilgiris'], 'loo': ['look', 'looking', 'blood', 'balloon'], 'sha': ['sharing', 'shallow', 'share'], 'rin': ['sharing', 'exploring', 'inspiring', 'encountering', 'sheltering', 'monitoring', 'shrinking', 'ensuring', 'nurturing', 'entering', 'springing', 'shrine', 'pairing', 'print', 'printed'], 'cha': ['characteristics', 'change', 'chance', 'charging', 'chain', 'characteristic', 'chairmanship', 'orchards', 'challenges', 'chances', 'ramachandran', 'chandrapur', 'chausingha', 'changeable', 'changes', 'charm', 'chat', 'enchanting'], 'rac': ['characteristics', 'track', 'characteristic', 'racing', 'race', 'tracked', 'tracks', 'racoons', 'trackers'], 'ics': ['characteristics', 'logistics'], 'non': ['non'], 'log': ['logged', 'blog', 'methodology', 'ecological', 'login', 'biological', 'loggers', 'mythological', 'logistics', 'loging'], 'ogg': ['logged', 'toggle', 'loggers'], 'ged': ['logged', 'managed', 'merged'], 'ito': ['editors', 'territorial', 'visitors', 'monitoring', 'monitor'], 'tor': ['editors', 'territorial', 'visitors', 'predator', 'monitoring', 'torn', 'predators', 'torrid', 'storks', 'monitor', 'tortoise', 'raptors', 'historic'], 'ors': ['editors', 'visitors', 'forsyth', 'predators', 'worshipped', 'raptors', 'moors'], 'set': ['set'], 'rne': ['governed', 'horned', 'journey', 'earned', 'wilderness', 'learned', 'internet'], 'adm': ['administrated', 'administration'], 'dmi': ['administrated', 'administration'], 'rat': ['administrated', 'preparation', 'rate', 'corporation', 'moderately', 'separately', 'administration', 'concentration', 'reiterated', 'reiterate', 'declaration', 'comparatively', 'generated', 'celebrates', 'gujarat', 'bharat', 'operates', 'temperate', 'maharashtratehsil', 'corporates', 'exploration'], 'rot': ['protected', 'protection', 'protecting', 'protect'], 'ote': ['protected', 'protection', 'note', 'protecting', 'hotels', 'notes', 'protect', 'hotel'], 'tec': ['protected', 'protection', 'protecting', 'protect'], 'esi': ['designated', 'resident', 'reside', 'residents'], 'gna': ['designated', 'signals'], 'nip': ['ranipur'], 'ipu': ['ranipur', 'jaipur'], 'san': ['sanctuary', 'sanctuaries', 'sanction', 'incessant', 'thousands', 'sands'], 'nct': ['sanctuary', 'sanctuaries', 'functions', 'sanction', 'extinction'], 'tua': ['sanctuary', 'sanctuaries', 'situated', 'actual'], 'urt': ['fourth', 'courtesy', 'nurturing', 'courteous'], 'lly': ['globally', 'equally', 'internationally', 'illegally', 'hopefully', 'dramatically', 'naturally', 'generally', 'specially', 'normally', 'finally', 'officially'], 'jan': ['jan', 'january'], 'boa': ['board', 'dashboard'], 'oar': ['board', 'dashboard'], 'hea': ['heart', 'healthy', 'heactare', 'ahead', 'hear', 'headed', 'northeast'], 'mig': ['might'], 'giv': ['give', 'gives'], 'eso': ['resources', 'resorts'], 'urc': ['resources', 'sources'], 'etc': ['etc'], 'onc': ['concern', 'concentration'], 'cer': ['concern', 'officer'], 'eco': ['ecosystem', 'second', 'ecological', 'eco', 'become', 'recorded', 'secondary', 'ecotourism', 'economic', 'recommend'], 'cos': ['ecosystem'], 'osy': ['ecosystem'], 'sys': ['ecosystem', 'systems', 'system'], 'yst': ['ecosystem', 'mysterious', 'systems', 'system', 'keystone'], 'tem': ['ecosystem', 'september', 'systems', 'system', 'temple', 'temperate'], 'org': ['organizations', 'unforgettable', 'forget', 'organization'], 'rga': ['organizations', 'nahargarh', 'durgadevi', 'organization'], 'gan': ['organizations', 'began', 'organization'], 'niz': ['organizations', 'organization'], 'iza': ['organizations', 'organization'], 'zat': ['organizations', 'organization'], 'bhu': ['bhupender'], 'hup': ['bhupender'], 'upe': ['bhupender', 'supernatural'], 'nde': ['bhupender', 'wonder', 'understanding', 'indeed', 'vande', 'dependent', 'meanders', 'lanneacoromandelica', 'pandemic', 'wonderful', 'dandeli'], 'der': ['bhupender', 'consider', 'fodder', 'cheerleader', 'moderately', 'order', 'wonder', 'understanding', 'considerably', 'meanders', 'wilderness', 'wonderful', 'stakeholders'], 'yad': ['yadav'], 'ada': ['yadav', 'ladakh', 'gorewada', 'adapted'], 'dav': ['yadav'], 'gol': ['golden'], 'ite': ['sites', 'visited', 'reiterated', 'reiterate', 'united', 'excited', 'termites', 'termite', 'awaited', 'quite', 'limited', 'site', 'cite'], 'oli': ['holiday', 'policy', 'policies', 'foliage', 'holidify'], 'lid': ['holiday', 'validation', 'validi', 'holidify'], 'ida': ['holiday', 'validation', 'noida'], 'pos': ['proposed', 'composition', 'possible', 'post', 'possibilities', 'possums'], 'ose': ['proposed', 'rose', 'closed', 'choose', 'lose', 'disclose', 'disclosed'], 'nyw': ['anywhere'], 'ywh': ['anywhere'], 'whe': ['anywhere', 'nowhere'], 'her': ['anywhere', 'panthers', 'researchers', 'others', 'bathery', 'altogether', 'periphery', 'watchers', 'photographers', 'nowhere', 'herd', 'poachers', 'southern', 'bheria', 'panther', 'heritage', 'panthera', 'herbs', 'either', 'mother', 'another', 'herculean'], 'cam': ['came', 'camp', 'camping'], 'jhi': ['jhinna', 'jhirna', 'jhilmil'], 'inn': ['jhinna', 'innovations'], 'amp': ['camp', 'hampi', 'camping'], 'sec': ['second', 'insects', 'section', 'secluded', 'insect', 'secondary', 'sec', 'secure'], 'ice': ['price', 'magnificent', 'priced', 'officer', 'rejoiced', 'services'], 'two': ['two', 'network'], 'gua': ['guaranteed', 'jaguars', 'guarantee'], 'tee': ['guaranteed', 'guarantee', 'teeming'], 'eed': ['guaranteed', 'needed', 'indeed', 'need', 'needs', 'speed'], 'kes': ['takes', 'makes', 'lakes', 'snakes', 'hikes', 'likes'], 'not': ['note', 'nothing', 'notes', 'notably', 'another'], 'sse': ['assessment', 'passenger', 'classes', 'assess', 'blessed', 'passes', 'grasses', 'russel', 'lesser', 'essential', 'discussed', 'asserted'], 'ses': ['assessment', 'classes', 'assess', 'houses', 'raises', 'analyses', 'passes', 'densest', 'grasses', 'noises'], 'ess': ['assessment', 'witness', 'pressure', 'assess', 'impressive', 'less', 'awareness', 'success', 'progress', 'access', 'blessed', 'incessant', 'inquisitiveness', 'process', 'wilderness', 'lesser', 'essential'], 'ssm': ['assessment'], 'sme': ['assessment'], 'anu': ['january', 'abhimanu'], 'nua': ['january'], 'sep': ['september', 'separately'], 'ept': ['september', 'reptiles', 'reptile', 'exception'], 'pte': ['september', 'adopted', 'adapted'], 'emb': ['september', 'embark', 'remember', 'members'], 'ded': ['decided', 'dedicated', 'needed', 'secluded', 'folded', 'recorded', 'headed', 'dedication'], 'lok': ['lok'], 'chi': ['chief', 'watching', 'catching', 'achieved', 'crouching', 'searching', 'chimur', 'chital', 'chirping', 'achievement', 'achieve'], 'hie': ['chief', 'achieved', 'achievement', 'achieve'], 'ief': ['chief'], 'sri': ['sri'], 'ank': ['lanka', 'thank', 'banks', 'thanks', 'plankton'], 'nka': ['lanka'], 'sea': ['search', 'researchers', 'season', 'searching', 'disease'], 'arc': ['search', 'researchers', 'march', 'searching', 'antarctica'], 'rch': ['search', 'researchers', 'march', 'orchards', 'searching'], 'rav': ['travelling', 'travel', 'traveling', 'traverse', 'bhadravati', 'travels', 'aggravated', 'travelers', 'travellers'], 'ave': ['travelling', 'broadleaved', 'travel', 'traveling', 'traverse', 'caves', 'leaves', 'travels', 'travelers', 'travellers'], 'vel': ['travelling', 'travel', 'comparatively', 'traveling', 'livelihoods', 'livelihood', 'velvet', 'travels', 'developing', 'travelers', 'travellers'], 'hos': ['host'], 'the': ['panthers', 'others', 'bathery', 'altogether', 'southern', 'panther', 'panthera', 'northeast', 'either', 'mother', 'another'], 'dve': ['adventure', 'adverse', 'advertisement'], 'ntu': ['adventure'], 'atc': ['catch', 'watch', 'watching', 'match', 'unmatched', 'catching', 'watchers', 'patches'], 'tch': ['catch', 'watch', 'watching', 'match', 'unmatched', 'catching', 'watchers', 'patches'], 'gli': ['glimpse', 'angling', 'english'], 'imp': ['glimpse', 'impact', 'important', 'implement', 'impressive', 'importance'], 'mps': ['glimpse', 'pumps'], 'pse': ['glimpse'], 'mys': ['mysterious'], 'goi': ['going', 'ongoing'], 'oin': ['going', 'ongoing', 'appointed'], 'vie': ['view', 'viewing'], 'iew': ['view', 'viewing'], 'max': ['maximum'], 'axi': ['maximum', 'axis'], 'xim': ['maximum', 'approximately'], 'imu': ['maximum', 'chimur'], 'mum': ['maximum'], 'kar': ['karnataka', 'karaya'], 'ata': ['karnataka', 'data', 'database', 'patagonia'], 'aka': ['karnataka'], 'eal': ['healthy', 'ideal', 'real', 'arboreal', 'reality'], 'alt': ['healthy', 'altogether', 'altitude', 'although'], 'lth': ['healthy', 'sulthan', 'although'], 'thy': ['healthy'], 'erp': ['serpent'], 'rpe': ['serpent'], 'eag': ['eagle'], 'agl': ['eagle'], 'ntc': ['ntca'], 'tca': ['ntca'], 'mpa': ['impact', 'compared', 'comparatively', 'company', 'compatibility'], 'how': ['however', 'shows', 'show', 'showed', 'shown', 'howls', 'howling'], 'wev': ['however'], 'oul': ['could', 'would'], 'uld': ['could', 'would'], 'err': ['territorial', 'terrain', 'terrapins'], 'rri': ['territorial', 'horrified', 'torrid'], 'ria': ['territorial', 'variables', 'clearias', 'bheria', 'myriad'], 'uts': ['outside', 'uts'], 'tsi': ['outside', 'bigcatsindia'], 'jai': ['jaipur'], 'aip': ['jaipur'], 'gui': ['guide', 'guides', 'guidelines'], 'uid': ['guide', 'guides', 'guidelines'], 'ick': ['thick', 'flickr', 'thickets', 'crickets'], 'ecr': ['decrease'], 'sho': ['shows', 'show', 'showed', 'threshold', 'shortly', 'shown'], 'ows': ['shows', 'browser'], 'sub': ['subtropical', 'subcontinent', 'subsistence'], 'ubt': ['subtropical', 'undoubtedly'], 'btr': ['subtropical'], 'omp': ['comprising', 'composition', 'compared', 'comparatively', 'computerized', 'company', 'incomplete', 'completely', 'compatibility'], 'mpr': ['comprising', 'impressive'], 'eff': ['efforts'], 'ffo': ['efforts'], 'mag': ['image', 'magnificent'], 'mmi': ['commitment'], 'itm': ['commitment'], 'occ': ['occasion', 'occupants', 'occasions', 'occasional'], 'cca': ['occasion', 'occasions', 'occasional'], 'cas': ['occasion', 'occasions', 'occasional'], 'sio': ['occasion', 'decision', 'occasions', 'occasional', 'version'], 'mes': ['times', 'comes', 'programmes', 'domestic', 'homes'], 'pti': ['reptiles', 'reptile', 'exception', 'options'], 'ile': ['reptiles', 'mobile', 'agile', 'silence', 'reptile', 'crocodile', 'mobileno'], 'onl': ['online'], 'nli': ['online', 'unlike'], 'oug': ['though', 'sought', 'roughly', 'throughout', 'enough', 'ought', 'although'], 'ugh': ['though', 'sought', 'roughly', 'throughout', 'enough', 'ought', 'although'], 'une': ['june', 'unesco'], 'fam': ['family', 'famous', 'infamous'], 'ami': ['family', 'examination', 'tamil'], 'ily': ['family', 'civilsdaily', 'easily'], 'lic': ['policy', 'policies', 'conflict', 'publications', 'flickr', 'lanneacoromandelica'], 'icy': ['policy'], 'mar': ['march', 'marked', 'primary', 'marks', 'remarkable', 'pachmarhi', 'marsh', 'marg'], 'mpo': ['important', 'composition', 'importance'], 'rta': ['important', 'importance'], 'tan': ['important', 'understanding', 'importance', 'distant', 'militants', 'adjutant', 'montane', 'pantanal', 'stand'], 'dic': ['dedicated', 'medicinal', 'medicine', 'dedication'], 'wou': ['would'], 'alw': ['always'], 'lwa': ['always'], 'ays': ['always', 'days'], 'fai': ['fair', 'faith'], 'air': ['fair', 'pairs', 'chairmanship', 'pairing'], 'aso': ['season', 'reason'], 'son': ['season', 'reason', 'sonanadi', 'personalized', 'parkinson'], 'let': ['let', 'plethora', 'incomplete', 'completely'], 'erf': ['perfect', 'interface', 'wonderful'], 'rfe': ['perfect'], 'fec': ['perfect'], 'itn': ['witness'], 'tne': ['witness'], 'arr': ['array', 'arrange', 'arranging'], 'rra': ['array', 'hurray', 'arrange', 'terrain', 'terrapins', 'arranging'], 'ray': ['array', 'hurray', 'karaya'], 'roc': ['rocky', 'procedures', 'proclaimed', 'process', 'crocodile'], 'ock': ['rocky', 'pockets'], 'cky': ['rocky', 'lucky'], 'rhi': ['rhino', 'pachmarhi'], 'ino': ['rhino', 'bituminous'], 'oth': ['others', 'sloth', 'nothing', 'mother', 'another'], 'apa': ['apart'], 'eca': ['decades', 'decade'], 'cad': ['decades', 'decade'], 'eei': ['seeing'], 'ein': ['seeing'], 'mou': ['famous', 'infamous', 'mountain'], 'hit': ['hit', 'chital'], 'kil': ['killed', 'kilometre', 'kilometers', 'skilled'], 'lle': ['killed', 'smaller', 'illegally', 'unparalleled', 'challenges', 'collection', 'illegal', 'repellent', 'skilled', 'travellers'], 'led': ['killed', 'unparalleled', 'tabled', 'furled', 'ruled', 'coupled', 'skilled'], 'squ': ['square', 'squirrel'], 'qua': ['square', 'equally', 'equal', 'quaint', 'acquainted', 'quality'], 'hel': ['helping', 'held', 'sheltering', 'dhela'], 'elp': ['helping'], 'tea': ['team', 'teak', 'buteamonosperma', 'tea'], 'eam': ['team', 'streams', 'buteamonosperma'], 'ped': ['expeditions', 'worshipped', 'striped', 'expedition'], 'slo': ['sloth'], 'lot': ['sloth'], 'alo': ['along', 'alone'], 'lon': ['along', 'long', 'colonies', 'alone'], 'bar': ['barking', 'embark', 'malabar', 'bar', 'bark', 'sambar', 'nicobar'], 'rki': ['barking', 'working', 'parkinson'], 'orn': ['horned', 'torn', 'born'], 'gau': ['gaur', 'gauhri'], 'aur': ['gaur', 'aura'], 'xcl': ['exclusive'], 'lus': ['exclusive', 'elusive', 'reclusive', 'villus', 'inclusive'], 'siv': ['exclusive', 'impressive', 'elusive', 'reclusive', 'inclusive'], 'equ': ['equally', 'required', 'equal'], 'ual': ['equally', 'equal', 'actual', 'quality'], 'civ': ['civet', 'civets', 'civilsdaily', 'civil'], 'vet': ['civet', 'civets', 'velvet'], 'sky': ['sky'], 'fue': ['fuelwood'], 'uel': ['fuelwood'], 'elw': ['fuelwood'], 'lwo': ['fuelwood'], 'woo': ['fuelwood', 'woods', 'wooly', 'wood', 'woodlands'], 'fod': ['fodder'], 'odd': ['fodder', 'toddy'], 'dde': ['fodder', 'suddenly', 'hidden'], 'bam': ['bamboo'], 'rod': ['products'], 'odu': ['products'], 'cts': ['products', 'insects', 'aspects', 'projects', 'conducts'], 'blo': ['blog', 'blood'], 'liv': ['live', 'living', 'livelihoods', 'livelihood'], 'leg': ['legend', 'illegally', 'legs', 'illegal'], 'ege': ['legend', 'vegetation'], 'gen': ['legend', 'urgent', 'generated', 'generally'], 'got': ['got'], 'nks': ['banks', 'thanks'], 'cin': ['medicinal', 'racing', 'vicinity', 'medicine', 'fascinates'], 'ina': ['medicinal', 'examination', 'predominantly', 'predominant', 'finally', 'illuminated', 'extraordinary', 'fascinates', 'destinations', 'cardinals', 'destination'], 'wer': ['lower', 'towers'], 'agn': ['magnificent'], 'gni': ['magnificent', 'significant'], 'nif': ['magnificent', 'coniferous', 'significant'], 'ods': ['woods', 'livelihoods'], 'awe': ['awe', 'udawalawe'], 'nsp': ['inspiring', 'transport', 'unsplash'], 'spi': ['inspiring'], 'pir': ['inspiring'], 'iri': ['inspiring', 'nilgiris', 'pairing'], 'ret': ['returns', 'retired', 'retreat', 'pretty'], 'etu': ['returns'], 'oft': ['often'], 'fte': ['often', 'crafted'], 'ten': ['often', 'tend', 'content', 'subsistence', 'tendu', 'contents', 'intensely'], 'fiv': ['five'], 'afr': ['africa'], 'fri': ['africa', 'friendly', 'friends', 'frisked', 'frigid'], 'dam': ['andaman'], 'ama': ['andaman', 'dramatically', 'ramachandran', 'mahuamadhuca'], 'net': ['network', 'cabinet', 'planet', 'internet'], 'etw': ['network'], 'ork': ['network', 'work', 'working', 'storks', 'worked'], 'mob': ['mobile', 'mobileno'], 'obi': ['mobile', 'mobileno'], 'bil': ['mobile', 'billion', 'possibilities', 'mobileno', 'compatibility'], 'req': ['required'], 'qui': ['required', 'squirrel', 'inquisitiveness', 'quite'], 'uir': ['required', 'squirrel'], 'ire': ['required', 'directives', 'retired', 'directly', 'redirected', 'entire'], 'iel': ['fields'], 'eld': ['fields', 'held'], 'lds': ['fields', 'holds', 'unfolds'], 'rke': ['marked', 'worked'], 'ked': ['marked', 'linked', 'frisked', 'tracked', 'asked', 'worked'], 'ans': ['transport', 'chairmanship', 'expanse'], 'orp': ['corporation', 'corporates'], 'rpo': ['corporation', 'corporates'], 'ksr': ['ksrtc'], 'srt': ['ksrtc'], 'rtc': ['ksrtc'], 'aya': ['wayanad', 'himalayan', 'karaya', 'himalaya'], 'yan': ['wayanad', 'adityanath', 'himalayan', 'kalyan'], 'ana': ['wayanad', 'adityanath', 'sonanadi', 'managed', 'analyses', 'managing', 'pantanal'], 'nad': ['wayanad', 'sonanadi', 'nadu'], 'fla': ['flak', 'flame'], 'lak': ['flak', 'lakes', 'lake'], 'hre': ['threat', 'threshold', 'three'], 'ilo': ['kilometre', 'kilometers'], 'lom': ['kilometre', 'kilometers', 'colombia'], 'met': ['kilometre', 'methodology', 'kilometers', 'something'], 'etr': ['kilometre', 'retreat'], 'sul': ['sulthan', 'result', 'peninsular', 'results'], 'ult': ['sulthan', 'ultimate', 'vultures', 'cultivated', 'culture', 'agriculture', 'cult', 'result', 'cultural', 'multitude', 'difficulty', 'results'], 'bat': ['bathery'], 'ath': ['bathery', 'adityanath', 'jonathan', 'beneath', 'breathtaking'], 'pon': ['ponkuzhy', 'response', 'upon'], 'onk': ['ponkuzhy', 'monkeys'], 'nku': ['ponkuzhy'], 'kuz': ['ponkuzhy'], 'uzh': ['ponkuzhy'], 'zhy': ['ponkuzhy'], 'ute': ['route', 'ute', 'computerized', 'institute', 'absolutely', 'undisputed', 'routes', 'buteamonosperma', 'contribute'], 'rgi': ['charging'], 'gin': ['charging', 'login', 'encouraging', 'begin', 'springing', 'origin', 'arranging', 'managing', 'loging'], 'pas': ['passenger', 'pastures', 'pass', 'passes', 'past'], 'sen': ['passenger', 'sense', 'representative', 'presence', 'present', 'essential', 'send', 'resend', 'risen'], 'beg': ['began', 'begin'], 'ega': ['began', 'segregate', 'illegally', 'illegal', 'megadiverse'], 'sat': ['saturday', 'satpuda', 'satpura'], 'urd': ['saturday'], 'rda': ['saturday', 'hirda'], 'lai': ['claim', 'acclaimed', 'proclaimed', 'reclaimed', 'claimed', 'salai'], 'aim': ['claim', 'acclaimed', 'proclaimed', 'reclaimed', 'claimed'], 'ece': ['receiving', 'recently', 'recent'], 'cei': ['receiving'], 'eiv': ['receiving'], 'vin': ['receiving', 'loving', 'driving', 'thriving', 'conserving', 'living', 'vindhya', 'moving'], 'esp': ['response', 'respective'], 'sco': ['scour', 'unesco', 'scoffed'], 'nkt': ['plankton'], 'kto': ['plankton'], 'ton': ['plankton', 'tone', 'keystone'], 'stu': ['pastures', 'disturbace', 'stuff', 'study', 'studying'], 'lad': ['ladakh', 'halad'], 'dak': ['ladakh'], 'lev': ['elevation'], 'eva': ['elevation'], 'maj': ['major', 'majestic'], 'ajo': ['major'], 'jor': ['major'], 'lem': ['elements', 'implement'], 'eme': ['elements', 'implement', 'movements', 'remember', 'supreme', 'tremendous', 'advertisement', 'achievement'], 'def': ['define', 'defined', 'definition'], 'osi': ['composition', 'closing'], 'mor': ['moreover', 'memorable'], 'reo': ['moreover'], 'eov': ['moreover'], 'upo': ['upon'], 'ela': ['relationship', 'dhela', 'delayed'], 'nsh': ['relationship', 'chairmanship'], 'shi': ['relationship', 'chairmanship', 'shiva', 'worshipped'], 'hip': ['relationship', 'chairmanship', 'worshipped'], 'urr': ['surrounding', 'hurray', 'currently', 'current'], 'rro': ['surrounding'], 'oni': ['coniferous', 'monitoring', 'colonies', 'monitor', 'patagonia', 'mentioning', 'conservationists'], 'ero': ['coniferous'], 'bro': ['broadleaved', 'broadly', 'brooks', 'browser'], 'adl': ['broadleaved', 'broadly', 'sadly'], 'dle': ['broadleaved'], 'eav': ['broadleaved', 'leaves'], 'wet': ['wet', 'wetland'], 'apr': ['april'], 'can': ['canter', 'canopies', 'canopy', 'significant'], 'cto': ['october'], 'tob': ['october'], 'obe': ['october', 'probes'], 'nee': ['needed', 'need', 'needs'], 'ede': ['needed'], 'bre': ['break', 'breathtaking', 'breeze'], 'eak': ['break', 'freaking', 'teak', 'speaking'], 'wan': ['want', 'bhawan', 'surewani', 'wants'], 'hee': ['cheerleader', 'sheer', 'cheetah'], 'erl': ['cheerleader'], 'rle': ['cheerleader', 'furled'], 'pea': ['peace', 'disappeared', 'speaking'], 'eac': ['peace', 'heactare', 'lanneacoromandelica', 'beaches'], 'lov': ['loving', 'love', 'lover', 'lovers'], 'spa': ['space', 'spatial', 'spa'], 'dea': ['ideal', 'idea'], 'taw': ['getaway'], 'awa': ['getaway', 'jawai', 'bhawan', 'udawalawe', 'awareness', 'away', 'awaited'], 'fre': ['freaking', 'free'], 'aki': ['freaking', 'breathtaking', 'speaking'], 'ayl': ['daylight'], 'yli': ['daylight'], 'ntl': ['recently', 'currently', 'predominantly'], 'tly': ['recently', 'currently', 'directly', 'shortly', 'predominantly'], 'pat': ['patrols', 'patrolling', 'wilpattu', 'spatial', 'patches', 'patagonia', 'compatibility'], 'atr': ['patrols', 'patrolling'], 'ols': ['patrols'], 'sis': ['assisting', 'basis', 'subsistence'], 'nin': ['maintaining', 'meaning', 'combining', 'nine', 'evening', 'peninsular', 'mentioning'], 'unl': ['unlawful', 'unlike'], 'nla': ['unlawful'], 'law': ['unlawful', 'udawalawe'], 'awf': ['unlawful'], 'wfu': ['unlawful'], 'obs': ['observe'], 'bse': ['observe'], 'rus': ['rusty', 'rustle', 'russel', 'brush'], 'ust': ['rusty', 'customized', 'custom', 'rustle', 'adjusted', 'sustains', 'must'], 'sty': ['rusty', 'lifestyle', 'styles'], 'ets': ['civets', 'pockets', 'thickets', 'crickets', 'meets'], 'htj': ['nightjars'], 'tja': ['nightjars'], 'jar': ['nightjars', 'gujarat'], 'sib': ['visible', 'possible', 'possibilities'], 'ape': ['apex', 'landscapes'], 'pex': ['apex'], 'eda': ['predator', 'predators', 'beheda'], 'ato': ['predator', 'predators'], 'hai': ['chain', 'chairmanship', 'thailand'], 'opm': ['topmost'], 'pmo': ['topmost'], 'dog': ['dog'], 'eek': ['seek'], 'gam': ['game'], 'exi': ['existed', 'coexist', 'existing', 'exists'], 'xis': ['existed', 'coexist', 'axis', 'existing', 'exists'], 'sma': ['smaller', 'small'], 'ler': ['smaller', 'travelers', 'travellers'], 'wei': ['weighs'], 'eig': ['weighs', 'eight', 'eighty'], 'ghs': ['weighs'], 'fro': ['front', 'offroad'], 'ont': ['front', 'contact', 'control', 'subcontinent', 'content', 'contours', 'month', 'montane', 'contains', 'contents', 'continued', 'contribute'], 'rvi': ['survival', 'conserving', 'services'], 'viv': ['survival'], 'iva': ['survival', 'shiva', 'cultivated', 'privacy', 'private'], 'val': ['survival', 'validation', 'validi'], 'agi': ['agile', 'encouraging', 'managing'], 'gil': ['agile', 'vigil'], 'tac': ['contact', 'attacks'], 'dgf': ['dgfsi'], 'gfs': ['dgfsi'], 'dot': ['dot'], 'nic': ['nic', 'picnic', 'scenic', 'nicobar'], 'dly': ['broadly', 'rapidly', 'friendly', 'sadly', 'undoubtedly'], 'ode': ['moderately', 'abode', 'mode'], 'tel': ['moderately', 'separately', 'absolutely', 'hotels', 'antelope', 'approximately', 'hotel', 'completely'], 'dop': ['adopted'], 'opt': ['adopted', 'options', 'opt'], 'rms': ['norms', 'terms'], 'seg': ['segregate'], 'egr': ['segregate'], 'gat': ['segregate', 'navigation'], 'eno': ['enormity', 'renowned', 'mobileno', 'enough'], 'imi': ['limitations', 'limit', 'limits', 'timings', 'limited'], 'eth': ['methodology', 'altogether', 'plethora', 'something'], 'hod': ['methodology'], 'odo': ['methodology'], 'dol': ['methodology'], 'olo': ['methodology', 'ecological', 'biological', 'solo', 'colonies', 'mythological', 'colombia'], 'ogy': ['methodology'], 'tex': ['texture', 'latex'], 'ext': ['texture', 'extraordinary', 'extinction'], 'xtu': ['texture'], 'col': ['ecological', 'collection', 'colonies', 'colombia'], 'ogi': ['ecological', 'login', 'yogi', 'biological', 'mythological', 'logistics', 'loging'], 'gic': ['ecological', 'biological', 'mythological'], 'fun': ['functions', 'fund', 'refundable', 'fun'], 'aye': ['played', 'delayed'], 'yed': ['played', 'delayed'], 'oru': ['forum'], 'rum': ['forum', 'instrumental'], 'rdc': ['hardcore'], 'dco': ['hardcore'], 'thu': ['enthusiasts', 'thus'], 'hus': ['enthusiasts', 'thus'], 'htl': ['nightlife'], 'tli': ['nightlife', 'coastline'], 'mea': ['meaning', 'meanders', 'meantime'], 'ean': ['meaning', 'meanders', 'bean', 'herculean', 'meantime'], 'lto': ['altogether'], 'tog': ['altogether', 'toggle', 'photographers'], 'oge': ['altogether'], 'rde': ['order', 'recorded'], 'sun': ['sun'], 'goe': ['goes'], 'oes': ['goes'], 'dro': ['drop'], 'sil': ['silence', 'easily', 'tehsil', 'maharashtratehsil'], 'len': ['silence', 'plenty', 'challenges', 'repellent', 'splendour', 'mobileno'], 'wai': ['wait', 'jawai', 'awaited'], 'ait': ['wait', 'portrait', 'faith', 'awaited'], 'ngd': ['kingdom'], 'gdo': ['kingdom'], 'dom': ['kingdom', 'domestic', 'predominantly', 'predominant'], 'mom': ['moment'], 'gyp': ['gypsy'], 'yps': ['gypsy'], 'psy': ['gypsy'], 'aci': ['racing', 'facilities'], 'fas': ['fast', 'fascinates'], 'usa': ['usain', 'thousands', 'usa'], 'bol': ['bolt'], 'olt': ['bolt'], 'won': ['wonder', 'wonderful'], 'sud': ['suddenly'], 'udd': ['suddenly'], 'enl': ['suddenly'], 'nly': ['suddenly', 'mainly'], 'fet': ['lifetime'], 'eti': ['lifetime', 'meeting', 'retired'], 'piv': ['pivotal'], 'ivo': ['pivotal'], 'vot': ['pivotal'], 'fes': ['lifestyle', 'safest'], 'tyl': ['lifestyle', 'styles'], 'yle': ['lifestyle', 'styles'], 'ssu': ['pressure', 'issuance', 'issued', 'possums'], 'mpl': ['implement', 'employment', 'temple', 'incomplete', 'completely'], 'rog': ['programmes', 'progress'], 'ram': ['programmes', 'dramatically', 'ramachandran'], 'mme': ['programmes', 'swimmers', 'immense', 'recommend'], 'elf': ['welfare'], 'lfa': ['welfare'], 'rev': ['prevention'], 'hte': ['highlighted'], 'htt': ['http'], 'ttp': ['http'], 'moe': ['moef'], 'oef': ['moef'], 'hum': ['human'], 'uma': ['human', 'pumas'], 'onf': ['conflict', 'confidence'], 'nfl': ['conflict', 'influence'], 'fli': ['conflict', 'flickr', 'flights'], 'hur': ['hurray', 'motichur'], 'ggl': ['toggle'], 'nav': ['navigation'], 'avi': ['navigation', 'avid'], 'vig': ['navigation', 'vigil'], 'iga': ['navigation'], 'nst': ['constitution', 'institute', 'instrumental'], 'tit': ['constitution', 'institute', 'altitude', 'multitude'], 'itu': ['constitution', 'bituminous', 'institute', 'situated', 'altitude', 'multitude'], 'tut': ['constitution', 'institute'], 'cut': ['cut'], 'pak': ['pakhro'], 'khr': ['pakhro'], 'hro': ['pakhro', 'throughout'], 'ctr': ['ctr'], 'pai': ['pairs', 'painted', 'pairing'], 'cur': ['currently', 'current', 'secure'], 'rre': ['currently', 'current', 'squirrel'], 'ava': ['available', 'bhadravati', 'aggravated', 'javascript'], 'vai': ['available'], 'ila': ['available', 'thailand'], 'lab': ['available', 'malabar'], 'abl': ['available', 'unforgettable', 'reliable', 'refundable', 'enjoyable', 'memorable', 'suitable', 'remarkable', 'variables', 'tabled', 'considerably', 'notably', 'establish', 'changeable', 'probably', 'table'], 'noi': ['noida', 'noise', 'noises'], 'oid': ['noida', 'avoid'], 'gor': ['gorewada', 'category'], 'rew': ['gorewada', 'surewani', 'rewarding'], 'ewa': ['gorewada', 'surewani', 'rewarding'], 'wad': ['gorewada'], 'clo': ['closed', 'closing', 'enclosures', 'disclose', 'disclosed'], 'los': ['closed', 'closing', 'lose', 'enclosures', 'disclose', 'disclosed'], 'unm': ['unmatched'], 'nma': ['unmatched'], 'cra': ['crafted'], 'raf': ['crafted'], 'aft': ['crafted'], 'jaw': ['jawai'], 'ccl': ['acclaimed'], 'mba': ['embark', 'sambar'], 'ert': ['expert', 'fertility', 'advertisement', 'asserted'], 'rag': ['encouraging'], 'twi': ['twitter'], 'itt': ['twitter', 'little'], 'lgg': ['lggcumqbx'], 'ggc': ['lggcumqbx'], 'gcu': ['lggcumqbx'], 'cum': ['lggcumqbx'], 'umq': ['lggcumqbx'], 'mqb': ['lggcumqbx'], 'qbx': ['lggcumqbx'], 'idd': ['siddhartha', 'hidden'], 'ddh': ['siddhartha'], 'kuk': ['kukrail'], 'ukr': ['kukrail'], 'kra': ['kukrail'], 'yog': ['yogi'], 'tya': ['adityanath'], 'cap': ['capital', 'captain', 'landscapes'], 'api': ['capital', 'rapidly', 'terrapins'], 'pit': ['capital', 'precipitation'], 'cis': ['decision'], 'cab': ['cabinet'], 'bin': ['cabinet', 'combining'], 'mee': ['meeting', 'meet', 'meets'], 'eet': ['meeting', 'meet', 'meets', 'cheetah'], 'tue': ['tuesday'], 'ues': ['tuesday', 'guests', 'question'], 'esd': ['tuesday'], 'sda': ['tuesday', 'civilsdaily'], 'bha': ['bhawan', 'bharat', 'bhadravati'], 'haw': ['bhawan', 'hawk', 'hawks'], 'irm': ['chairmanship', 'reaffirmed'], 'rma': ['chairmanship', 'buteamonosperma', 'normally', 'informal'], 'omb': ['combining', 'colombia'], 'mbi': ['combining', 'colombia'], 'elu': ['elusive'], 'ots': ['spots'], 'tru': ['truly', 'true', 'instrumental'], 'rul': ['truly', 'ruled'], 'uly': ['truly', 'july'], 'unf': ['unforgettable', 'unfolds'], 'tab': ['unforgettable', 'sitabani', 'suitable', 'tabled', 'database', 'notably', 'establish', 'table'], 'ngu': ['langurs'], 'gur': ['langurs'], 'hop': ['hopefully'], 'pef': ['hopefully'], 'efu': ['hopefully', 'refundable', 'refuges'], 'shy': ['shy'], 'luc': ['lucky'], 'uck': ['lucky', 'suckling', 'blackbuck'], 'fly': ['fly'], 'uda': ['udawalawe', 'satpuda', 'dhauda'], 'daw': ['udawalawe'], 'wal': ['udawalawe', 'walking', 'walk'], 'sor': ['resorts'], 'ink': ['link', 'shrinking', 'inkling', 'linked', 'uplink', 'think'], 'wis': ['wise', 'swish', 'wish'], 'ise': ['wise', 'noise', 'raises', 'promise', 'disease', 'tortoise', 'noises', 'rise', 'paradise', 'advertisement', 'risen'], 'elt': ['sheltering'], 'lte': ['sheltering', 'filters'], 'him': ['himalayan', 'chimur', 'himalaya', 'abhimanu'], 'iss': ['issuance', 'bliss', 'issued'], 'sua': ['issuance'], 'uan': ['issuance'], 'dir': ['directives', 'directly', 'redirected'], 'ois': ['noise', 'moist', 'tortoise', 'noises'], 'urb': ['disturbace'], 'rba': ['disturbace'], 'mov': ['movements', 'moving'], 'vem': ['movements', 'achievement'], 'emp': ['employment', 'temple', 'temperate'], 'loy': ['employment'], 'oym': ['employment'], 'yme': ['employment', 'payment'], 'you': ['youths'], 'ths': ['youths'], 'gad': ['bandhavgad', 'durgadevi', 'megadiverse'], 'atp': ['satpuda', 'satpura'], 'tpu': ['satpuda', 'satpura'], 'pud': ['satpuda'], 'eli': ['reliable', 'guidelines', 'traveling', 'prelims', 'believe', 'livelihoods', 'livelihood', 'lanneacoromandelica', 'dandeli'], 'lia': ['reliable', 'foliage'], 'iab': ['reliable', 'variables'], 'tuf': ['stuff'], 'lti': ['ultimate', 'cultivated', 'altitude', 'multitude'], 'bui': ['built', 'build'], 'uil': ['built', 'kuili', 'build'], 'ilt': ['built', 'filters', 'stilt'], 'nah': ['nahargarh'], 'iol': ['biological'], 'ffr': ['offroad'], 'alk': ['walking', 'walk'], 'lki': ['walking'], 'goa': ['goa'], 'hot': ['hottest', 'hot', 'hotels', 'photographers', 'photo', 'hotel'], 'esc': ['unesco', 'described'], 'cau': ['caused'], 'aus': ['caused', 'chausingha'], 'tud': ['study', 'altitude', 'studying', 'multitude'], 'udy': ['study', 'studying'], 'niv': ['university'], 'pid': ['rapidly'], 'idl': ['rapidly'], 'nki': ['shrinking'], 'urg': ['urgent', 'petersburg', 'durgadevi'], 'esy': ['courtesy'], 'rei': ['reiterated', 'reiterate'], 'eit': ['reiterated', 'reiterate', 'either'], 'afe': ['safe', 'safely', 'safest'], 'nur': ['nurturing'], 'ndl': ['friendly'], 'ems': ['systems'], 'ach': ['achieved', 'pachmarhi', 'poachers', 'ramachandran', 'beaches', 'achievement', 'achieve'], 'dou': ['doubling', 'splendour', 'double', 'undoubtedly', 'tremendous'], 'oub': ['doubling', 'double', 'undoubtedly'], 'ahe': ['ahead'], 'sch': ['schedule'], 'edu': ['schedule', 'procedures', 'education'], 'dul': ['schedule'], 'ule': ['schedule', 'ruled', 'herculean'], 'pet': ['petersburg'], 'ete': ['petersburg', 'kilometers', 'eternal', 'incomplete', 'completely'], 'rsb': ['petersburg'], 'sbu': ['petersburg'], 'bur': ['petersburg'], 'sle': ['sleeps'], 'lee': ['sleeps'], 'eps': ['sleeps', 'keeps'], 'was': ['waste'], 'don': ['done'], 'iph': ['periphery'], 'phe': ['periphery', 'photographers'], 'hts': ['nights', 'flights'], 'tum': ['bituminous'], 'umi': ['bituminous', 'illuminated'], 'nou': ['bituminous', 'announced', 'enough'], 'ads': ['roads'], 'sna': ['snakes'], 'nak': ['snakes'], 'pyt': ['pythons', 'python'], 'yth': ['pythons', 'forsyth', 'mythological', 'python', 'everything'], 'hon': ['pythons', 'honey', 'python'], 'war': ['warmer', 'awareness', 'war', 'inwards', 'rewarding', 'towards'], 'arm': ['warmer', 'army', 'armenia', 'charm'], 'mer': ['warmer', 'swimmers', 'merged'], 'bij': ['bijrani', 'bija'], 'ijr': ['bijrani'], 'jra': ['bijrani'], 'dhi': ['dhikala'], 'hik': ['dhikala', 'hikes'], 'ika': ['dhikala'], 'kal': ['dhikala', 'jackals', 'kalyan'], 'hir': ['jhirna', 'hirda', 'chirping'], 'irn': ['jhirna'], 'nan': ['sonanadi', 'predominantly', 'predominant'], 'dur': ['durgadevi', 'procedures', 'durshet'], 'dev': ['durgadevi', 'developing'], 'evi': ['durgadevi'], 'dhe': ['dhela'], 'oce': ['procedures', 'process'], 'ced': ['procedures', 'priced', 'experienced', 'rejoiced', 'fenced', 'announced'], 'flu': ['influence'], 'lue': ['influence', 'blue'], 'uen': ['influence'], 'sol': ['solely', 'solo', 'absolutely', 'solve'], 'lel': ['solely', 'unparalleled'], 'asp': ['aspects'], 'uto': ['automated'], 'tom': ['automated', 'customized', 'custom'], 'oma': ['automated', 'lanneacoromandelica', 'romantic'], 'mpu': ['computerized'], 'put': ['computerized', 'undisputed'], 'riz': ['computerized'], 'ize': ['computerized', 'specializes', 'customized', 'personalized', 'specialize', 'citizens'], 'zed': ['computerized', 'customized', 'personalized'], 'rib': ['distribution', 'tribes', 'described', 'contribute'], 'ibu': ['distribution', 'contribute'], 'but': ['distribution', 'buteamonosperma', 'contribute'], 'nov': ['nov', 'innovations'], 'ner': ['generated', 'energy', 'generally', 'itineraries'], 'pay': ['payment'], 'aym': ['payment'], 'ref': ['refundable', 'refuges', 'refers', 'reflecting'], 'dab': ['refundable'], 'igc': ['bigcatsindia', 'bigcatssafari'], 'gca': ['bigcatsindia', 'bigcatssafari'], 'tss': ['bigcatssafari'], 'liz': ['specializes', 'personalized', 'specialize'], 'zes': ['specializes'], 'cus': ['customized', 'custom', 'discussed'], 'sto': ['customized', 'custom', 'storks', 'astound', 'keystone', 'historic'], 'omi': ['customized', 'prominent', 'promise', 'predominantly', 'predominant', 'economic'], 'miz': ['customized'], 'oup': ['group', 'couples', 'groups', 'coupled'], 'upl': ['couples', 'uplink', 'coupled'], 'tir': ['retired', 'entire'], 'sug': ['suggestions'], 'ugg': ['suggestions'], 'fut': ['future'], 'utu': ['future'], 'ocl': ['proclaimed'], 'idf': ['idf'], 'cel': ['celebrates'], 'leb': ['celebrates'], 'bra': ['celebrates', 'cobra', 'brazil', 'zebra'], 'ais': ['raises'], 'vil': ['civilsdaily', 'civil', 'villus', 'village', 'villas'], 'lsd': ['civilsdaily'], 'dai': ['civilsdaily'], 'rsp': ['perspective'], 'aba': ['sitabani', 'malabar', 'database'], 'abs': ['absolutely'], 'bso': ['absolutely'], 'olu': ['absolutely', 'volume'], 'veh': ['vehicles'], 'ehi': ['vehicles', 'behind'], 'hiv': ['shiva'], 'hal': ['shallow', 'challenges', 'halad'], 'roo': ['brooks'], 'oks': ['brooks'], 'yab': ['enjoyable'], 'fis': ['fishes', 'fish'], 'bec': ['become'], 'mem': ['memorable', 'remember', 'members'], 'emo': ['memorable'], 'rab': ['memorable', 'considerably'], 'rom': ['prominent', 'promise', 'lanneacoromandelica', 'romantic'], 'nen': ['prominent', 'subcontinent'], 'nkl': ['inkling'], 'kli': ['inkling', 'suckling'], 'nty': ['plenty', 'seventy'], 'vul': ['vultures'], 'ltu': ['vultures', 'culture', 'agriculture', 'cultural'], 'isa': ['disappeared'], 'sap': ['disappeared'], 'app': ['disappeared', 'appointed', 'appreciate', 'applies', 'approximately', 'appreciated'], 'ppe': ['disappeared', 'worshipped'], 'tow': ['towns', 'towers', 'towards'], 'wns': ['towns'], 'sel': ['selected', 'russel', 'intensely'], 'lec': ['selected', 'collection', 'reflecting'], 'rem': ['premium', 'remarkable', 'remember', 'supreme', 'tremendous'], 'emi': ['premium', 'pandemic', 'teeming'], 'miu': ['premium'], 'ium': ['premium', 'medium'], 'lod': ['lodges', 'lodge'], 'odg': ['lodges', 'dodgy', 'lodge'], 'dge': ['lodges', 'lodge', 'badger'], 'els': ['hotels', 'else', 'travels'], 'fil': ['fill', 'filters'], 'epr': ['representative'], 'ubc': ['subcontinent'], 'bco': ['subcontinent'], 'unp': ['unparalleled'], 'npa': ['unparalleled'], 'jou': ['journey'], 'ney': ['journey', 'honey'], 'guj': ['gujarat'], 'uja': ['gujarat'], 'abo': ['abode', 'abound'], 'bod': ['abode', 'bodies', 'body'], 'sui': ['suitable'], 'uit': ['suitable', 'fruits', 'fruit', 'quite'], 'utb': ['outback'], 'tba': ['outback'], 'exh': ['exhibiting'], 'xhi': ['exhibiting'], 'hib': ['exhibiting'], 'ibi': ['exhibiting', 'possibilities', 'compatibility'], 'poc': ['pockets'], 'cke': ['pockets', 'tracked', 'thickets', 'crickets', 'trackers'], 'ket': ['pockets', 'thickets', 'crickets'], 'ssl': ['grasslands', 'sslc'], 'sla': ['grasslands', 'island', 'islands'], 'rka': ['remarkable'], 'kab': ['remarkable'], 'veg': ['vegetation'], 'ndo': ['endowments', 'splendour', 'undoubtedly', 'tremendous', 'endowed'], 'dow': ['endowments', 'download', 'endowed'], 'owm': ['endowments'], 'wme': ['endowments'], 'cip': ['precipitation'], 'ipi': ['precipitation'], 'cul': ['cultivated', 'culture', 'agriculture', 'cult', 'cultural', 'particularly', 'herculean', 'difficulty'], 'ops': ['crops'], 'fru': ['fruits', 'fruit'], 'rui': ['fruits', 'fruit'], 'orc': ['orchards', 'porcupine'], 'ims': ['prelims'], 'eck': ['checking'], 'cki': ['checking'], 'nke': ['linked', 'monkeys'], 'wnl': ['download'], 'nlo': ['download'], 'loa': ['download'], 'pdf': ['pdf'], 'suc': ['success', 'suckling'], 'ucc': ['success'], 'cce': ['success', 'access'], 'mpi': ['hampi', 'camping'], 'onu': ['monuments'], 'ume': ['monuments', 'instrumental', 'volume'], 'cho': ['choose', 'chousinga'], 'hoo': ['choose', 'hoot', 'livelihoods', 'livelihood'], 'oos': ['choose', 'zoos', 'boost'], 'ilp': ['wilpattu'], 'lpa': ['wilpattu'], 'att': ['wilpattu', 'attacks', 'catterall'], 'ttu': ['wilpattu'], 'sad': ['sadly'], 'ngo': ['ongoing'], 'isl': ['island', 'islands'], 'pok': ['spoke'], 'oke': ['spoke'], 'ctl': ['directly'], 'tol': ['told'], 'cof': ['scoffed'], 'fed': ['scoffed'], 'icn': ['picnic'], 'cni': ['picnic'], 'rue': ['true'], 'pho': ['photographers', 'photo'], 'oto': ['photographers', 'photo', 'ecotourism'], 'die': ['bodies', 'wodier'], 'nfa': ['infamous'], 'ili': ['militants', 'possibilities', 'facilities', 'kuili', 'fertility', 'compatibility'], 'lit': ['militants', 'possibilities', 'facilities', 'little', 'fertility', 'reality', 'compatibility', 'quality'], 'cks': ['attacks', 'tracks'], 'orr': ['horrified', 'torrid'], 'rif': ['horrified'], 'ccu': ['occupants'], 'cup': ['occupants', 'porcupine', 'cup'], 'upa': ['occupants'], 'mio': ['mio'], 'uns': ['unsplash'], 'spl': ['unsplash', 'splendour', 'displaying'], 'rno': ['arnold'], 'nol': ['arnold'], 'nto': ['antoo', 'contours'], 'too': ['antoo'], 'pli': ['uplink', 'applies'], 'nno': ['innovations', 'announced'], 'ova': ['innovations'], 'olv': ['solve'], 'lve': ['solve', 'velvet'], 'rso': ['personalized'], 'aly': ['analyses', 'kalyan'], 'lys': ['analyses'], 'yse': ['analyses'], 'ips': ['ips'], 'ilm': ['jhilmil'], 'lmi': ['jhilmil'], 'mot': ['motichur', 'mother'], 'oti': ['motichur', 'exotic'], 'ich': ['motichur', 'richest'], 'chu': ['motichur'], 'auh': ['gauhri'], 'uhr': ['gauhri'], 'fug': ['refuges'], 'uge': ['refuges'], 'gst': ['amongst'], 'wne': ['renowned'], 'xce': ['exception'], 'cep': ['exception'], 'lou': ['flourishes'], 'chm': ['pachmarhi'], 'hma': ['pachmarhi'], 'sce': ['scenic', 'scenes'], 'eni': ['scenic', 'evening', 'armenia', 'peninsular'], 'apt': ['captain', 'adapted', 'raptors'], 'pta': ['captain'], 'rsy': ['forsyth'], 'syt': ['forsyth'], 'rob': ['probes', 'probably'], 'uai': ['quaint', 'acquainted'], 'ams': ['streams'], 'arb': ['arboreal'], 'rbo': ['arboreal'], 'gem': ['gem'], 'ngi': ['springing', 'arranging'], 'ffs': ['cliffs'], 'cav': ['caves'], 'ano': ['canopies', 'canopy', 'llanos', 'another'], 'nop': ['canopies', 'canopy'], 'pie': ['canopies'], 'isk': ['frisked'], 'ske': ['frisked', 'asked'], 'dod': ['dodgy'], 'dgy': ['dodgy'], 'gia': ['giant'], 'irr': ['squirrel'], 'isp': ['undisputed', 'displaying'], 'spu': ['undisputed'], 'coe': ['coexist'], 'oex': ['coexist'], 'avo': ['avoid'], 'voi': ['avoid'], 'rfa': ['interface'], 'fac': ['interface', 'facilities'], 'upr': ['supreme'], 'mis': ['promise'], 'asu': ['treasure'], 'bel': ['believe'], 'lie': ['believe', 'applies', 'lies'], 'stl': ['rustle', 'coastline'], 'tle': ['rustle', 'myrtle', 'little'], 'eaf': ['leaf', 'reaffirmed'], 'ouc': ['crouching'], 'egs': ['legs'], 'inw': ['inwards'], 'nwa': ['inwards'], 'ady': ['ready'], 'swi': ['swish', 'swimmers'], 'erh': ['waterholes', 'waterhole'], 'rho': ['waterholes', 'waterhole'], 'owh': ['nowhere'], 'lse': ['else'], 'bef': ['befits'], 'oot': ['hoot'], 'ckl': ['suckling'], 'nie': ['colonies', 'bernie'], 'rid': ['torrid'], 'sal': ['sal', 'salai'], 'rtr': ['portrait'], 'rej': ['rejoiced', 'rejuvenating'], 'ejo': ['rejoiced'], 'joi': ['rejoiced'], 'oic': ['rejoiced'], 'elo': ['antelope', 'developing'], 'lop': ['antelope', 'developing'], 'ckb': ['blackbuck'], 'kbu': ['blackbuck'], 'buc': ['blackbuck'], 'erd': ['herd'], 'eys': ['monkeys', 'keystone'], 'etl': ['wetland'], 'tla': ['wetland'], 'lap': ['lapwing'], 'apw': ['lapwing'], 'pwi': ['lapwing'], 'ool': ['wooly', 'cool', 'pool'], 'oly': ['wooly'], 'adj': ['adjutant', 'adjusted', 'adjacent'], 'dju': ['adjutant', 'adjusted'], 'jut': ['adjutant'], 'uta': ['adjutant'], 'acq': ['acquainted'], 'cqu': ['acquainted'], 'inq': ['inquisitiveness'], 'nqu': ['inquisitiveness'], 'uis': ['inquisitiveness'], 'poi': ['appointed'], 'xpa': ['expanse', 'expanding'], 'iag': ['foliage'], 'ppr': ['appreciate', 'approximately', 'appreciated'], 'ddy': ['toddy'], 'rcu': ['porcupine', 'herculean'], 'upi': ['porcupine'], 'bly': ['considerably', 'notably', 'probably'], 'fur': ['furled', 'fur'], 'url': ['furled'], 'mha': ['mha'], 'bie': ['biennial'], 'enn': ['biennial'], 'nni': ['biennial'], 'nia': ['biennial', 'armenia', 'patagonia'], 'agr': ['agriculture', 'agree'], 'gri': ['agriculture'], 'icu': ['agriculture', 'particularly', 'difficulty'], 'hly': ['roughly'], 'rur': ['rural'], 'ubs': ['subsistence', 'cubs'], 'bsi': ['subsistence'], 'lih': ['livelihoods', 'livelihood'], 'iho': ['livelihoods', 'livelihood'], 'rgy': ['energy'], 'eds': ['needs'], 'uat': ['situated'], 'igi': ['vigil', 'frigid', 'origin'], 'kee': ['keeping', 'keeps'], 'epi': ['keeping'], 'poa': ['poachers'], 'oac': ['poachers'], 'ppl': ['applies'], 'cil': ['facilities'], 'osu': ['enclosures'], 'fen': ['fenced'], 'ewi': ['viewing'], 'lao': ['laos'], 'aos': ['laos'], 'eem': ['seem', 'teeming'], 'wim': ['swimmers'], 'imm': ['swimmers', 'immense'], 'jon': ['jonathan'], 'sso': ['rossouw'], 'ouw': ['rossouw'], 'dap': ['adapted'], 'rig': ['frigid', 'origin', 'right'], 'gid': ['frigid'], 'ury': ['surya'], 'rya': ['surya'], 'mac': ['ramachandran'], 'cub': ['cubs'], 'rni': ['bernie'], 'ckr': ['flickr'], 'moi': ['moist'], 'mpe': ['temperate'], 'ane': ['montane', 'planet'], 'meg': ['megadiverse'], 'nea': ['nearly', 'beneath', 'lanneacoromandelica'], 'arl': ['nearly', 'particularly'], 'rly': ['nearly', 'particularly'], 'gfw': ['gfw'], 'jus': ['adjusted'], 'opy': ['canopy'], 'inl': ['mainly'], 'fib': ['fiber'], 'ibe': ['fiber', 'tribes', 'described'], 'pal': ['palm', 'palas'], 'exa': ['examination'], 'xam': ['examination'], 'das': ['dashboard'], 'shb': ['dashboard'], 'hbo': ['dashboard'], 'rtl': ['shortly', 'myrtle'], 'hrs': ['hrs'], 'dyi': ['studying'], 'yin': ['studying', 'varying', 'displaying'], 'rct': ['antarctica'], 'thw': ['thwa'], 'hwa': ['thwa'], 'baa': ['tadobaandhari'], 'aan': ['tadobaandhari'], 'apu': ['chandrapur'], 'rox': ['approximately'], 'oxi': ['approximately'], 'teh': ['tehsil', 'maharashtratehsil'], 'ehs': ['tehsil', 'maharashtratehsil'], 'hsi': ['tehsil', 'maharashtratehsil'], 'had': ['bhadravati'], 'adr': ['bhadravati'], 'god': ['god'], 'aru': ['taru'], 'rsh': ['worshipped', 'marsh', 'durshet'], 'ipp': ['worshipped'], 'efe': ['refers'], 'lla': ['village', 'llanos', 'villas'], 'lag': ['village'], 'myt': ['mythological'], 'dei': ['deified'], 'eif': ['deified'], 'gon': ['gond', 'patagonia'], 'vic': ['vicinity', 'services'], 'mur': ['chimur'], 'hun': ['hunting'], 'dja': ['adjacent'], 'jac': ['adjacent', 'jackals'], 'edo': ['predominantly', 'predominant'], 'odl': ['woodlands'], 'dla': ['woodlands'], 'hty': ['eighty'], 'oco': ['crocodile'], 'cod': ['crocodile'], 'dil': ['crocodile'], 'ija': ['bija'], 'hau': ['dhauda', 'chausingha'], 'aud': ['dhauda'], 'sem': ['semal', 'advertisement'], 'beh': ['beheda', 'behind'], 'ehe': ['beheda'], 'gum': ['gum'], 'ahu': ['mahuamadhuca'], 'hua': ['mahuamadhuca'], 'uam': ['mahuamadhuca'], 'dhu': ['mahuamadhuca'], 'huc': ['mahuamadhuca'], 'uca': ['mahuamadhuca', 'education'], 'myr': ['myrtle', 'myriad'], 'yrt': ['myrtle'], 'lam': ['flame'], 'ono': ['buteamonosperma', 'economic'], 'nos': ['buteamonosperma', 'llanos'], 'osp': ['buteamonosperma'], 'aco': ['lanneacoromandelica', 'racoons'], 'oro': ['lanneacoromandelica'], 'wod': ['wodier'], 'ier': ['wodier'], 'gho': ['throughout'], 'abu': ['abundance'], 'bun': ['abundance'], 'haj': ['khaj'], 'kui': ['kuili'], 'elv': ['velvet'], 'nso': ['parkinson'], 'bhe': ['bheria'], 'pel': ['repellent'], 'nil': ['nilgai', 'nilgiris'], 'ilg': ['nilgai', 'nilgiris'], 'lga': ['nilgai'], 'gai': ['nilgai'], 'dho': ['dhole'], 'ipe': ['striped', 'viper'], 'hye': ['hyena'], 'yen': ['hyena'], 'ena': ['hyena', 'rejuvenating'], 'ngh': ['chausingha'], 'bad': ['badger'], 'adg': ['badger'], 'rto': ['tortoise'], 'toi': ['tortoise'], 'cob': ['cobra', 'nicobar'], 'obr': ['cobra'], 'uss': ['russel', 'discussed'], 'vip': ['viper'], 'pto': ['raptors'], 'rey': ['grey'], 'gea': ['changeable'], 'eab': ['changeable'], 'awk': ['hawk', 'hawks'], 'may': ['may'], 'tht': ['breathtaking'], 'hta': ['breathtaking'], 'lgi': ['nilgiris'], 'bab': ['probably'], 'yri': ['myriad'], 'iad': ['myriad'], 'bou': ['abound', 'bounce'], 'nci': ['ancient'], 'tus': ['status'], 'fel': ['safely'], 'say': ['say'], 'gis': ['logistics', 'registered'], 'mas': ['mastered', 'pumas', 'landmass'], 'gue': ['guests'], 'nfi': ['confidence'], 'fid': ['confidence'], 'tag': ['stage', 'heritage', 'patagonia'], 'tam': ['tamil', 'untamed'], 'adu': ['nadu'], 'dar': ['secondary', 'dark'], 'slc': ['sslc'], 'diu': ['medium'], 'esu': ['result', 'results'], 'jul': ['july'], 'isc': ['disclose', 'disclosed', 'discussed'], 'scl': ['disclose', 'disclosed'], 'lef': ['left'], 'eft': ['left'], 'ify': ['holidify'], 'pvt': ['pvt'], 'ltd': ['ltd'], 'mus': ['must'], 'het': ['durshet'], 'blu': ['blue'], 'lum': ['illuminated', 'volume'], 'ttl': ['little'], 'inv': ['invest'], 'nve': ['invest'], 'bon': ['bonds'], 'sue': ['issued'], 'ued': ['issued', 'continued'], 'psu': ['psu'], 'nbf': ['nbfcs'], 'bfc': ['nbfcs'], 'fcs': ['nbfcs'], 'ayt': ['daytime'], 'yti': ['daytime'], 'teo': ['courteous'], 'eou': ['courteous'], 'ush': ['brush', 'bushes'], 'eez': ['breeze'], 'eze': ['breeze'], 'ryt': ['everything'], 'xtr': ['extraordinary'], 'rao': ['extraordinary'], 'aor': ['extraordinary'], 'som': ['something'], 'rmy': ['army'], 'tty': ['pretty'], 'coo': ['cool', 'racoons'], 'moo': ['moors'], 'oor': ['moors'], 'wli': ['howling'], 'irp': ['chirping'], 'rpi': ['chirping'], 'cri': ['crickets', 'javascript', 'described'], 'yes': ['eyes'], 'efl': ['reflecting'], 'fle': ['reflecting'], 'bus': ['bushes'], 'pum': ['pumps', 'pumas'], 'ump': ['pumps'], 'toe': ['toe'], 'yet': ['yet'], 'agg': ['aggravated'], 'ggr': ['aggravated'], 'teg': ['category'], 'ego': ['category'], 'ory': ['category'], 'aje': ['majestic'], 'jes': ['majestic'], 'bte': ['undoubtedly'], 'edl': ['undoubtedly'], 'asc': ['fascinates', 'javascript'], 'xti': ['extinction'], 'oon': ['boon', 'racoons', 'balloon'], 'asy': ['easy'], 'eau': ['beauties'], 'sab': ['sabi'], 'cot': ['ecotourism'], 'dem': ['pandemic'], 'mic': ['pandemic', 'economic'], 'mmu': ['communities'], 'mun': ['communities'], 'suf': ['suffered'], 'ldw': ['worldwide'], 'dwi': ['worldwide'], 'jag': ['jaguars'], 'agu': ['jaguars'], 'nom': ['economic'], 'bia': ['colombia'], 'ago': ['patagonia'], 'raz': ['brazil'], 'azi': ['brazil'], 'zil': ['brazil'], 'wks': ['hawks'], 'car': ['cardinals'], 'sum': ['possums'], 'ums': ['possums'], 'erb': ['herbs'], 'rbs': ['herbs'], 'ico': ['nicobar'], 'coa': ['coastline', 'coastal'], 'oas': ['coastline', 'coastal'], 'scu': ['discussed'], 'abh': ['abhimanu'], 'bhi': ['abhimanu'], 'vac': ['privacy'], 'acy': ['privacy'], 'mbh': ['ranthambhore'], 'bho': ['ranthambhore'], 'voy': ['voygr'], 'oyg': ['voygr'], 'ygr': ['voygr'], 'ski': ['skilled'], 'six': ['six'], 'ndm': ['landmass'], 'dma': ['landmass'], 'hid': ['hidden'], 'gul': ['gulf'], 'ulf': ['gulf'], 'ryi': ['varying'], 'exo': ['exotic'], 'xot': ['exotic'], 'poo': ['pool'], 'eju': ['rejuvenating'], 'juv': ['rejuvenating'], 'uve': ['rejuvenating'], 'ody': ['body'], 'ask': ['asked'], 'usl': ['seriously'], 'sly': ['seriously'], 'rfu': ['wonderful'], 'mul': ['multitude'], 'dsc': ['landscapes'], 'sca': ['landscapes'], 'usd': ['usd'], 'lty': ['difficulty'], 'wse': ['browser'], 'css': ['css'], 'obt': ['obtain'], 'bta': ['obtain'], 'tib': ['compatibility'], 'rer': ['explorer'], 'inu': ['continued'], 'nue': ['continued'], 'ayi': ['displaying'], 'jav': ['javascript'], 'vas': ['javascript'], 'ipt': ['javascript'], 'vol': ['volume'], 'lts': ['results'], 'cyc': ['cycle'], 'ycl': ['cycle'], 'lya': ['kalyan'], 'bed': ['described'], 'his': ['historic'], 'aff': ['reaffirmed'], 'owa': ['towards'], 'pee': ['speed'], 'keh': ['stakeholders'], 'eho': ['stakeholders'], 'ecu': ['secure'], 'tiz': ['citizens'], 'zen': ['citizens'], 'zeb': ['zebra'], 'tah': ['cheetah']}\n"
     ]
    }
   ],
   "source": [
    "tri_grams = make_trigrams()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to merge the posting lists using trigram index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge algo of 2 slightly changed to find intersection of trigram lists\n",
    "def merge_algo_2(queries):\n",
    "\n",
    "  \"\"\" \n",
    "  This function will merge the posting lists of the queries using the merge algorithm 2 ie. using the trigrams\n",
    "\n",
    "  Parameters:\n",
    "  queries (list): A list of queries\n",
    "\n",
    "  Returns:\n",
    "  list: A list of documents that match the queries\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  answer_postings = []\n",
    "  \n",
    "  # check if the queries are in the inverted index, if not return an empty list\n",
    "  for i in queries:\n",
    "    if tri_grams.get(i) == None:\n",
    "      return []\n",
    "    else:\n",
    "      answer_postings.append(tri_grams[i])\n",
    "\n",
    "  # help restrictive intersection first\n",
    "  answer_postings.sort(key=len)\n",
    "\n",
    "  # sort the posting lists\n",
    "  for i in answer_postings:\n",
    "    i.sort()\n",
    "  \n",
    "  # get the first posting list\n",
    "  top = answer_postings[0]\n",
    "\n",
    "  # traverse through the remaining posting lists\n",
    "  for i in range(1, len(answer_postings)):\n",
    "    #intersection with first list\n",
    "    m = len(top)\n",
    "    \n",
    "    n = len(answer_postings[i])\n",
    "    \n",
    "    k=0\n",
    "    l=0\n",
    "    merged = []\n",
    "\n",
    "    # merge the posting lists\n",
    "    while(k<m and l<n):\n",
    "\n",
    "      # if the posting lists have the same document id, append it to the merged list\n",
    "      if(top[k] == answer_postings[i][l]):\n",
    "        merged.append(top[k])\n",
    "        k = k+1\n",
    "        l = l+1\n",
    "      \n",
    "      else:\n",
    "        # if the document id of the first posting list is less than the document id of the second posting list, increment the index of the first posting list\n",
    "        if(top[k] < answer_postings[i][l]):\n",
    "          k = k+1\n",
    "        # if the document id of the first posting list is greater than the document id of the second posting list, increment the index of the second posting list\n",
    "        else:\n",
    "          l=l+1\n",
    "\n",
    "    top = merged\n",
    "\n",
    "  return top"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the assumptions taken is that the word must contain atleast two correct trigrams, so that intersection of the trigrams of the words exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(wrong_word):\n",
    "\n",
    "  \"\"\" \n",
    "  This function will find the correct spelling of the word\n",
    "\n",
    "  Parameters:\n",
    "  wrong_word (str): The word whose spelling is to be checked\n",
    "\n",
    "  Returns:\n",
    "  list: A list of words that are similar to the word whose spelling is to be checked\n",
    "  \n",
    "  \"\"\"\n",
    "\n",
    "  j=0\n",
    "  word_tri = []\n",
    "\n",
    "  #make trigrams of the word\n",
    "  while((j+2)<len(wrong_word)):\n",
    "\n",
    "    # make the trigram\n",
    "    tri = wrong_word[j] + wrong_word[j+1] + wrong_word[j+2]\n",
    "\n",
    "    #append to list\n",
    "    word_tri.append(tri)\n",
    "    j=j+1\n",
    "\n",
    "  i=0\n",
    "\n",
    "  # if only one trigram is present, return an empty list\n",
    "  if(len(word_tri)<2):\n",
    "    print(\"Spellcheck can't take place\")\n",
    "    return []\n",
    "  \n",
    "\n",
    "  intersections = []\n",
    "  #pairs of trigrams taken to find intersection\n",
    "  for i in range(len(word_tri)):\n",
    "    for j in range(i+1, len(word_tri)):\n",
    "      intersections.append(merge_algo_2([word_tri[i], word_tri[j]]))\n",
    "\n",
    "  return intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to choose word containing most trigram parts\n",
    "def most_frequent(List):\n",
    "    return max(set(List), key = List.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_1d(user_query):\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  This function will process the query and return the documents that match the query\n",
    "\n",
    "  Parameters:\n",
    "  user_query (str): The query to be processed\n",
    "\n",
    "  Returns:\n",
    "  list: A list of documents that match the query\n",
    "\n",
    "  \"\"\"\n",
    "  \n",
    "  user_query = extract_queries(user_query)\n",
    "\n",
    "  print(\"query\" , user_query)\n",
    "\n",
    "  # loop through the query\n",
    "  for i in range(len(user_query)):\n",
    "\n",
    "    # if the word is not present in the inverted index, find the correct spelling of the word\n",
    "    if user_query[i] not in sorted_inverted_index.keys():\n",
    "\n",
    "      # find the correct spelling of the word\n",
    "      possibilities = spell_check(user_query[i])\n",
    "      all_pos = []\n",
    "\n",
    "      # append all the words that are similar to the word whose spelling is to be checked\n",
    "      for j in possibilities:\n",
    "        all_pos = all_pos + j\n",
    "\n",
    "      # if no words are similar to the word whose spelling is to be checked, return an empty list\n",
    "      if(len(all_pos)<1):\n",
    "        user_query[i] = user_query[i]\n",
    "\n",
    "      # if words are similar to the word whose spelling is to be checked, choose the word that contains the most trigrams\n",
    "      else:\n",
    "        user_query[i] = most_frequent(all_pos)\n",
    "\n",
    "  print(user_query)\n",
    "\n",
    "  # merge the posting lists of the words in the query\n",
    "  retrieved_docs = merge_algo(user_query)\n",
    "\n",
    "  \n",
    "  print('Retrieved Docs :' , retrieved_docs)\n",
    "  return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query ['tiger', 'saphari']\n",
      "['tiger', 'sharing']\n",
      "happen\n",
      "[62, 6]\n",
      "[21, 62, 23, 44, 25, 65, 66, 27, 47, 67, 28, 48, 68, 69, 30, 51, 71, 32, 52, 13, 33, 54, 74, 35, 37, 78, 39, 40, 80]\n",
      "Retrieved Docs : [62]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[62]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_query_1d('Tiger AND Saphari')\n",
    "#Note we see the corrected word is not Safari, since as per our assumption, the word Saphari has only one trigram correct which is not enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query ['wyldlife', 'poching']\n",
      "['wildlife', 'watching']\n",
      "happen\n",
      "[42]\n",
      "[41, 42, 23, 43, 4, 24, 45, 65, 66, 27, 47, 48, 69, 30, 70, 52, 54, 74, 56, 37, 77, 18]\n",
      "Retrieved Docs : [42]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[42]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_query_1d('Wyldlife AND Poching')\n",
    "#same as first case, better results could be found if bigram was used instead of trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query ['lepard', 'night']\n",
      "['department', 'night']\n",
      "happen\n",
      "[21, 42, 47, 48, 49, 30]\n",
      "[41, 42, 43, 45, 46, 47, 48, 71, 32, 52, 55, 56, 57]\n",
      "Retrieved Docs : [42, 47, 48]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[42, 47, 48]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_query_1d('Lepard AND Night')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "Extend your system from part 2 to perform simple TF-IDF scoring of the retrieved results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(term, inverted_index):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the inverse document frequency of a term.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    term (str): A term\n",
    "    inverted_index (dict): An inverted index of the documents\n",
    "\n",
    "    Returns:\n",
    "    float: The inverse document frequency of the term\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #  find the number of documents in the corpus\n",
    "    N = 80\n",
    "\n",
    "    #  find the number of documents in the corpus that contain the term\n",
    "    df = len(inverted_index[term])\n",
    "\n",
    "    #  calculate the inverse document frequency upto 5 decimal places\n",
    "    idf = round(math.log10(N / df), 10)\n",
    "\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(term, doc_id):\n",
    "    \"\"\"\n",
    "    Calculate the term frequency of a term in a document.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    term (str): A term\n",
    "    document (str): A document\n",
    "\n",
    "    Returns:\n",
    "    int: The term frequency of the term in the document\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    FILE_NAME = \"documents\\d\" + str((doc_id - 1) % 20 + 1 ) + \"_q\" + str((doc_id - 1) // 20 + 1) + \".txt\"\n",
    "\n",
    "    #  open the document\n",
    "    with open(FILE_NAME, 'r') as file:\n",
    "        document = file.read()\n",
    "    \n",
    "    #  split the document into words\n",
    "    words = document.split()\n",
    "\n",
    "    #  count the number of times the term appears in the document\n",
    "    count = words.count(term)\n",
    "\n",
    "    # upto 5 decimal places\n",
    "    if count == 0:\n",
    "        tf = 0\n",
    "        \n",
    "    tf = round(0.5 + 0.5 * count / max(words.count(word) for word in words), 5)\n",
    "\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(term, doc_id, inverted_index):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the tf-idf score of a term in a document.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    term (str): A term\n",
    "    document (str): A document\n",
    "    inverted_index (dict): An inverted index of the documents\n",
    "\n",
    "    Returns:\n",
    "    float: The tf-idf score of the term in the document\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #  calculate the term frequency\n",
    "    tf = term_frequency(term, doc_id)\n",
    "\n",
    "    #  calculate the inverse document frequency\n",
    "    idf = inverse_document_frequency(term, inverted_index)\n",
    "\n",
    "    #  calculate the tf-idf score upto 5 decimal places\n",
    "    tf_idf = round(tf * idf, 5)\n",
    "\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform simple TF-IDF scoring of the retrieved results\n",
    "\n",
    "def tf_idf_score(query, doc_id, inverted_index):\n",
    "    \n",
    "        \"\"\"\n",
    "        Calculate the TF-IDF score of a document.\n",
    "    \n",
    "        Parameters:\n",
    "    \n",
    "        query (list): A list of words in the query\n",
    "        document (str): The document to calculate the TF-IDF score of\n",
    "        inverted_index (dict): An inverted index of the documents\n",
    "    \n",
    "        Returns:\n",
    "        float: The TF-IDF score of the document\n",
    "    \n",
    "        \"\"\"\n",
    "    \n",
    "        #  create a list to store the TF-IDF scores of the words in the query\n",
    "        tf_idf_scores = []\n",
    "    \n",
    "        #  calculate the TF-IDF score of each word in the query\n",
    "        for word in query:\n",
    "            if word not in inverted_index:\n",
    "                print(f\"{word} not in inverted index\")\n",
    "                return 0\n",
    "            \n",
    "            tf_idf_scores.append(tf_idf(word, doc_id, inverted_index))\n",
    "    \n",
    "        #  return the sum of the TF-IDF scores\n",
    "        return sum(tf_idf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  find the tf idf of term in documents in the result set of the query\n",
    "import operator\n",
    "\n",
    "\n",
    "def cal_tf_idf(query, inverted_index, result):\n",
    "    \"\"\"\n",
    "    Calculate the TF-IDF score of the documents in the result set of the query.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    query (list): A list of words in the query\n",
    "    inverted_index (dict): An inverted index of the documents\n",
    "    result (list): A list of documents that match the query\n",
    "\n",
    "    Returns:\n",
    "    list: A list of documents that match the query sorted by the TF-IDF score\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #  create a dictionary to store the TF-IDF score of the documents in the result set\n",
    "    tf_idf_scores = {}\n",
    "\n",
    "    #  calculate the TF-IDF score of the documents in the result set\n",
    "    for doc_id in result:\n",
    "        doc_name = \"d\" + str((doc_id - 1) % 20 + 1 ) + \"_q\" + str((doc_id - 1) // 20 + 1)\n",
    "        tf_idf_scores[doc_name] = tf_idf_score(query, doc_id, inverted_index)\n",
    "\n",
    "    #  sort the documents in the result set by the TF-IDF score\n",
    "    sorted_tf_idf_scores = sorted(tf_idf_scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    return sorted_tf_idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  Tiger AND Safari\n",
      "tf-idf:  [('d20_q4', 0.91242), ('d5_q4', 0.8022499999999999), ('d14_q4', 0.79707), ('d4_q3', 0.7944800000000001), ('d5_q2', 0.7944800000000001), ('d18_q4', 0.76034), ('d2_q4', 0.75518), ('d8_q4', 0.75518), ('d7_q3', 0.72373), ('d10_q2', 0.69621), ('d7_q4', 0.6869000000000001), ('d6_q4', 0.6858599999999999), ('d9_q4', 0.68431), ('d19_q2', 0.62535), ('d8_q3', 0.59617)]\n",
      "Query:  Wildlife AND Poaching\n",
      "tf-idf:  []\n",
      "Query:  Leopard AND Night\n",
      "tf-idf:  [('d11_q4', 1.0348000000000002)]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_query = []\n",
    "\n",
    "for i in range(len(queries)):\n",
    "    print(\"Query: \", sample_queries[i])\n",
    "\n",
    "    tf_idf_query.append(cal_tf_idf(queries[i], inverted_index, results[i]))\n",
    "\n",
    "    print(\"tf-idf: \", tf_idf_query[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0318b595dd22f0d5b404ef1f7207d08d49d70aaa8348992cda9c68521fdf7933"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
